{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCLToKc4zrVF"
      },
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVGoIqp_zokl"
      },
      "source": [
        "In text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.\n",
        "Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IovxLZ64VLUF",
        "outputId": "b288f686-af0d-4003-a792-2309a23b937d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install stop-words\n",
        "%pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rCX0iv26CU45"
      },
      "outputs": [],
      "source": [
        "# We will start by taking a few sentences and treating them as documents\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# create English stop words list\n",
        "en_stop = get_stop_words('en')\n",
        "\n",
        "# Create p_stemmer of class PorterStemmer\n",
        "p_stemmer = PorterStemmer()\n",
        "    \n",
        "# create sample documents\n",
        "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
        "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
        "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
        "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
        "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
        "\n",
        "# compile sample documents into a list\n",
        "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLpDr3IwVCNg",
        "outputId": "c5d8bd87-2a3f-4bc6-9249-63bc0e190b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother'], ['mother', 'spend', 'lot', 'time', 'drive', 'brother', 'around', 'basebal', 'practic'], ['health', 'expert', 'suggest', 'drive', 'may', 'caus', 'increas', 'tension', 'blood', 'pressur'], ['often', 'feel', 'pressur', 'perform', 'well', 'school', 'mother', 'never', 'seem', 'drive', 'brother', 'better'], ['health', 'profession', 'say', 'brocolli', 'good', 'health']]\n"
          ]
        }
      ],
      "source": [
        "# list for tokenized documents in loop\n",
        "texts = []\n",
        "\n",
        "# We will clean and tokenize our “documents using stemming\n",
        "# loop through document list\n",
        "for doc in doc_set:\n",
        "    # clean and tokenize document string\n",
        "    raw = doc.lower()\n",
        "    tokens = tokenizer.tokenize(raw)\n",
        "\n",
        "    # remove stop words from tokens\n",
        "    stopped_tokens = [token for token in tokens if not token in en_stop]\n",
        "    \n",
        "    # stem tokens\n",
        "    stemmed_tokens = [p_stemmer.stem(token) for token in stopped_tokens]\n",
        "    \n",
        "    # add tokens to list\n",
        "    texts.append(stemmed_tokens)\n",
        "print(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm09i2gJ0MCk"
      },
      "source": [
        "During this cleaning process we introduce a new cleaning step called stemming\n",
        "(bringing words to their original stem).\n",
        "\n",
        "We will also be introducing lemmatization later in this laboratory.\n",
        "\n",
        "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word. Lemma on the other hand is an actual word of target language.\n",
        "\n",
        "Stemming follows an algorithm with steps to perform on the words which makes it faster. \n",
        "\n",
        "Lemmatization, however, uses both WordNet corpus and stop words corpus which makes it slower than stemming. You also had to define parts-of-speech to obtain the correct lemma.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu7wZRXI0zmn"
      },
      "source": [
        "We create a dictionary containing all the words in our documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dETUsA50VAMG",
        "outputId": "48ec00dd-817c-435f-9db0-bc360f933356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "brocolli brother eat good like mother\n"
          ]
        }
      ],
      "source": [
        "# turn our tokenized documents into a id <-> term dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "print(dictionary[0], dictionary[1], dictionary[2], dictionary[3], dictionary[4], dictionary[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1TZI-xtVVzr",
        "outputId": "ea315e23-fa86-4b5c-8b3b-b580f89afee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)]\n",
            "[(1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n"
          ]
        }
      ],
      "source": [
        "# convert tokenized documents into a document-term matrix (each row consisting of (word_id_in_dictionary, word_frequency_in_current_document))\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "print(corpus[0]) # ['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother']\n",
        "print(corpus[1]) # ['mother', 'spend', 'lot', 'time', 'drive', 'brother', 'around', 'basebal', 'practice']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge7YIHUa04jP"
      },
      "source": [
        "Finally we create our LDA model and specify the number of topics that we want/expect and for how many passes LDA should run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XWqLJQAlVXY8"
      },
      "outputs": [],
      "source": [
        "# generate LDA model\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDkXiayA0-AK"
      },
      "source": [
        "We can now access the model’s identified topics and the words that they are comprised of"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gUSNqkjVaVN",
        "outputId": "74f46246-fc9e-4ce7-b044-13bb2415abf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, '0.112*\"good\" + 0.112*\"brocolli\" + 0.082*\"health\" + 0.080*\"eat\"'), (1, '0.075*\"drive\" + 0.053*\"brother\" + 0.053*\"mother\" + 0.053*\"pressur\"')]\n"
          ]
        }
      ],
      "source": [
        "print(ldamodel.print_topics(num_topics=2, num_words=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9oopF1E1AtO"
      },
      "source": [
        "### Wikipedia\n",
        "Next up we are going to also introduce the wikipedia python module which we can use to download wikipedia pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8FPiu5dVeeu",
        "outputId": "4505aba9-70b0-474b-e1c4-ca6cfea8141c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeb1pl8iVdas",
        "outputId": "9b66e6c8-c5c6-4058-d889-0f6da2f9f2b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\bogda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "religion = wikipedia.page(\"Religion\")\n",
        "artificial_intelligence = wikipedia.page(\"Artificial Intelligence\")\n",
        "mona_lisa = wikipedia.page(\"Mona_Lisaa\")\n",
        "eiffel_tower = wikipedia.page(\"Eiffel Tower\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4728U5xRV9xV"
      },
      "outputs": [],
      "source": [
        "corpus = [religion.content, artificial_intelligence.content, mona_lisa.content, eiffel_tower.content]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6-sFG7KV_Y3",
        "outputId": "0bbd20e5-59fb-43ce-ac86-e54262f38a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Mona Lisa (; Italian: Gioconda [dʒoˈkonda] or Monna Lisa [ˈmɔnna ˈliːza]; French: Joconde [ʒɔkɔ̃d]) is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as \"the best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world\". The painting's novel qualities include the subject's enigmatic expression, the monumentality of the composition, the subtle modelling of forms, and the atmospheric illusionism.The painting is probably of the Italian noblewoman Lisa Gherardini, the wife of Francesco del Giocondo. It is painted in oil on a white Lombardy poplar panel. Leonardo never gave the painting to the Giocondo family, and later it is believed he left it in his will to his favored apprentice Salaì. It had been believed to have been painted between 1503 and 1506; however, Leonardo may have continued working on it as late as 1517. It was acquired by King Francis I of France and is now the property of the French Republic. It has been on permanent display at the Louvre in Paris since 1797.The Mona Lisa is one of the most valuable paintings in the world. It holds the Guinness World Record for the highest known painting insurance valuation in history at US$100 million in 1962 (equivalent to $870 million in 2021).\n",
            "\n",
            "\n",
            "== Title and subject ==\n",
            "\n",
            "The title of the painting, which is known in English as Mona Lisa, comes from a description by Renaissance art historian Giorgio Vasari, who wrote \"Leonardo undertook to paint, for Francesco del Giocondo, the portrait of Mona Lisa, his wife.\" Monna in Italian is a polite form of address originating as ma donna – similar to Ma'am, Madam, or my lady in English. This became madonna, and its contraction monna. The title of the painting, though traditionally spelled Mona in English, is spelled in Italian as Monna Lisa (mona being a vulgarity in Italian), but this is rare in English.Vasari's account of the Mona Lisa comes from his biography of Leonardo published in 1550, 31 years after the artist's death. It has long been the best-known source of information on the provenance of the work and identity of the sitter. Leonardo's assistant Salaì, at his death in 1524, owned a portrait which in his personal papers was named la Gioconda, a painting bequeathed to him by Leonardo.\n",
            "\n",
            "That Leonardo painted such a work, and its date, were confirmed in 2005 when a scholar at Heidelberg University discovered a marginal note in a 1477 printing of a volume by ancient Roman philosopher Cicero. Dated October 1503, the note was written by Leonardo's contemporary Agostino Vespucci. This note likens Leonardo to renowned Greek painter Apelles, who is mentioned in the text, and states that Leonardo was at that time working on a painting of Lisa del Giocondo.In response to the announcement of the discovery of this document, Vincent Delieuvin, the Louvre representative, stated \"Leonardo da Vinci was painting, in 1503, the portrait of a Florentine lady by the name of Lisa del Giocondo. About this we are now certain. Unfortunately, we cannot be absolutely certain that this portrait of Lisa del Giocondo is the painting of the Louvre.\"The likely model, Lisa del Giocondo, was a member of the Gherardini family of Florence and Tuscany, and the wife of wealthy Florentine silk merchant Francesco del Giocondo. The painting is thought to have been commissioned for their new home, and to celebrate the birth of their second son, Andrea. The Italian name for the painting, La Gioconda, means 'jocund' ('happy' or 'jovial') or, literally, 'the jocund one', a pun on the feminine form of Lisa's married name, Giocondo. In French, the title La Joconde has the same meaning.\n",
            "Before that discovery, scholars had developed several alternative views as to the subject of the painting. Some argued that Lisa del Giocondo was the subject of a different portrait, identifying at least four other paintings as the Mona Lisa referred to by Vasari. Several other women have been proposed as the subject of the painting. Isabella d'Este, Isabella of Aragon, Cecilia Gallerani, Costanza d'Avalos, Duchess of Francavilla, Pacifica Brandano or Brandino, Isabela Gualanda, Caterina Sforza, Bianca Giovanna Sforza—even Salaì and Leonardo himself—are all among the list of posited models portrayed in the painting. The catalogue raisonné Leonardo da Vinci (2019) documents the opinion that the painting likely depicts Lisa del Giocondo and that Isabella d'Este is the only plausible alternative.\n",
            "\n",
            "\n",
            "== Description ==\n",
            "\n",
            "The Mona Lisa bears a strong resemblance to many Renaissance depictions of the Virgin Mary, who was at that time seen as an ideal for womanhood. The woman sits markedly upright in a \"pozzetto\" armchair with her arms folded, a sign of her reserved posture. Her gaze is fixed on the observer.  The woman appears alive to an unusual extent, which Leonardo achieved by his method of not drawing outlines (sfumato). The soft blending creates an ambiguous mood \"mainly in two features: the corners of the mouth, and the corners of the eyes\".The depiction of the sitter in three-quarter profile is similar to late 15th-century works by Lorenzo di Credi and Agnolo di Domenico del Mazziere. Zöllner notes that the sitter's general position can be traced back to Flemish models and that \"in particular the vertical slices of columns at both sides of the panel had precedents in Flemish portraiture.\" Woods-Marsden cites Hans Memling's portrait of Benedetto Portinari (1487) or Italian imitations such as Sebastiano Mainardi's pendant portraits for the use of a loggia, which has the effect of mediating between the sitter and the distant landscape, a feature missing from Leonardo's earlier portrait of Ginevra de' Benci.\n",
            "The painting was one of the first portraits to depict the sitter in front of an imaginary landscape, and Leonardo was one of the first painters to use aerial perspective. The enigmatic woman is portrayed seated in what appears to be an open loggia with dark pillar bases on either side. Behind her, a vast landscape recedes to icy mountains. Winding paths and a distant bridge give only the slightest indications of human presence. Leonardo has chosen to place the horizon line not at the neck, as he did with Ginevra de' Benci, but on a level with the eyes, thus linking the figure with the landscape and emphasizing the mysterious nature of the painting.Mona Lisa has no clearly visible eyebrows or eyelashes, although Vasari describes the eyebrows in detail. In 2007, French engineer Pascal Cotte announced that his ultra-high resolution scans of the painting provide evidence that Mona Lisa was originally painted with eyelashes and eyebrows, but that these had gradually disappeared over time, perhaps as a result of overcleaning. Cotte discovered the painting had been reworked several times, with changes made to the size of the Mona Lisa's face and the direction of her gaze. He also found that in one layer the subject was depicted wearing numerous hairpins and a headdress adorned with pearls which was later scrubbed out and overpainted.There has been much speculation regarding the painting's model and landscape. For example, Leonardo probably painted his model faithfully since her beauty is not seen as being among the best, \"even when measured by late quattrocento (15th century) or even twenty-first century standards.\" Some art historians in Eastern art, such as Yukio Yashiro, argue that the landscape in the background of the picture was influenced by Chinese paintings, but this thesis has been contested for lack of clear evidence.Research in 2003 by Professor Margaret Livingstone of Harvard University said that Mona Lisa's smile disappears when observed with direct vision, known as foveal. Because of the way the human eye processes visual information, it is less suited to pick up shadows directly; however, peripheral vision can pick up shadows well.Research in 2008 by a geomorphology professor at Urbino University and an artist-photographer revealed likenesses of Mona Lisa's landscapes to some views in the Montefeltro region in the Italian provinces of Pesaro and Urbino, and Rimini.\n",
            "\n",
            "\n",
            "== History ==\n",
            "\n",
            "\n",
            "=== Creation and date ===\n",
            "Of Leonardo da Vinci's works, the Mona Lisa is the only portrait whose authenticity has never been seriously questioned, and one of four works – the others being Saint Jerome in the Wilderness, Adoration of the Magi and The Last Supper – whose attribution has avoided controversy. He had begun working on a portrait of Lisa del Giocondo, the model of the Mona Lisa, by October 1503. It is believed by some that the Mona Lisa was begun in 1503 or 1504 in Florence. Although the Louvre states that it was \"doubtless painted between 1503 and 1506\", art historian Martin Kemp says that there are some difficulties in confirming the dates with certainty. Alessandro Vezzosi believes that the painting is characteristic of Leonardo's style in the final years of his life, post-1513. Other academics argue that, given the historical documentation, Leonardo would have painted the work from 1513. According to Vasari, \"after he had lingered over it four years, [he] left it unfinished\". In 1516, Leonardo was invited by King Francis I to work at the Clos Lucé near the Château d'Amboise; it is believed that he took the Mona Lisa with him and continued to work on it after he moved to France. Art historian Carmen C. Bambach has concluded that Leonardo probably continued refining the work until 1516 or 1517. Leonardo's right hand was paralytic circa 1517, which may indicate why he left the Mona Lisa unfinished.\n",
            "\n",
            "Circa 1505, Raphael executed a pen-and-ink sketch, in which the columns flanking the subject are more apparent. Experts universally agree that it is based on Leonardo's portrait. Other later copies of the Mona Lisa, such as those in the National Museum of Art, Architecture and Design and The Walters Art Museum, also display large flanking columns. As a result, it was thought that the Mona Lisa had been trimmed. However, by 1993, Frank Zöllner observed that the painting surface had never been trimmed; this was confirmed through a series of tests in 2004. In view of this, Vincent Delieuvin, curator of 16th-century Italian painting at the Louvre, states that the sketch and these other copies must have been inspired by another version, while Zöllner states that the sketch may be after another Leonardo portrait of the same subject.The record of an October 1517 visit by Louis d'Aragon states that the Mona Lisa was executed for the deceased Giuliano de' Medici, Leonardo's steward at the Belvedere Palace between 1513 and 1516—but this was likely an error. According to Vasari, the painting was created for the model's husband, Francesco del Giocondo. A number of experts have argued that Leonardo made two versions (because of the uncertainty concerning its dating and commissioner, as well as its fate following Leonardo's death in 1519, and the difference of details in Raphael's sketch—which may be explained by the possibility that he made the sketch from memory). The hypothetical first portrait, displaying prominent columns, would have been commissioned by Giocondo circa 1503, and left unfinished in Leonardo's pupil and assistant Salaì's possession until his death in 1524. The second, commissioned by Giuliano de' Medici circa 1513, would have been sold by Salaì to Francis I in 1518 and is the one in the Louvre today. Others believe that there was only one true Mona Lisa, but are divided as to the two aforementioned fates. At some point in the 16th century, a varnish was applied to the painting. It was kept at the Palace of Fontainebleau until Louis XIV moved it to the Palace of Versailles, where it remained until the French Revolution. In 1797, it went on permanent display at the Louvre.\n",
            "\n",
            "\n",
            "=== Refuge, theft and vandalism ===\n",
            "\n",
            "After the French Revolution, the painting was moved to the Louvre, but spent a brief period in the bedroom of Napoleon (d. 1821) in the Tuileries Palace. The Mona Lisa was not widely known outside the art world, but in the 1860s, a portion of the French intelligentsia began to hail it as a masterwork of Renaissance painting.\n",
            "During the Franco-Prussian War (1870–1871), the painting was moved from the Louvre to the Brest Arsenal.In 1911, the painting was still not popular among the lay-public. On 21 August 1911, the painting was stolen from the Louvre. The painting was first missed the next day by painter Louis Béroud. After some confusion as to whether the painting was being photographed somewhere, the Louvre was closed for a week for investigation. French poet Guillaume Apollinaire came under suspicion and was arrested and imprisoned. Apollinaire implicated his friend Pablo Picasso, who was brought in for questioning. Both were later exonerated. The real culprit was Louvre employee Vincenzo Peruggia, who had helped construct the painting's glass case. He carried out the theft by entering the building during regular hours, hiding in a broom closet, and walking out with the painting hidden under his coat after the museum had closed.\n",
            " \n",
            "Peruggia was an Italian patriot who believed that Leonardo's painting should have been returned to an Italian museum. Peruggia may have been motivated by an associate whose copies of the original would significantly rise in value after the painting's theft. After having kept the Mona Lisa in his apartment for two years, Peruggia grew impatient and was caught when he attempted to sell it to Giovanni Poggi, director of the Uffizi Gallery in Florence. It was exhibited in the Uffizi Gallery for over two weeks and returned to the Louvre on 4 January 1914. Peruggia served six months in prison for the crime and was hailed for his patriotism in Italy. A year after the theft, Saturday Evening Post journalist Karl Decker wrote that he met an alleged accomplice named Eduardo de Valfierno, who claimed to have masterminded the theft. Forger Yves Chaudron was to have created six copies of the painting to sell in the US while concealing the location of the original. Decker published this account of the theft in 1932.During World War II, it was again removed from the Louvre and taken first to the Château d'Amboise, then to the Loc-Dieu Abbey and Château de Chambord, then finally to the Ingres Museum in Montauban.\n",
            "On 30 December 1956, Bolivian Ugo Ungaza Villegas threw a rock at the Mona Lisa while it was on display at the Louvre. He did so with such force that it shattered the glass case and dislodged a speck of pigment near the left elbow. The painting was protected by glass because a few years earlier a man who claimed to be in love with the painting had cut it with a razor blade and tried to steal it.\n",
            "Since then, bulletproof glass has been used to shield the painting from any further attacks. Subsequently, on 21 April 1974, while the painting was on display at the Tokyo National Museum, a woman sprayed it with red paint as a protest against that museum's failure to provide access for disabled people. On 2 August 2009, a Russian woman, distraught over being denied French citizenship, threw a ceramic teacup purchased at the Louvre; the vessel shattered against the glass enclosure. In both cases, the painting was undamaged.\n",
            "In recent decades, the painting has been temporarily moved to accommodate renovations to the Louvre on three occasions: between 1992 and 1995, from 2001 to 2005, and again in 2019. A new queuing system introduced in 2019 reduces the amount of time museum visitors have to wait in line to see the painting. After going through the queue, a group has about 30 seconds to see the painting.\n",
            "\n",
            "\n",
            "=== Modern analysis ===\n",
            "In the early 21st century, French scientist Pascal Cotte hypothesized a hidden portrait underneath the surface of the painting. He analyzed the painting in the Louvre with  reflective light technology beginning in 2004, and produced circumstantial evidence for his theory. Cotte admits that his investigation was only carried out only in support of his hypotheses and should not be considered as definitive proof. The underlying portrait appears to be of a model looking to the side, and lacks flanking columns, but does not fit with historical descriptions of the painting. Both Vasari and Gian Paolo Lomazzo describe the subject as smiling, unlike the subject in Cotte's supposed portrait. In 2020, Cotte published a study alleging that the painting has an underdrawing, transferred from a preparatory drawing via the spolvero technique.\n",
            "\n",
            "\n",
            "== Conservation ==\n",
            "\n",
            "The Mona Lisa has survived for more than 500 years, and an international commission convened in 1952 noted that \"the picture is in a remarkable state of preservation.\" It has never been fully restored, so the current condition is partly due to a variety of conservation treatments the painting has undergone. A detailed analysis in 1933 by Madame de Gironde revealed that earlier restorers had \"acted with a great deal of restraint.\" Nevertheless, applications of varnish made to the painting had darkened even by the end of the 16th century, and an aggressive 1809 cleaning and revarnishing removed some of the uppermost portion of the paint layer, resulting in a washed-out appearance to the face of the figure. Despite the treatments, the Mona Lisa has been well cared for throughout its history, and although the panel's warping caused the curators \"some worry\", the 2004–05 conservation team was optimistic about the future of the work.\n",
            "\n",
            "\n",
            "=== Poplar panel ===\n",
            "At some point, the Mona Lisa was removed from its original frame. The unconstrained poplar panel warped freely with changes in humidity, and as a result, a crack developed near the top of the panel, extending down to the hairline of the figure. In the mid-18th century to early 19th century, two butterfly-shaped walnut braces were inserted into the back of the panel to a depth of about one third the thickness of the panel. This intervention was skilfully executed, and successfully stabilized the crack. Sometime between 1888 and 1905, or perhaps during the picture's theft, the upper brace fell out. A later restorer glued and lined the resulting socket and crack with cloth.The picture is kept under strict, climate-controlled conditions in its bulletproof glass case. The humidity is maintained at 50% ±10%, and the temperature is maintained between 18 and 21 °C. To compensate for fluctuations in relative humidity, the case is supplemented with a bed of silica gel treated to provide 55% relative humidity.\n",
            "\n",
            "\n",
            "=== Frame ===\n",
            "Because the Mona Lisa's poplar support expands and contracts with changes in humidity, the picture has experienced some warping. In response to warping and swelling experienced during its storage during World War II, and to prepare the picture for an exhibit to honour the anniversary of Leonardo's 500th birthday, the Mona Lisa was fitted in 1951 with a flexible oak frame with beech crosspieces. This flexible frame, which is used in addition to the decorative frame described below, exerts pressure on the panel to keep it from warping further. In 1970, the beech crosspieces were switched to maple after it was found that the beechwood had been infested with insects. In 2004–05, a conservation and study team replaced the maple crosspieces with sycamore ones, and an additional metal crosspiece was added for scientific measurement of the panel's warp.The Mona Lisa has had many different decorative frames in its history, owing to changes in taste over the centuries. In 1909, the art collector Comtesse de Béhague gave the portrait its current frame, a Renaissance-era work consistent with the historical period of the Mona Lisa. The edges of the painting have been trimmed at least once in its history to fit the picture into various frames, but no part of the original paint layer has been trimmed.\n",
            "\n",
            "\n",
            "=== Cleaning and touch-up ===\n",
            "The first and most extensive recorded cleaning, revarnishing, and touch-up of the Mona Lisa was an 1809 wash and revarnishing undertaken by Jean-Marie Hooghstoel, who was responsible for restoration of paintings for the galleries of the Musée Napoléon. The work involved cleaning with spirits, touch-up of colour, and revarnishing the painting. In 1906, Louvre restorer Eugène Denizard performed watercolour retouches on areas of the paint layer disturbed by the crack in the panel. Denizard also retouched the edges of the picture with varnish, to mask areas that had been covered initially by an older frame. In 1913, when the painting was recovered after its theft, Denizard was again called upon to work on the Mona Lisa. Denizard was directed to clean the picture without solvent, and to lightly touch up several scratches to the painting with watercolour. In 1952, the varnish layer over the background in the painting was evened out. After the second 1956 attack, restorer Jean-Gabriel Goulinat was directed to touch up the damage to Mona Lisa's left elbow with watercolour.In 1977, a new insect infestation was discovered in the back of the panel as a result of crosspieces installed to keep the painting from warping. This was treated on the spot with carbon tetrachloride, and later with an ethylene oxide treatment. In 1985, the spot was again treated with carbon tetrachloride as a preventive measure.\n",
            "\n",
            "\n",
            "=== Display ===\n",
            "\n",
            "On 6 April 2005—following a period of curatorial maintenance, recording, and analysis—the painting was moved to a new location within the museum's Salle des États. It is displayed in a purpose-built, climate-controlled enclosure behind bulletproof glass. Since 2005 the painting has been illuminated by an LED lamp, and in 2013 a new 20 watt LED lamp was installed, specially designed for this painting. The lamp has a Colour Rendering Index up to 98, and minimizes infrared and ultraviolet radiation which could otherwise degrade the painting. The renovation of the gallery where the painting now resides was financed by the Japanese broadcaster Nippon Television. As of 2019, about 10.2 million people view the painting at the Louvre each year.On the 500th anniversary of the master's death, the Louvre held the largest ever single exhibit of Leonardo works, from 24 October 2019 to 24 February 2020. The Mona Lisa was not included because it is in such great demand among visitors to the museum; the painting remained on display in its gallery.\n",
            "\n",
            "\n",
            "== Legacy ==\n",
            "\n",
            "The Mona Lisa began influencing contemporary Florentine painting even before its completion. Raphael, who had been to Leonardo's workshop several times, promptly used elements of the portrait's composition and format in several of his works, such as Young Woman with Unicorn (c. 1506), and Portrait of Maddalena Doni (c. 1506). Later paintings by Raphael, such as La velata (1515–16) and Portrait of Baldassare Castiglione (c. 1514–15), continued to borrow from Leonardo's painting. Zollner states that \"None of Leonardo's works would exert more influence upon the evolution of the genre than the Mona Lisa.  It became the definitive example of the Renaissance portrait and perhaps for this reason is seen not just as the likeness of a real person, but also as the embodiment of an ideal.\"Early commentators such as Vasari and André Félibien praised the picture for its realism, but by the Victorian era, writers began to regard the Mona Lisa as imbued with a sense of mystery and romance.  In 1859, Théophile Gautier wrote that the Mona Lisa was a \"sphinx of beauty who smiles so mysteriously\" and that \"Beneath the form expressed one feels a thought that is vague, infinite, inexpressible. One is moved, troubled ... repressed desires, hopes that drive one to despair, stir painfully.\" Walter Pater's famous essay of 1869 described the sitter as \"older than the rocks among which she sits; like the vampire, she has been dead many times, and learned the secrets of the grave; and has been a diver in the deep seas, and keeps their fallen day about her.\"By the early 20th century, some critics started to feel the painting had become a repository for subjective exegeses and theories. Upon the painting's theft in 1911, Renaissance historian Bernard Berenson admitted that it had \"simply become an incubus, and [he] was glad to be rid of her.\" Jean Metzinger's Le goûter (Tea Time) was exhibited at the 1911 Salon d'Automne and was sarcastically described as \"la Joconde à la cuiller\" (Mona Lisa with a spoon) by art critic Louis Vauxcelles on the front page of Gil Blas. André Salmon subsequently described the painting as \"The Mona Lisa of Cubism\".The avant-garde art world has made note of the Mona Lisa's undeniable popularity. Because of the painting's overwhelming stature, Dadaists and Surrealists often produce modifications and caricatures. In 1883, Le rire, an image of a Mona Lisa smoking a pipe, by Sapeck (Eugène Bataille), was shown at the \"Incoherents\" show in Paris. In 1919, Marcel Duchamp, one of the most influential modern artists, created L.H.O.O.Q., a Mona Lisa parody made by adorning a cheap reproduction with a moustache and goatee. Duchamp added an inscription, which when read out loud in French sounds like \"Elle a chaud au cul\" meaning: \"she has a hot ass\", implying the woman in the painting is in a state of sexual excitement and intended as a Freudian joke. According to Rhonda R. Shearer, the apparent reproduction is in fact a copy partly modelled on Duchamp's own face.Salvador Dalí, famous for his surrealist work, painted Self portrait as Mona Lisa in 1954. Andy Warhol created serigraph prints of multiple Mona Lisas, called Thirty Are Better than One, following the painting's visit to the United States in 1963. The French urban artist known pseudonymously as Invader has created versions of the Mona Lisa on city walls in Paris and Tokyo using a mosaic style. A 2014 New Yorker magazine cartoon parodies the supposed enigma of the Mona Lisa smile in an animation showing progressively more maniacal smiles.\n",
            "\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\n",
            "\n",
            "=== Fame ===\n",
            "\n",
            "Today the Mona Lisa is considered the most famous painting in the world, a destination painting, but until the 20th century it was simply one among many highly regarded artworks.\n",
            "Once part of King Francis I of France's collection, the Mona Lisa was among the first artworks to be exhibited in the Louvre, which became a national museum after the French Revolution. Leonardo began to be revered as a genius, and the painting's popularity grew in the mid-19th century when French intelligentsia praised it as mysterious and a representation of the femme fatale. The Baedeker guide in 1878 called it \"the most celebrated work of Leonardo in the Louvre\", but the painting was known more by the intelligentsia than the general public.The 1911 theft of the Mona Lisa and its subsequent return was reported worldwide, leading to a massive increase in public recognition of the painting. During the 20th century it was an object for mass reproduction, merchandising, lampooning and speculation, and was claimed to have been reproduced in \"300 paintings and 2,000 advertisements\". The Mona Lisa was regarded as \"just another Leonardo until early last century, when the scandal of the painting's theft from the Louvre and subsequent return kept a spotlight on it over several years.\"\n",
            "\n",
            "From December 1962 to March 1963, the French government lent it to the United States to be displayed in New York City and Washington, D.C. It was shipped on the new ocean liner SS France. In New York, an estimated 1.7 million people queued \"in order to cast a glance at the Mona Lisa for 20 seconds or so.\" While exhibited in the Metropolitan Museum of Art, the painting was nearly drenched in water because of a faulty sprinkler, but the painting's bullet-proof glass case protected it.In 1974, the painting was exhibited in Tokyo and Moscow.In 2014, 9.3 million people visited the Louvre. Former director Henri Loyrette reckoned that \"80 percent of the people only want to see the Mona Lisa.\"\n",
            "\n",
            "\n",
            "=== Financial worth ===\n",
            "Before the 1962–1963 tour, the painting was assessed for insurance at $100 million (equivalent to $670 million in 2020), making it, in practice, the most highly-valued painting in the world. The insurance was not purchased; instead, more was spent on security.In 2014, a France 24 article suggested that the painting could be sold to help ease the national debt, although it was observed that the Mona Lisa and other such art works were prohibited from being sold due to French heritage law, which states that \"Collections held in museums that belong to public bodies are considered public property and cannot be otherwise.\"\n",
            "\n",
            "\n",
            "== Early versions and copies ==\n",
            "\n",
            "\n",
            "=== Prado Museum La Gioconda ===\n",
            "\n",
            "A version of Mona Lisa known as Mujer de mano de Leonardo Abince (\"Woman by Leonardo da Vinci's hand\", Museo del Prado, Madrid) was for centuries considered to be a work by Leonardo. However, since its restoration in 2012, it is now thought to have been executed by one of Leonardo's pupils in his studio at the same time as Mona Lisa was being painted. The Prado's conclusion that the painting is probably by Salaì (1480–1524) or by Melzi (1493–1572) has been called into question by others.The restored painting is from a slightly different perspective than the original Mona Lisa, leading to the speculation that it is part of the world's first stereoscopic pair. However, a more recent report has demonstrated that this stereoscopic pair in fact gives no reliable stereoscopic depth.\n",
            "\n",
            "\n",
            "=== Isleworth Mona Lisa ===\n",
            "\n",
            "A version of the Mona Lisa known as the Isleworth Mona Lisa was first bought by an English nobleman in 1778 and was rediscovered in 1913 by Hugh Blaker, an art connoisseur. The painting was presented to the media in 2012 by the Mona Lisa Foundation. It is a painting of the same subject as Leonardo da Vinci's Mona Lisa. The current scholarly consensus on attribution is unclear. Some experts, including Frank Zöllner, Martin Kemp and Luke Syson denied the attribution to Leonardo; professors such as Salvatore Lorusso, Andrea Natali, and John F Asmus supported it; others like Alessandro Vezzosi and Carlo Pedretti were uncertain.\n",
            "\n",
            "\n",
            "=== Hermitage Mona Lisa ===\n",
            "\n",
            "A version known as the Hermitage Mona Lisa is in the Hermitage Museum. It was made by an unknown 16th-century artist.\n",
            "\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\n",
            "\n",
            "== See also ==\n",
            "\n",
            "List of most expensive paintings\n",
            "List of stolen paintings\n",
            "\n",
            "\n",
            "== References ==\n",
            "Footnotes\n",
            "\n",
            "Citations\n",
            "\n",
            "\n",
            "== Sources ==\n",
            "\n",
            "\n",
            "== External links ==\n",
            "Sassoon, Donald, Prof. (21 January 2014). #26: Why is the Mona Lisa Famous?. La Trobe University podcast blog. Archived from the original on 4 July 2015.{{cite AV media}}:  CS1 maint: bot: original URL status unknown (link)  of the podcast audio.\n",
            "Kobbé, Gustav \"The Smile of the Mona Lisa\" The Lotus Magazine Vol. 8, No. 2 (Nov. 1916), pp. 67–74\n",
            "\"Mona Lisa, Leonardo's Earlier Version\". Zürich, Switzerland: The Mona Lisa Foundation. Archived from the original on 26 June 2015. Retrieved 5 November 2015.\n",
            "\"True Colors of the Mona Lisa Revealed\" (Press release). Paris: Lumiere Technology. 19 October 2006. Archived from the original on 26 June 2015. Retrieved 5 November 2015.\n",
            "Scientific analyses conducted by the Center for Research and Restoration of the Museums of France (C2RMF) Compare layers of the painting as revealed by x-radiography, infrared reflectographya and ultraviolet fluorescence\n",
            "\"Stealing Mona Lisa\". Dorothy & Thomas Hoobler. May 2009. excerpt of book. Vanity Fair\n",
            "Discussion by Janina Ramirez and Martin Kemp: Art Detective Podcast, 18 Jan 2017\n",
            "Leonardo's Mona Lisa, Smarthistory (video)\n",
            "Secrets of the Mona Lisa, Discovery Channel documentary on YouTube\n"
          ]
        }
      ],
      "source": [
        "print(mona_lisa.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYLlGbRRWAf9",
        "outputId": "257c7a64-f4b2-466f-e783-6fef4dfc4154"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\bogda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\bogda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization\n",
        "\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# We are going to create a function for text preprocessing since we are going to be analyzing multiple texts and will want to reuse the code.\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(document):\n",
        "        # Remove all the special characters\n",
        "        document = re.sub(r'\\W', ' ', str(document))\n",
        "\n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "\n",
        "        # Lemmatization\n",
        "        tokens = document.split()\n",
        "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "        tokens = [word for word in tokens if word not in en_stop]\n",
        "        tokens = [word for word in tokens if len(word)  > 5]\n",
        "\n",
        "        return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSnwjVDQWJgO",
        "outputId": "e16104d3-4bcf-42a9-c166-008b64e1eb11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Religion is usually defined as a social-cultural system of designated behaviors and practices, morals, beliefs, worldviews, texts, sanctified places, prophecies, ethics, or organizations, that generally re\n",
            "['religion', 'usually', 'defined', 'social', 'cultural', 'system', 'designated', 'behavior', 'practice', 'belief', 'worldviews', 'sanctified', 'prophecy', 'organization', 'generally']\n"
          ]
        }
      ],
      "source": [
        "processed_data = [];\n",
        "for doc in corpus:\n",
        "    tokens = preprocess_text(doc)\n",
        "    processed_data.append(tokens)\n",
        "\n",
        "print(corpus[0][:205]) # Original text\n",
        "print(processed_data[0][:15]) # Processed tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MsxcWXR1Wni"
      },
      "source": [
        "In this text preprocessing we’ve added the lemmatization step instead of the stemming one (remember the difference discussed earlier)\n",
        "\n",
        "We preprocess all of the documents and add them to the processed_data list of documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKn3xGTaYVaT",
        "outputId": "1e447448-52a4-4f00-fbe9-4abeb1b9cd06"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install gensim==4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sK8G-a8UWKqp"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "\n",
        "gensim_dictionary = corpora.Dictionary(processed_data)\n",
        "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in processed_data]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY6V2k2s1cCe"
      },
      "source": [
        "(Optionally) - we can save our dictionary, corpus and model if we want to load them back later on (and not run it again risking to obtain different values next time we run it)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y7SQyzMyWM8n"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(gensim_corpus, open('gensim_corpus_corpus.pkl', 'wb'))\n",
        "gensim_dictionary.save('gensim_dictionary.gensim')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QhpWyUh5WOtF"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(gensim_corpus, num_topics=4, id2word=gensim_dictionary, passes=150)\n",
        "lda_model.save('gensim_model.gensim')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR_ltiNhWRRn",
        "outputId": "aedc22c8-f97c-4e5b-f8ec-a8e9e12666df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.035*\"painting\" + 0.017*\"leonardo\" + 0.009*\"louvre\" + 0.009*\"portrait\" + 0.006*\"century\" \n",
            "\n",
            "0.058*\"religion\" + 0.019*\"religious\" + 0.009*\"belief\" + 0.008*\"culture\" + 0.007*\"practice\" \n",
            "\n",
            "0.021*\"intelligence\" + 0.016*\"artificial\" + 0.014*\"original\" + 0.014*\"archived\" + 0.013*\"retrieved\" \n",
            "\n",
            "0.025*\"eiffel\" + 0.007*\"second\" + 0.006*\"french\" + 0.006*\"structure\" + 0.006*\"exposition\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic[1], '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFBBzoi0Wb5F",
        "outputId": "474babd0-c7bd-4986-abb6-c73558a0cd08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.000*\"religion\" + 0.000*\"intelligence\" + 0.000*\"original\" + 0.000*\"archived\" + 0.000*\"religious\" \n",
            "\n",
            "0.023*\"intelligence\" + 0.018*\"artificial\" + 0.016*\"original\" + 0.015*\"archived\" + 0.014*\"retrieved\" \n",
            "\n",
            "0.000*\"intelligence\" + 0.000*\"archived\" + 0.000*\"religion\" + 0.000*\"original\" + 0.000*\"artificial\" \n",
            "\n",
            "0.000*\"religion\" + 0.000*\"religious\" + 0.000*\"intelligence\" + 0.000*\"artificial\" + 0.000*\"original\" \n",
            "\n",
            "0.000*\"intelligence\" + 0.000*\"painting\" + 0.000*\"original\" + 0.000*\"artificial\" + 0.000*\"archived\" \n",
            "\n",
            "0.001*\"religion\" + 0.000*\"religious\" + 0.000*\"belief\" + 0.000*\"culture\" + 0.000*\"practice\" \n",
            "\n",
            "0.023*\"painting\" + 0.018*\"eiffel\" + 0.011*\"leonardo\" + 0.008*\"french\" + 0.006*\"second\" \n",
            "\n",
            "0.066*\"religion\" + 0.021*\"religious\" + 0.010*\"belief\" + 0.009*\"culture\" + 0.008*\"practice\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "lda_model_tst = gensim.models.ldamodel.LdaModel(gensim_corpus, num_topics=8, id2word=gensim_dictionary, passes=15)\n",
        "topics = lda_model_tst.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic[1], '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kltKhB6X1oG7"
      },
      "source": [
        "Let’s test our LDA model that was trained on our previous 4 documents (resulting in 3 or 4 topics) on some sentences written by us. We pretend a sentence is a document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8RseIiRUWdxE"
      },
      "outputs": [],
      "source": [
        "test_doc = 'There are many passages about Christ in The Bible'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKG-XvbMWhmB",
        "outputId": "74acfdad-a3c3-407f-b9d3-bd0fd2cb21b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.12511058), (1, 0.6247228), (2, 0.12506433), (3, 0.12510233)]\n"
          ]
        }
      ],
      "source": [
        "# To which topic does the sentence above resemble the most?\n",
        "test_doc = preprocess_text(test_doc)\n",
        "bow_test_doc = gensim_dictionary.doc2bow(test_doc)\n",
        "\n",
        "print(lda_model.get_document_topics(bow_test_doc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHj_I9yIWkiG",
        "outputId": "eb626064-6337-471e-e746-c78649ed0edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.5551714), (1, 0.062584914), (2, 0.062921524), (3, 0.3193222)]\n"
          ]
        }
      ],
      "source": [
        "test_doc = 'Da Vinci created impressive paintings'\n",
        "test_doc = preprocess_text(test_doc)\n",
        "bow_test_doc = gensim_dictionary.doc2bow(test_doc)\n",
        "\n",
        "print(lda_model.get_document_topics(bow_test_doc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYBOA01VWr5D",
        "outputId": "1b6a4ed9-6d0d-4414-9253-8791c806dbe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.050172966), (1, 0.05115974), (2, 0.84851), (3, 0.050157256)]\n"
          ]
        }
      ],
      "source": [
        "test_doc = 'Neural networks are taking over the scene of image recognition'\n",
        "test_doc = preprocess_text(test_doc)\n",
        "bow_test_doc = gensim_dictionary.doc2bow(test_doc)\n",
        "\n",
        "print(lda_model.get_document_topics(bow_test_doc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQhnfkSJ1xFI"
      },
      "source": [
        "There are some metrics that we can print which give us an idea of how well the model is doing (in comparison to models for other documents). These metrics are perplexity and coherence.\n",
        "You can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
        "In order to decide the optimum number of topics to be extracted using LDA, topic coherence score is always used to measure how well the topics are extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C1mRPgxWzcJ",
        "outputId": "ef9b91bb-bbab-4dbe-bec0-2ae61f8d1eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: -7.597080705848661\n"
          ]
        }
      ],
      "source": [
        "print('Perplexity:', lda_model.log_perplexity(gensim_corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score: 0.6459205922312374\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "coherence_score_lda = CoherenceModel(model=lda_model, texts=processed_data, dictionary=gensim_dictionary, coherence='c_v')\n",
        "coherence_score = coherence_score_lda.get_coherence()\n",
        "\n",
        "print('Coherence Score:', coherence_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wIFxAwWW51D",
        "outputId": "b6134ace-771b-42c1-fa80-9c40fb138d17"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMXN559CDls"
      },
      "source": [
        "### Visualizing topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "VDV8Zu9tW4j1",
        "outputId": "942aa08b-190b-4714-d248-ede8095ddec6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Python310\\lib\\site-packages\\pyLDAvis\\_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1423217421855975206494327848\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1423217421855975206494327848_data = {\"mdsDat\": {\"x\": [0.09505639771137397, -0.16676941963228456, -0.031111782552713827, 0.10282480447362441], \"y\": [0.04729973490124641, 0.0890493943009835, -0.18198646452997327, 0.04563733532774342], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [13.660584979420381, 30.32467245507304, 39.98591585336167, 16.028826712144912]}, \"tinfo\": {\"Term\": [\"religion\", \"painting\", \"eiffel\", \"intelligence\", \"religious\", \"artificial\", \"leonardo\", \"archived\", \"retrieved\", \"machine\", \"learning\", \"belief\", \"louvre\", \"portrait\", \"culture\", \"network\", \"french\", \"practice\", \"problem\", \"second\", \"original\", \"algorithm\", \"research\", \"exposition\", \"museum\", \"structure\", \"tallest\", \"neural\", \"century\", \"giocondo\", \"leonardo\", \"painting\", \"portrait\", \"giocondo\", \"italian\", \"louvre\", \"vasari\", \"renaissance\", \"sitter\", \"crosspiece\", \"exhibited\", \"gallery\", \"humidity\", \"peruggia\", \"warping\", \"picture\", \"cleaning\", \"conservation\", \"denizard\", \"earlier\", \"executed\", \"francesco\", \"francis\", \"gioconda\", \"palace\", \"poplar\", \"probably\", \"raphael\", \"restorer\", \"revarnishing\", \"museum\", \"painted\", \"version\", \"subject\", \"landscape\", \"french\", \"believed\", \"century\", \"display\", \"sketch\", \"artist\", \"column\", \"historian\", \"million\", \"several\", \"original\", \"france\", \"english\", \"created\", \"considered\", \"described\", \"religion\", \"religious\", \"judaism\", \"christianity\", \"mythology\", \"sacred\", \"christian\", \"buddhism\", \"western\", \"religio\", \"muslim\", \"health\", \"tradition\", \"hinduism\", \"practice\", \"practiced\", \"adherent\", \"prophet\", \"sometimes\", \"superstition\", \"abrahamic\", \"indian\", \"worship\", \"divine\", \"ethnic\", \"spiritual\", \"ancient\", \"africa\", \"catholic\", \"indigenous\", \"belief\", \"culture\", \"movement\", \"concept\", \"cultural\", \"church\", \"defined\", \"people\", \"country\", \"century\", \"include\", \"social\", \"theory\", \"including\", \"definition\", \"science\", \"philosophy\", \"general\", \"machine\", \"learning\", \"intelligence\", \"network\", \"artificial\", \"algorithm\", \"neural\", \"retrieved\", \"intelligent\", \"archived\", \"computing\", \"program\", \"robotics\", \"optimization\", \"computer\", \"classifier\", \"neuron\", \"processing\", \"application\", \"turing\", \"solution\", \"search\", \"problem\", \"perception\", \"planning\", \"research\", \"researcher\", \"process\", \"automation\", \"bibcode\", \"reasoning\", \"future\", \"original\", \"october\", \"january\", \"knowledge\", \"august\", \"decision\", \"system\", \"november\", \"science\", \"general\", \"example\", \"eiffel\", \"exposition\", \"tallest\", \"restaurant\", \"construction\", \"hydraulic\", \"height\", \"monument\", \"copyright\", \"gustave\", \"minute\", \"engineer\", \"german\", \"structural\", \"transmitter\", \"constructed\", \"edison\", \"mounted\", \"office\", \"passenger\", \"ascent\", \"bottom\", \"chrysler\", \"diameter\", \"exploitation\", \"journey\", \"lattice\", \"located\", \"sauvestre\", \"soci\\u00e9t\\u00e9\", \"completed\", \"visitor\", \"second\", \"replaced\", \"installed\", \"building\", \"structure\", \"aerial\", \"pillar\", \"protest\", \"design\", \"france\", \"french\", \"company\", \"million\", \"public\", \"original\", \"mechanism\", \"drawing\", \"various\"], \"Freq\": [226.0, 63.0, 51.0, 106.0, 72.0, 84.0, 30.0, 74.0, 67.0, 57.0, 44.0, 37.0, 16.0, 15.0, 36.0, 33.0, 23.0, 29.0, 43.0, 20.0, 90.0, 28.0, 34.0, 11.0, 12.0, 16.0, 10.0, 23.0, 39.0, 9.0, 29.769865842595248, 61.237844833286246, 14.961948059708211, 8.791985967152728, 8.174987973299167, 15.579109100925642, 5.090004899069139, 4.4730081219869495, 3.8560105337238464, 3.239012337075057, 3.239012337075057, 3.239012337075057, 3.239012337075057, 3.239012337075057, 3.239012337075057, 6.324060914163954, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 2.622012720859667, 10.026057394684923, 5.70704629110164, 5.090042618981675, 6.941147732327682, 4.47303448536668, 10.026045226971203, 3.2390407284070744, 10.643091080498735, 4.473053953708635, 3.239031602621783, 3.8560174287616222, 3.2390632386774594, 3.23902896628381, 4.473043408356743, 4.473051520165891, 5.706817943674131, 3.856008911362017, 3.2390723644627504, 3.2390577632062847, 3.2390419451784465, 3.239034238959756, 226.0716465566175, 72.10033280049215, 15.826536645522378, 15.044954192597398, 14.263375341098795, 14.263375341098795, 13.481791987817223, 12.700212235962026, 12.700212235962026, 11.918628882680453, 10.355472080039844, 9.57388872675827, 9.57388872675827, 8.792307174189885, 27.55058406242218, 8.010726521978095, 8.010726521978095, 8.010726521978095, 8.010726521978095, 8.010726521978095, 7.229144519231413, 7.229144519231413, 7.229144519231413, 6.447562066306434, 6.447562066306434, 6.447562066306434, 13.481873920267324, 5.6659800635597515, 5.6659800635597515, 5.6659800635597515, 33.80324008439563, 32.24002655928957, 18.17144876955411, 17.389757373481196, 10.355515297156382, 10.355510795373409, 10.355510795373409, 18.95307533995222, 11.918741427254767, 20.516175420127375, 15.045146868908624, 11.137082444019253, 14.263466277114842, 14.263534704216026, 10.355496389667897, 13.481816297445274, 10.355484685032167, 10.355536905714649, 57.145297793317546, 43.94191859543527, 105.00802523539136, 33.21415964110699, 82.72721035135292, 28.262888286685907, 23.31162168108223, 66.22292818098697, 19.185555887308134, 71.99942765293765, 15.059496029555788, 15.059496029555788, 14.234284532887058, 13.409073036218329, 24.96215864603721, 10.108223487930362, 10.108223487930362, 10.108223487930362, 20.836084541832673, 9.283011991261631, 8.457798713786378, 20.836065546563084, 40.641414523612916, 7.632583655504601, 7.632583655504601, 32.38908348573406, 17.53520293902728, 15.059569636225438, 6.8073697844271726, 6.8073697844271726, 15.05956488740804, 11.758704654280931, 73.65032552801479, 34.03977953607183, 23.311806884960703, 21.661265171188322, 19.185806387425814, 14.234332021061025, 29.91363419960635, 17.535261112040388, 26.612603009052954, 20.836181892589305, 14.234438869452452, 50.523715196827375, 11.281802214451126, 9.973739558816655, 8.01164176812585, 7.357609964403727, 7.357609964403727, 6.703578160681603, 6.703578160681603, 4.741478942276135, 4.741478942276135, 4.741478942276135, 9.319794845688941, 4.087445234934462, 4.087445234934462, 4.087445234934462, 3.433411051687902, 3.433411051687902, 3.433411051687902, 3.433411051687902, 3.433411051687902, 2.7793761545840105, 2.7793761545840105, 2.7793761545840105, 2.7793761545840105, 2.7793761545840105, 2.7793761545840105, 2.7793761545840105, 2.7793761545840105, 2.7793761545840105, 2.7793761545840105, 6.049584905255367, 7.357665645275562, 15.206322362966, 4.741519394191571, 6.049586332970029, 7.35772085024251, 11.281933564200072, 4.087479500086361, 4.087479500086361, 4.087479500086361, 8.665795879404055, 8.01173219005447, 11.935916349718784, 7.357638042792088, 8.011775021494344, 6.7036543054636, 7.357458626649508, 4.087499488091635, 4.087498060376973, 4.087495680852535], \"Total\": [226.0, 63.0, 51.0, 106.0, 72.0, 84.0, 30.0, 74.0, 67.0, 57.0, 44.0, 37.0, 16.0, 15.0, 36.0, 33.0, 23.0, 29.0, 43.0, 20.0, 90.0, 28.0, 34.0, 11.0, 12.0, 16.0, 10.0, 23.0, 39.0, 9.0, 30.335071998660343, 63.27983257448096, 15.527154215773306, 9.357192123217823, 8.740194129364262, 16.797867344406633, 5.655211055134234, 5.0382142780520445, 4.421216689788942, 3.804218493140152, 3.804218493140152, 3.804218493140152, 3.804218493140152, 3.804218493140152, 3.804218493140152, 7.542927128566291, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 3.187218876924762, 12.553012154851887, 7.579994589481739, 6.962997223101534, 11.982646806260519, 6.687942580341661, 23.926118375512356, 4.457942933116528, 39.7493265343041, 9.303806903710466, 5.11200583781825, 7.691119079728142, 5.282383246599422, 5.366854106002704, 15.273396786835299, 18.290110346229625, 90.03551112492751, 13.050334692436085, 8.40821342502099, 6.1076748725248935, 13.970061304952745, 14.540176316875979, 226.59570666704803, 72.62439291092268, 16.350596755952907, 15.569014303027927, 14.787435451529324, 14.787435451529324, 14.005852098247752, 13.224272346392555, 13.224272346392555, 12.442688993110982, 10.879532190470373, 10.097948837188799, 10.097948837188799, 9.316367284620414, 29.344833117157794, 8.534786632408624, 8.534786645083324, 8.534786645083324, 8.534786645083324, 8.534786645083324, 7.753204642336643, 7.753204642336643, 7.753204642336643, 6.971622189411663, 6.971622189411663, 6.971622189411663, 14.622605897260218, 6.190040186664981, 6.190040186664981, 6.190040186664981, 37.418701178644945, 36.54657369512559, 22.649280573163573, 22.039175746679682, 12.187325701675725, 12.529266348933897, 12.529266348933897, 29.302991134353157, 15.57140413451085, 39.7493265343041, 27.395510755577384, 15.786529013321031, 27.573566108572198, 27.75082039963309, 15.830166030880811, 42.37388227911256, 19.131024057006982, 34.05081466498152, 57.65845005767928, 44.455070859797004, 106.30175749835277, 33.727311905468724, 84.02103276251837, 28.776040551047647, 23.82477394544397, 67.96960993545773, 19.698708151669873, 74.36310779198041, 15.572648293917526, 15.572648293917526, 14.747436797248795, 13.922225300580067, 26.128942753952867, 10.6213757522921, 10.6213757522921, 10.6213757522921, 21.965894086190463, 9.796164255623369, 8.970950978148116, 22.13013893212925, 43.37063608883105, 8.14573591986634, 8.14573591986634, 34.752900714351405, 18.829289717397785, 16.189402197841687, 7.32052204878891, 7.32052204878891, 16.226388197988804, 12.888550397587046, 90.03551112492751, 39.72613760170958, 27.020406905035898, 25.300187628277875, 22.3672371873237, 16.31008002088383, 43.56413436154603, 21.897944283168123, 42.37388227911256, 34.05081466498152, 21.323960903632624, 51.07966241549067, 11.83774943311442, 10.529686777479949, 8.567588986789143, 7.913557183067019, 7.913557183067019, 7.259525379344896, 7.259525379344896, 5.297426160939428, 5.297426160939428, 5.297426160939428, 10.492394122800697, 4.643392481733898, 4.643392481733898, 4.643392481733898, 3.9893583238367416, 3.9893583238367416, 3.9893583238367416, 3.9893583238367416, 3.9893583238367416, 3.3353234267328506, 3.3353234267328506, 3.3353234267328506, 3.3353234267328506, 3.3353234267328506, 3.3353234267328506, 3.3353234267328506, 3.3353234267328506, 3.3353234267328506, 3.3353234267328506, 7.429927929111885, 9.147319449423474, 20.452093735135165, 5.914163438350061, 7.839251493650747, 10.180022729840765, 16.65693587715647, 5.260129932294555, 5.260129932294555, 5.260129932294555, 13.963774430287511, 13.050334692436085, 23.926118375512356, 12.864185870921252, 15.273396786835299, 20.027548790664014, 90.03551112492751, 6.248830307467715, 6.658147466376837, 13.987275871305584], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.0693, -3.348, -4.7573, -5.2889, -5.3617, -4.7169, -5.8355, -5.9647, -6.1132, -6.2875, -6.2875, -6.2875, -6.2875, -6.2875, -6.2875, -5.6184, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -6.4988, -5.1576, -5.7211, -5.8355, -5.5253, -5.9647, -5.1576, -6.2875, -5.0979, -5.9647, -6.2875, -6.1132, -6.2875, -6.2875, -5.9647, -5.9647, -5.7211, -6.1132, -6.2875, -6.2875, -6.2875, -6.2875, -2.8394, -3.9822, -5.4985, -5.5492, -5.6025, -5.6025, -5.6589, -5.7186, -5.7186, -5.7821, -5.9227, -6.0012, -6.0012, -6.0864, -4.9442, -6.1795, -6.1795, -6.1795, -6.1795, -6.1795, -6.2821, -6.2821, -6.2821, -6.3965, -6.3965, -6.3965, -5.6589, -6.5258, -6.5258, -6.5258, -4.7397, -4.787, -5.3604, -5.4044, -5.9227, -5.9227, -5.9227, -5.3183, -5.7821, -5.239, -5.5492, -5.85, -5.6025, -5.6025, -5.9227, -5.6589, -5.9227, -5.9227, -4.4912, -4.7539, -3.8828, -5.0338, -4.1213, -5.1953, -5.3878, -4.3438, -5.5826, -4.2601, -5.8248, -5.8248, -5.8811, -5.9409, -5.3194, -6.2235, -6.2235, -6.2235, -5.5001, -6.3086, -6.4017, -5.5001, -4.832, -6.5044, -6.5044, -5.059, -5.6726, -5.8248, -6.6188, -6.6188, -5.8248, -6.0722, -4.2375, -5.0093, -5.3878, -5.4613, -5.5826, -5.8811, -5.1385, -5.6726, -5.2554, -5.5001, -5.8811, -3.7002, -5.1995, -5.3227, -5.5418, -5.6269, -5.6269, -5.72, -5.72, -6.0663, -6.0663, -6.0663, -5.3905, -6.2147, -6.2147, -6.2147, -6.3891, -6.3891, -6.3891, -6.3891, -6.3891, -6.6004, -6.6004, -6.6004, -6.6004, -6.6004, -6.6004, -6.6004, -6.6004, -6.6004, -6.6004, -5.8227, -5.6269, -4.901, -6.0663, -5.8227, -5.6269, -5.1995, -6.2147, -6.2147, -6.2147, -5.4633, -5.5418, -5.1431, -5.6269, -5.5417, -5.72, -5.6269, -6.2147, -6.2147, -6.2147], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.9718, 1.9579, 1.9536, 1.9284, 1.9238, 1.9153, 1.8854, 1.8717, 1.8539, 1.8298, 1.8298, 1.8298, 1.8298, 1.8298, 1.8298, 1.8144, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7954, 1.7659, 1.7068, 1.6773, 1.4447, 1.5884, 1.1209, 1.6712, 0.673, 1.2583, 1.5343, 1.3002, 1.5016, 1.4857, 0.7626, 0.5824, -0.7679, 0.7715, 1.0367, 1.3564, 0.529, 0.489, 1.1909, 1.186, 1.1606, 1.159, 1.1571, 1.1571, 1.1551, 1.1528, 1.1528, 1.1502, 1.1438, 1.1399, 1.1399, 1.1353, 1.1301, 1.1298, 1.1298, 1.1298, 1.1298, 1.1298, 1.1232, 1.1232, 1.1232, 1.1151, 1.1151, 1.1151, 1.112, 1.1047, 1.1047, 1.1047, 1.0916, 1.0678, 0.9729, 0.9563, 1.0303, 1.0027, 1.0027, 0.7575, 0.9259, 0.5318, 0.5939, 0.8443, 0.5341, 0.5276, 0.7688, 0.048, 0.5794, 0.0029, 0.9077, 0.905, 0.9044, 0.9013, 0.9011, 0.8986, 0.8949, 0.8906, 0.8902, 0.8843, 0.8831, 0.8831, 0.8812, 0.8791, 0.871, 0.8671, 0.8671, 0.8671, 0.8638, 0.8628, 0.8577, 0.8564, 0.8516, 0.8516, 0.8516, 0.8462, 0.8454, 0.8443, 0.844, 0.844, 0.842, 0.8249, 0.7158, 0.7622, 0.769, 0.7614, 0.7632, 0.7805, 0.5407, 0.6945, 0.4515, 0.4255, 0.5125, 1.8198, 1.7827, 1.7765, 1.7637, 1.7579, 1.7579, 1.7511, 1.7511, 1.7199, 1.7199, 1.7199, 1.7123, 1.7033, 1.7033, 1.7033, 1.6807, 1.6807, 1.6807, 1.6807, 1.6807, 1.6484, 1.6484, 1.6484, 1.6484, 1.6484, 1.6484, 1.6484, 1.6484, 1.6484, 1.6484, 1.6253, 1.6131, 1.5344, 1.6098, 1.5716, 1.5061, 1.4412, 1.5786, 1.5786, 1.5786, 1.3537, 1.3429, 1.1354, 1.2721, 1.1856, 0.7363, -0.6737, 1.4063, 1.3429, 0.6006]}, \"token.table\": {\"Topic\": [2, 2, 1, 4, 2, 3, 1, 2, 1, 3, 1, 3, 2, 3, 1, 4, 4, 1, 2, 3, 4, 3, 1, 2, 3, 1, 4, 3, 4, 2, 1, 3, 4, 2, 1, 2, 3, 4, 2, 2, 4, 2, 3, 3, 1, 1, 3, 4, 3, 4, 3, 4, 3, 4, 3, 2, 3, 1, 1, 2, 3, 4, 4, 4, 4, 2, 3, 4, 1, 3, 4, 1, 2, 4, 2, 3, 4, 2, 3, 2, 3, 2, 3, 1, 1, 2, 3, 4, 1, 3, 4, 4, 1, 3, 4, 2, 1, 2, 4, 1, 4, 4, 1, 4, 1, 2, 3, 4, 2, 1, 2, 3, 4, 1, 1, 4, 4, 1, 2, 4, 1, 1, 1, 2, 4, 1, 3, 1, 1, 2, 3, 4, 4, 1, 1, 4, 2, 4, 2, 1, 2, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 2, 1, 4, 2, 3, 3, 1, 1, 3, 4, 4, 2, 2, 3, 1, 3, 4, 3, 1, 4, 1, 4, 3, 2, 3, 4, 1, 2, 3, 4, 4, 4, 4, 2, 3, 4, 1, 4, 2, 2, 3, 3, 3, 1, 3, 4, 1, 2, 3, 4, 4, 3, 1, 2, 3, 4, 1, 4, 1, 3, 4, 1, 4, 1, 2, 3, 4, 3, 1, 2, 3, 1, 4, 1, 4, 3, 1, 1, 1, 2, 4, 2, 1, 2, 3, 4, 1, 3, 3, 3, 2, 1, 4, 1, 2, 3, 4, 1, 3, 4, 2, 2, 2, 1, 1, 4, 1, 3, 2, 3, 4, 1, 1, 3, 1, 3, 2, 4, 2, 3, 4, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 4, 2, 3, 4, 3, 2, 2, 4, 2, 3, 4, 1, 2, 3, 4, 2, 1, 2, 3, 4, 4, 1, 2, 3, 2, 4, 3, 1, 2, 3, 4, 1, 1, 4, 1, 4, 1, 2, 2], \"Freq\": [0.9028524749335594, 0.9373403615904797, 0.19010937236749653, 0.7604374894699861, 0.9692990383044073, 0.9730317119316336, 0.06838726332543546, 0.889034423230661, 0.04552512163065927, 0.9560275542438447, 0.02689505669390121, 0.9682220409804435, 0.011901781817255859, 0.9878478908322363, 0.5200803626279816, 0.3900602719709862, 0.8994629953889299, 0.04470824857022285, 0.04470824857022285, 0.849456722834234, 0.04470824857022285, 0.9562159574614031, 0.026724604769839136, 0.9086365621745306, 0.08017381430951741, 0.6729561246094089, 0.2243187082031363, 0.9562159574614031, 0.8994629953889299, 0.9830408554423236, 0.09823160778106062, 0.19646321556212124, 0.6876212544674243, 0.9692990383044073, 0.2767342483276259, 0.5283108377163767, 0.12578829469437539, 0.07547297681662524, 0.9281834413792223, 0.9634521304976088, 0.8994629953889299, 0.7981313288028944, 0.15962626576057887, 0.941497620761792, 0.9412594854152587, 0.5679254722631636, 0.18930849075438785, 0.18930849075438785, 0.3886759760912823, 0.5441463665277952, 0.134590807547649, 0.807544845285894, 0.9567934009200554, 0.038271736036802215, 0.9632273019264684, 0.7713537110189405, 0.18149499082798598, 0.9412594854152587, 0.21474494166581953, 0.28632658888775936, 0.35790823610969924, 0.07158164722193984, 0.7520006368128817, 0.8845579602278231, 0.9438545905306808, 0.7706434112389673, 0.19266085280974182, 0.06422028426991394, 0.49118528124268174, 0.3274568541617878, 0.1637284270808939, 0.788598237827208, 0.8205245551634864, 0.08205245551634865, 0.8755950767627777, 0.08208703844651041, 0.027362346148836803, 0.12262355533750603, 0.8583648873625422, 0.7981313288028944, 0.15962626576057887, 0.6317053137972418, 0.3158526568986209, 0.9412594854152587, 0.20632487080078019, 0.20632487080078019, 0.41264974160156037, 0.20632487080078019, 0.07161387524500495, 0.2864555009800198, 0.6445248772050446, 0.8994629953889299, 0.4299315367782143, 0.21496576838910714, 0.3224486525836607, 0.8606318353155539, 0.15019192726654487, 0.15019192726654487, 0.6007677090661795, 0.9412594854152587, 0.7520006368128817, 0.9984404279174228, 0.0953071327950721, 0.8577641951556488, 0.35679398801565393, 0.35679398801565393, 0.11893132933855131, 0.11893132933855131, 0.8606318353155539, 0.04689560276907307, 0.23447801384536535, 0.656538438767023, 0.04689560276907307, 0.9412594854152587, 0.788598237827208, 0.8994629953889299, 0.9292306837673946, 0.30650554903533483, 0.07662638725883371, 0.6130110980706697, 0.9412594854152587, 0.9412594854152587, 0.41795329451494695, 0.08359065890298939, 0.5015439534179363, 0.07758824453890616, 0.9310589344668739, 0.788598237827208, 0.02936787298156535, 0.29367872981565346, 0.6167253326128723, 0.02936787298156535, 0.8614391343689201, 0.9412594854152587, 0.9618269969757777, 0.9438545905306808, 0.9903001254246732, 0.9642503654462993, 0.9660417762680227, 0.5589866876844236, 0.3726577917896157, 0.788598237827208, 0.8845579602278231, 0.03650233094473015, 0.5475349641709523, 0.3650233094473015, 0.03650233094473015, 0.036034970699937274, 0.5044895897991218, 0.32431473629943547, 0.10810491209981182, 0.9028524749335594, 0.9692990383044073, 0.1275631992174165, 0.7653791953044989, 0.009407182190901179, 0.9877541300446238, 0.964530255167487, 0.9153114772499793, 0.037009065167468894, 0.8512084988517845, 0.07401813033493779, 0.8994629953889299, 0.9785575559604415, 0.11857619572144663, 0.869558768623942, 0.5980912592996059, 0.29904562964980297, 0.8994629953889299, 0.9897633531789385, 0.9889543034981048, 0.8994629953889299, 0.9525018665734191, 0.05953136666083869, 0.9885801637570799, 0.1600299497339433, 0.1600299497339433, 0.6401197989357732, 0.26189328122790256, 0.13094664061395128, 0.06547332030697564, 0.5237865624558051, 0.9438545905306808, 0.9642503654462993, 0.7520006368128817, 0.794727229496536, 0.1766060509992302, 0.04415151274980755, 0.7966215500026328, 0.15932431000052658, 0.9191571682428795, 0.9467496947587428, 0.9784355211139494, 0.9653816675309236, 0.941497620761792, 0.045666387084958064, 0.8219949675292452, 0.13699916125487419, 0.075517031886606, 0.025172343962202, 0.855859694714868, 0.025172343962202, 0.7520006368128817, 0.9337587719872884, 0.06664037250451973, 0.03332018625225987, 0.8218979275557434, 0.07774710125527302, 0.7915572932368324, 0.13192621553947206, 0.963972209126856, 0.015802823100440264, 0.015802823100440264, 0.9412594854152587, 0.7520006368128817, 0.10237862702292433, 0.6483979711451874, 0.13650483603056576, 0.10237862702292433, 0.9821089314335726, 0.788598237827208, 0.5227111716655529, 0.41816893733244237, 0.7954471649708805, 0.13257452749514675, 0.19010937236749653, 0.7604374894699861, 0.9821089314335726, 0.9412594854152587, 0.9660495279142783, 0.03407754939370585, 0.9541713830237638, 0.03407754939370585, 0.9373403629824897, 0.9412594854152587, 0.046114149580458806, 0.9453400663994056, 0.023057074790229403, 0.0617688033059872, 0.926532049589808, 0.941497620761792, 0.9632273019264684, 0.9373403615904797, 0.19010937236749653, 0.7604374894699861, 0.14979366827948867, 0.19972489103931823, 0.29958733655897735, 0.34951855931880693, 0.9412594854152587, 0.9244201369383724, 0.06162800912922483, 0.964421758563918, 0.9973710593381924, 0.9914024353816144, 0.7939320916589805, 0.1690856213941529, 0.8454281069707645, 0.05754915298837454, 0.9207864478139927, 0.05310874786084073, 0.9559574614951332, 0.933751608805658, 0.9412594854152587, 0.014712457537266662, 0.9710221974595997, 0.9412594854152587, 0.9493175114072546, 0.9467496947587428, 0.8994629953889299, 0.30679275300691805, 0.6371849485528298, 0.04719888507798739, 0.04518724455670578, 0.9489321356908214, 0.14668424851027476, 0.04889474950342492, 0.04889474950342492, 0.7334212425513738, 0.21869742304887577, 0.2733717788110947, 0.3827204903355326, 0.10934871152443788, 0.9047283317368799, 0.5868537899167127, 0.19561792997223754, 0.6967966163250928, 0.25338058775457917, 0.8994629953889299, 0.8917672183792771, 0.9373403615904797, 0.8606318353155539, 0.8614391343689201, 0.18010515392055013, 0.18010515392055013, 0.6603855643753505, 0.5841781129977679, 0.25036204842761484, 0.08345401614253828, 0.08345401614253828, 0.9373403615904797, 0.02295466246846162, 0.2065919622161546, 0.6886398740538486, 0.09181864987384648, 0.9496958657295675, 0.036266618400480136, 0.5077326576067219, 0.4351994208057617, 0.9903001254246732, 0.8614391343689201, 0.9187269389479316, 0.07149354950891228, 0.4289612970534737, 0.21448064852673684, 0.2859741980356491, 0.8841403002034057, 0.718081573178173, 0.1436163146356346, 0.10932164395581777, 0.7652515076907244, 0.788598237827208, 0.9830408554423236, 0.9028524749335594], \"Term\": [\"abrahamic\", \"adherent\", \"aerial\", \"aerial\", \"africa\", \"algorithm\", \"ancient\", \"ancient\", \"application\", \"application\", \"archived\", \"archived\", \"artificial\", \"artificial\", \"artist\", \"artist\", \"ascent\", \"august\", \"august\", \"august\", \"august\", \"automation\", \"belief\", \"belief\", \"belief\", \"believed\", \"believed\", \"bibcode\", \"bottom\", \"buddhism\", \"building\", \"building\", \"building\", \"catholic\", \"century\", \"century\", \"century\", \"century\", \"christian\", \"christianity\", \"chrysler\", \"church\", \"church\", \"classifier\", \"cleaning\", \"column\", \"column\", \"column\", \"company\", \"company\", \"completed\", \"completed\", \"computer\", \"computer\", \"computing\", \"concept\", \"concept\", \"conservation\", \"considered\", \"considered\", \"considered\", \"considered\", \"constructed\", \"construction\", \"copyright\", \"country\", \"country\", \"country\", \"created\", \"created\", \"created\", \"crosspiece\", \"cultural\", \"cultural\", \"culture\", \"culture\", \"culture\", \"decision\", \"decision\", \"defined\", \"defined\", \"definition\", \"definition\", \"denizard\", \"described\", \"described\", \"described\", \"described\", \"design\", \"design\", \"design\", \"diameter\", \"display\", \"display\", \"display\", \"divine\", \"drawing\", \"drawing\", \"drawing\", \"earlier\", \"edison\", \"eiffel\", \"engineer\", \"engineer\", \"english\", \"english\", \"english\", \"english\", \"ethnic\", \"example\", \"example\", \"example\", \"example\", \"executed\", \"exhibited\", \"exploitation\", \"exposition\", \"france\", \"france\", \"france\", \"francesco\", \"francis\", \"french\", \"french\", \"french\", \"future\", \"future\", \"gallery\", \"general\", \"general\", \"general\", \"general\", \"german\", \"gioconda\", \"giocondo\", \"gustave\", \"health\", \"height\", \"hinduism\", \"historian\", \"historian\", \"humidity\", \"hydraulic\", \"include\", \"include\", \"include\", \"include\", \"including\", \"including\", \"including\", \"including\", \"indian\", \"indigenous\", \"installed\", \"installed\", \"intelligence\", \"intelligence\", \"intelligent\", \"italian\", \"january\", \"january\", \"january\", \"journey\", \"judaism\", \"knowledge\", \"knowledge\", \"landscape\", \"landscape\", \"lattice\", \"learning\", \"leonardo\", \"located\", \"louvre\", \"louvre\", \"machine\", \"mechanism\", \"mechanism\", \"mechanism\", \"million\", \"million\", \"million\", \"million\", \"minute\", \"monument\", \"mounted\", \"movement\", \"movement\", \"movement\", \"museum\", \"museum\", \"muslim\", \"mythology\", \"network\", \"neural\", \"neuron\", \"november\", \"november\", \"november\", \"october\", \"october\", \"october\", \"october\", \"office\", \"optimization\", \"original\", \"original\", \"original\", \"original\", \"painted\", \"painted\", \"painting\", \"painting\", \"painting\", \"palace\", \"passenger\", \"people\", \"people\", \"people\", \"people\", \"perception\", \"peruggia\", \"philosophy\", \"philosophy\", \"picture\", \"picture\", \"pillar\", \"pillar\", \"planning\", \"poplar\", \"portrait\", \"practice\", \"practice\", \"practice\", \"practiced\", \"probably\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"processing\", \"program\", \"prophet\", \"protest\", \"protest\", \"public\", \"public\", \"public\", \"public\", \"raphael\", \"reasoning\", \"reasoning\", \"religio\", \"religion\", \"religious\", \"renaissance\", \"replaced\", \"replaced\", \"research\", \"research\", \"researcher\", \"researcher\", \"restaurant\", \"restorer\", \"retrieved\", \"retrieved\", \"revarnishing\", \"robotics\", \"sacred\", \"sauvestre\", \"science\", \"science\", \"science\", \"search\", \"search\", \"second\", \"second\", \"second\", \"second\", \"several\", \"several\", \"several\", \"several\", \"sitter\", \"sketch\", \"sketch\", \"social\", \"social\", \"soci\\u00e9t\\u00e9\", \"solution\", \"sometimes\", \"spiritual\", \"structural\", \"structure\", \"structure\", \"structure\", \"subject\", \"subject\", \"subject\", \"subject\", \"superstition\", \"system\", \"system\", \"system\", \"system\", \"tallest\", \"theory\", \"theory\", \"theory\", \"tradition\", \"transmitter\", \"turing\", \"various\", \"various\", \"various\", \"various\", \"vasari\", \"version\", \"version\", \"visitor\", \"visitor\", \"warping\", \"western\", \"worship\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1423217421855975206494327848\", ldavis_el1423217421855975206494327848_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1423217421855975206494327848\", ldavis_el1423217421855975206494327848_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1423217421855975206494327848\", ldavis_el1423217421855975206494327848_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If we want to load previously saved model\n",
        "# gensim_dictionary = gensim.corpora.Dictionary.load('gensim_dictionary.gensim')\n",
        "# gensim_corpus = pickle.load(open('gensim_corpus_corpus.pkl', 'rb'))\n",
        "# lda_model = gensim.models.ldamodel.LdaModel.load('gensim_model.gensim')\n",
        "\n",
        "\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "lda_visualization = pyLDAvis.gensim_models.prepare(lda_model, gensim_corpus, gensim_dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_visualization)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnYATp3V18Wy"
      },
      "source": [
        "pyLDAvis is quite powerful of a tool and provides multiple visualizations within one interactive interface.\n",
        "\n",
        "\n",
        "We can see the most relevant terms per topic and the most salient terms overall.\n",
        "We can also play around with the relevance metric to obtain terms that are more or less relevant to a given topic.\n",
        "\n",
        "\n",
        "The visualization is interactive, we can interact with what is being displayed and we can choose for instance what topic we want to further analyze. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zgZD1L5C4Ru"
      },
      "source": [
        "### LSA - Latent Semantic Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MohxfTZhDHmU"
      },
      "source": [
        "As an extra example we also have an LSA model available in our gensim module which may give more coherent topics at the cost of the separation quality between topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHNhXyxlCx6a",
        "outputId": "726c239e-2510-405c-a5b7-f5021b27567e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Python310\\lib\\site-packages\\gensim\\models\\lsimodel.py:932: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(m, n, samples, corpus.indptr, corpus.indices,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.806*\"religion\" + 0.257*\"religious\" + 0.123*\"belief\" + 0.117*\"culture\" + 0.108*\"intelligence\" + 0.098*\"practice\" + 0.087*\"original\" + 0.086*\"artificial\" + 0.079*\"century\" + 0.074*\"science\"\n",
            "-0.407*\"intelligence\" + -0.321*\"artificial\" + -0.285*\"original\" + -0.280*\"archived\" + 0.278*\"religion\" + -0.257*\"retrieved\" + -0.222*\"machine\" + -0.170*\"learning\" + -0.156*\"problem\" + -0.132*\"october\"\n",
            "0.697*\"painting\" + 0.337*\"leonardo\" + 0.178*\"louvre\" + 0.171*\"eiffel\" + 0.168*\"portrait\" + 0.152*\"french\" + 0.124*\"century\" + 0.119*\"museum\" + 0.098*\"giocondo\" + 0.091*\"italian\"\n",
            "-0.654*\"eiffel\" + 0.259*\"painting\" + -0.182*\"second\" + -0.144*\"exposition\" + -0.143*\"structure\" + 0.130*\"leonardo\" + -0.127*\"tallest\" + -0.116*\"engineer\" + -0.110*\"french\" + -0.106*\"design\"\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import LsiModel\n",
        "\n",
        "lsi_model = LsiModel(gensim_corpus, num_topics=4, id2word=gensim_dictionary)\n",
        "topics = lsi_model.print_topics(num_words=10)\n",
        "for topic in topics:\n",
        "    print(topic[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDLgDwPWAHWN"
      },
      "source": [
        "### Exercise 1\n",
        "Choose 8 Wikipedia articles of your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_ids = [\n",
        "    22934, # Probability\n",
        "    27619007, # Mossad\n",
        "    63478112, # 1975 Banqiao Dam Failure\n",
        "    147809, # Mochi\n",
        "    28661, # Defamation\n",
        "    344287, # Mercury Poisoning\n",
        "    4279208, # Peer-to-peer file sharing\n",
        "    1558354, # Demographics of Europe\n",
        "]\n",
        "\n",
        "articles = [wikipedia.page(pageid=id) for id in article_ids]\n",
        "article_content = [article.content for article in articles]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "1. Clean the data using **stemming** and print the resulting topics when applying LDA with num_topics = 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "en_stop = get_stop_words('en')\n",
        "\n",
        "wiki_texts = []\n",
        "for content in article_content:\n",
        "    raw = content.lower()\n",
        "    words = tokenizer.tokenize(raw)\n",
        "    \n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word not in en_stop]\n",
        "    \n",
        "    wiki_texts.append(stemmed_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_dict = corpora.Dictionary(wiki_texts)\n",
        "wiki_corpus = [wiki_dict.doc2bow(text) for text in wiki_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, '0.015*\"defam\" + 0.010*\"law\" + 0.009*\"libel\" + 0.008*\"public\"'), (1, '0.011*\"mercuri\" + 0.008*\"mossad\" + 0.007*\"probabl\" + 0.007*\"s\"')]\n"
          ]
        }
      ],
      "source": [
        "ldamodel = gensim.models.ldamodel.LdaModel(wiki_corpus, num_topics=2, id2word=wiki_dict, passes=20)\n",
        "\n",
        "print(ldamodel.print_topics(num_topics=2, num_words=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "2. Clean the data using **lemmatization** and print the resulting topics when applying LDA with num_topics = 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "wiki_texts = []\n",
        "for content in article_content:\n",
        "    lemmatized_words = preprocess_text(content)\n",
        "    wiki_texts.append(lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_dict = corpora.Dictionary(wiki_texts)\n",
        "wiki_corpus = [wiki_dict.doc2bow(text) for text in wiki_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.014*\"disaster\" + 0.012*\"people\" + 0.012*\"failure\" + 0.012*\"banqiao\"\n",
            "0.044*\"mercury\" + 0.016*\"sharing\" + 0.012*\"poisoning\" + 0.010*\"network\"\n",
            "0.036*\"population\" + 0.032*\"europe\" + 0.030*\"european\" + 0.016*\"country\"\n",
            "0.038*\"defamation\" + 0.017*\"statement\" + 0.016*\"article\" + 0.015*\"criminal\"\n",
            "0.000*\"defamation\" + 0.000*\"mercury\" + 0.000*\"statement\" + 0.000*\"probability\"\n",
            "0.000*\"mercury\" + 0.000*\"defamation\" + 0.000*\"mossad\" + 0.000*\"sharing\"\n",
            "0.034*\"probability\" + 0.014*\"displaystyle\" + 0.010*\"glutinous\" + 0.009*\"theory\"\n",
            "0.039*\"mossad\" + 0.017*\"israel\" + 0.016*\"israeli\" + 0.014*\"operation\"\n"
          ]
        }
      ],
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(wiki_corpus, num_topics=8, id2word=wiki_dict, passes=70)\n",
        "\n",
        "topics = lda_model.print_topics(num_topics=8, num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "3. Print the coherence and perplexity score of your lemmatization LDA (n=2) model.\n",
        "Tune your LDA model hyperparameters (num_topics, num_passes, etc.) until you obtain as good of a coherence score as possible.\n",
        "\n",
        "    Print the **coherence score, perplexity score and the topic separation** of your **new best model**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: -7.516149252318446\n"
          ]
        }
      ],
      "source": [
        "print('Perplexity:', lda_model.log_perplexity(wiki_corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score: 0.5302913887945808\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "coherence_score_lda = CoherenceModel(model=lda_model, texts=wiki_texts, dictionary=wiki_dict, coherence='c_v')\n",
        "coherence_score = coherence_score_lda.get_coherence()\n",
        "\n",
        "print('Coherence Score:', coherence_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "4. Apply LSA on your lemmatization-based cleaned articles and print the resulting topics using (num_topics = 2, num_topics = 4 and num_topics = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Python310\\lib\\site-packages\\gensim\\models\\lsimodel.py:932: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(m, n, samples, corpus.indptr, corpus.indices,\n",
            "C:\\Python310\\lib\\site-packages\\gensim\\models\\lsimodel.py:932: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(m, n, samples, corpus.indptr, corpus.indices,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSA for number of topics: 2\n",
            "0.599*\"defamation\" + 0.270*\"statement\" + 0.258*\"article\" + 0.240*\"criminal\" + 0.217*\"public\" + 0.216*\"person\"\n",
            "0.821*\"mercury\" + 0.215*\"poisoning\" + 0.184*\"mossad\" + 0.174*\"exposure\" + 0.097*\"compound\" + 0.097*\"methylmercury\"\n",
            "LSA for number of topics: 4\n",
            "0.599*\"defamation\" + 0.270*\"statement\" + 0.258*\"article\" + 0.240*\"criminal\" + 0.217*\"public\" + 0.216*\"person\"\n",
            "0.821*\"mercury\" + 0.215*\"poisoning\" + 0.184*\"mossad\" + 0.174*\"exposure\" + 0.097*\"compound\" + 0.097*\"methylmercury\"\n",
            "-0.626*\"mossad\" + 0.271*\"mercury\" + -0.269*\"israel\" + -0.249*\"israeli\" + -0.222*\"operation\" + -0.202*\"intelligence\"\n",
            "0.796*\"probability\" + 0.314*\"displaystyle\" + 0.210*\"theory\" + 0.124*\"outcome\" + 0.104*\"example\" + 0.093*\"number\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Python310\\lib\\site-packages\\gensim\\models\\lsimodel.py:932: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(m, n, samples, corpus.indptr, corpus.indices,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSA for number of topics: 8\n",
            "0.599*\"defamation\" + 0.270*\"statement\" + 0.258*\"article\" + 0.240*\"criminal\" + 0.217*\"public\" + 0.216*\"person\"\n",
            "-0.821*\"mercury\" + -0.215*\"poisoning\" + -0.184*\"mossad\" + -0.174*\"exposure\" + -0.097*\"compound\" + -0.097*\"methylmercury\"\n",
            "-0.626*\"mossad\" + 0.271*\"mercury\" + -0.269*\"israel\" + -0.249*\"israeli\" + -0.222*\"operation\" + -0.202*\"intelligence\"\n",
            "0.796*\"probability\" + 0.314*\"displaystyle\" + 0.210*\"theory\" + 0.124*\"outcome\" + 0.104*\"example\" + 0.093*\"number\"\n",
            "0.488*\"population\" + 0.430*\"europe\" + 0.408*\"european\" + 0.221*\"country\" + 0.199*\"language\" + 0.145*\"sharing\"\n",
            "-0.627*\"sharing\" + -0.376*\"network\" + -0.207*\"community\" + -0.149*\"percent\" + 0.145*\"population\" + 0.132*\"europe\"\n",
            "0.391*\"glutinous\" + 0.282*\"japanese\" + 0.216*\"traditional\" + 0.216*\"amylopectin\" + 0.202*\"starch\" + 0.168*\"called\"\n",
            "0.326*\"disaster\" + 0.290*\"banqiao\" + 0.290*\"failure\" + 0.259*\"people\" + 0.218*\"chinese\" + 0.215*\"august\"\n"
          ]
        }
      ],
      "source": [
        "for num_topics in [2, 4, 8]:\n",
        "    lsi_model = LsiModel(wiki_corpus, num_topics=num_topics, id2word=wiki_dict)\n",
        "    topics = lsi_model.print_topics(num_words=6)\n",
        "    print(f'LSA for number of topics: {num_topics}')\n",
        "    for topic in topics:\n",
        "        print(topic[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2lufLADDz9Z"
      },
      "source": [
        "### Topic Modelling on books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEQEspF1XGCh",
        "outputId": "f42010cc-4063-49e8-b1be-a05ccd5754bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\bogda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# We will try to apply topic modelling on the science books from last laboratories\n",
        "import pandas as pd\n",
        "import gutenbergpy.textget\n",
        "from tidytext import unnest_tokens\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(37729)\n",
        "galileo_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(14725)\n",
        "huygens_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(13476)\n",
        "tesla_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(30155)\n",
        "einstein_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "corpus = [galileo_text, huygens_text, tesla_text, einstein_text]\n",
        "processed_data = [];\n",
        "for doc in corpus:\n",
        "    tokens = preprocess_text(doc)\n",
        "    processed_data.append(tokens)\n",
        "\n",
        "gensim_dictionary = corpora.Dictionary(processed_data)\n",
        "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in processed_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-C6etHaX2J3",
        "outputId": "88206cd6-c4f8-41a8-93d0-c2e7df23061e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, '0.024*\"gravity\" + 0.021*\"figure\" + 0.013*\"therefore\" + 0.011*\"bottom\" + 0.011*\"matter\" + 0.009*\"equall\" + 0.008*\"resistance\" + 0.007*\"proportion\" + 0.007*\"_aristotle_\" + 0.007*\"absolute\" + 0.007*\"greater\" + 0.007*\"without\" + 0.007*\"motion\" + 0.006*\"weight\" + 0.006*\"descend\"') \n",
            "\n",
            "(1, '0.009*\"relativity\" + 0.008*\"theory\" + 0.008*\"frequency\" + 0.007*\"reference\" + 0.006*\"system\" + 0.006*\"current\" + 0.006*\"motion\" + 0.006*\"ordinate\" + 0.006*\"general\" + 0.006*\"result\" + 0.006*\"potential\" + 0.006*\"experiment\" + 0.005*\"distance\" + 0.005*\"energy\" + 0.005*\"velocity\"') \n",
            "\n",
            "(2, '0.000*\"terminate\" + 0.000*\"advancing\" + 0.000*\"mutually\" + 0.000*\"changed\" + 0.000*\"clearer\" + 0.000*\"obscure\" + 0.000*\"regarding\" + 0.000*\"applying\" + 0.000*\"understanding\" + 0.000*\"believed\" + 0.000*\"erected\" + 0.000*\"opposed\" + 0.000*\"opposition\" + 0.000*\"edition\" + 0.000*\"business\"') \n",
            "\n",
            "(3, '0.027*\"refraction\" + 0.019*\"crystal\" + 0.015*\"surface\" + 0.015*\"straight\" + 0.011*\"perpendicular\" + 0.010*\"parallel\" + 0.010*\"movement\" + 0.009*\"matter\" + 0.009*\"particle\" + 0.008*\"spheroid\" + 0.008*\"diameter\" + 0.007*\"another\" + 0.007*\"centre\" + 0.007*\"degree\" + 0.006*\"minute\"') \n",
            "\n"
          ]
        }
      ],
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(gensim_corpus, num_topics=4, id2word=gensim_dictionary, passes=255)\n",
        "\n",
        "topics = lda_model.print_topics(num_words=15)\n",
        "for topic in topics:\n",
        "    print(topic, '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g29BW5BnaXJR",
        "outputId": "3693b094-64fb-4877-f9c9-2b4ac262159b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.99991864)]\n",
            "[(3, 0.99990845)]\n",
            "[(1, 0.99993205)]\n",
            "[(1, 0.99991816)]\n"
          ]
        }
      ],
      "source": [
        "# See which book resembles to which topic\n",
        "\n",
        "print(lda_model[gensim_corpus[0]])\n",
        "print(lda_model[gensim_corpus[1]])\n",
        "print(lda_model[gensim_corpus[2]])\n",
        "print(lda_model[gensim_corpus[3]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRtIv_5_a1ZR",
        "outputId": "d9758c11-c765-4b98-b617-12f258a3f3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1, 0.99623775)]\n"
          ]
        }
      ],
      "source": [
        "# Let's see where some partial text from Tesla's text seems to belong to\n",
        "tesla_text_crumbs = tesla_text[1000:5000]\n",
        "\n",
        "tesla_text_crumbs = preprocess_text(tesla_text_crumbs)\n",
        "tesla_text_crumbs = gensim_dictionary.doc2bow(tesla_text_crumbs)\n",
        "\n",
        "print(lda_model.get_document_topics(tesla_text_crumbs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHd9dfFwE6v6"
      },
      "source": [
        "### Jane Austen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDiK72QEa7XM",
        "outputId": "408cd49b-0d34-4526-a0d7-88f013da80f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\bogda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, '0.008*\"elizabeth\" + 0.008*\"little\" + 0.006*\"nothing\" + 0.006*\"friend\" + 0.006*\"though\" + 0.006*\"harriet\" + 0.006*\"thought\" + 0.006*\"without\" + 0.005*\"always\" + 0.005*\"father\" + 0.005*\"weston\" + 0.005*\"sister\" + 0.004*\"feeling\" + 0.004*\"knightley\" + 0.004*\"indeed\"') \n",
            "\n",
            "(1, '0.021*\"elinor\" + 0.017*\"marianne\" + 0.010*\"sister\" + 0.008*\"mother\" + 0.008*\"edward\" + 0.008*\"dashwood\" + 0.007*\"jennings\" + 0.007*\"willoughby\" + 0.007*\"though\" + 0.006*\"nothing\" + 0.005*\"colonel\" + 0.005*\"without\" + 0.005*\"little\" + 0.005*\"however\" + 0.004*\"brandon\"') \n",
            "\n",
            "(2, '0.000*\"judiciously\" + 0.000*\"bespoke\" + 0.000*\"stealing\" + 0.000*\"abstraction\" + 0.000*\"contraction\" + 0.000*\"resettled\" + 0.000*\"resembling\" + 0.000*\"reseated\" + 0.000*\"reprobate\" + 0.000*\"despondence\" + 0.000*\"bespeak\" + 0.000*\"destruction\" + 0.000*\"weathered\" + 0.000*\"recognition\" + 0.000*\"altering\"') \n",
            "\n",
            "(3, '0.000*\"judiciously\" + 0.000*\"bespoke\" + 0.000*\"stealing\" + 0.000*\"abstraction\" + 0.000*\"contraction\" + 0.000*\"resettled\" + 0.000*\"resembling\" + 0.000*\"reseated\" + 0.000*\"reprobate\" + 0.000*\"despondence\" + 0.000*\"bespeak\" + 0.000*\"destruction\" + 0.000*\"weathered\" + 0.000*\"recognition\" + 0.000*\"altering\"') \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gutenbergpy.textget\n",
        "from tidytext import unnest_tokens\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(161)\n",
        "sense_sensibility_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(1342)\n",
        "pride_prejudice_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(158)\n",
        "emma_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(105)\n",
        "persuasion_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "corpus = [sense_sensibility_text, pride_prejudice_text, emma_text, persuasion_text]\n",
        "\n",
        "processed_data = [];\n",
        "for doc in corpus:\n",
        "    tokens = preprocess_text(doc)\n",
        "    processed_data.append(tokens)\n",
        "    \n",
        "gensim_dictionary = corpora.Dictionary(processed_data)\n",
        "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in processed_data]\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(gensim_corpus, num_topics=4, id2word=gensim_dictionary, passes=255)\n",
        "\n",
        "topics = lda_model.print_topics(num_words=15)\n",
        "for topic in topics:\n",
        "    print(topic, '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhWGN7xBbOLv",
        "outputId": "e36ba122-8ee1-4266-8e23-ab374018cf77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1, 0.99965495)]\n",
            "[(0, 0.9997716)]\n",
            "[(0, 0.9999787)]\n",
            "[(0, 0.9999622)]\n"
          ]
        }
      ],
      "source": [
        "print(lda_model[gensim_corpus[0]])\n",
        "print(lda_model[gensim_corpus[1]])\n",
        "print(lda_model[gensim_corpus[2]])\n",
        "print(lda_model[gensim_corpus[3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58hLmimfE-GK"
      },
      "source": [
        "# Dostoyevsky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "S1XUfGZjbVzd"
      },
      "outputs": [],
      "source": [
        "# Let's add some books from Dostoyevsky to our books corpus\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(2554)\n",
        "crime_and_punishment_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(600)\n",
        "notes_from_underground_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "\n",
        "raw_book = gutenbergpy.textget.get_text_by_id(8117)\n",
        "the_possessed_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N13DpB9YbZKb",
        "outputId": "f4a95acf-85bc-4ce5-b42a-81207834afd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.007*\"little\" + 0.006*\"sister\" + 0.006*\"nothing\" + 0.006*\"though\" + 0.006*\"elizabeth\" + 0.006*\"elinor\" + 0.005*\"without\" + 0.005*\"friend\" + 0.005*\"thought\" + 0.005*\"always\" + 0.005*\"marianne\" + 0.004*\"mother\" + 0.004*\"however\" + 0.004*\"harriet\" + 0.004*\"feeling\" \n",
            "\n",
            "0.010*\"though\" + 0.006*\"suddenly\" + 0.006*\"raskolnikov\" + 0.006*\"nothing\" + 0.006*\"something\" + 0.005*\"without\" + 0.005*\"thought\" + 0.005*\"little\" + 0.005*\"people\" + 0.004*\"looked\" + 0.004*\"perhaps\" + 0.004*\"almost\" + 0.004*\"stepan\" + 0.004*\"trofimovitch\" + 0.004*\"course\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Does the LDA model split the topics as being the 2 authors?\n",
        "authors_corpus = [sense_sensibility_text, pride_prejudice_text, emma_text, persuasion_text,\n",
        "                  crime_and_punishment_text, notes_from_underground_text, the_possessed_text]\n",
        "\n",
        "processed_data = []\n",
        "for doc in authors_corpus:\n",
        "    tokens = preprocess_text(doc)\n",
        "    processed_data.append(tokens)\n",
        "    \n",
        "gensim_dictionary = corpora.Dictionary(processed_data)\n",
        "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in processed_data]\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(gensim_corpus, num_topics=2, id2word=gensim_dictionary, passes=40)\n",
        "\n",
        "topics = lda_model.print_topics(num_words=15)\n",
        "for topic in topics:\n",
        "    print(topic[1], '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5jkbNiXbldc",
        "outputId": "b2f1c602-3834-4cb3-dbb7-d3cdffeacaef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.99997556)]\n",
            "[(0, 0.9999757)]\n",
            "[(0, 0.9999789)]\n",
            "[(0, 0.9999594)]\n",
            "[(1, 0.9999779)]\n",
            "[(1, 0.9993321)]\n",
            "[(1, 0.9999817)]\n"
          ]
        }
      ],
      "source": [
        "# What topic each of the books seems to belong to\n",
        "for g_corpus in gensim_corpus:\n",
        "    print(lda_model[g_corpus])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "ab1_4k_1btHF",
        "outputId": "876d53aa-eeb7-4b0d-a3b4-a49b3f2f2fc3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Python310\\lib\\site-packages\\pyLDAvis\\_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1423217421861712961021579111\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1423217421861712961021579111_data = {\"mdsDat\": {\"x\": [0.10220694004787638, -0.10220694004787638], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [49.830473664045186, 50.169526335954814]}, \"tinfo\": {\"Term\": [\"raskolnikov\", \"elizabeth\", \"elinor\", \"suddenly\", \"marianne\", \"stepan\", \"trofimovitch\", \"stepanovitch\", \"harriet\", \"petrovna\", \"weston\", \"shatov\", \"varvara\", \"nikolay\", \"knightley\", \"ivanovna\", \"simply\", \"razumihin\", \"bennet\", \"dounia\", \"woodhouse\", \"vsyevolodovitch\", \"bingley\", \"elliot\", \"petrovitch\", \"stavrogin\", \"edward\", \"sister\", \"dashwood\", \"kirillov\", \"elizabeth\", \"elinor\", \"marianne\", \"harriet\", \"weston\", \"knightley\", \"bennet\", \"woodhouse\", \"bingley\", \"elliot\", \"edward\", \"dashwood\", \"fairfax\", \"churchill\", \"jennings\", \"wentworth\", \"willoughby\", \"wickham\", \"collins\", \"charles\", \"hartfield\", \"russell\", \"brandon\", \"walter\", \"attachment\", \"musgrove\", \"ferrars\", \"catherine\", \"highbury\", \"middleton\", \"london\", \"affection\", \"colonel\", \"replied\", \"manner\", \"sister\", \"situation\", \"engagement\", \"pleasure\", \"spirit\", \"friend\", \"father\", \"family\", \"little\", \"however\", \"chapter\", \"feeling\", \"nothing\", \"always\", \"without\", \"mother\", \"thought\", \"indeed\", \"enough\", \"though\", \"letter\", \"better\", \"moment\", \"seemed\", \"really\", \"morning\", \"rather\", \"raskolnikov\", \"stepan\", \"trofimovitch\", \"stepanovitch\", \"petrovna\", \"shatov\", \"varvara\", \"nikolay\", \"ivanovna\", \"razumihin\", \"dounia\", \"vsyevolodovitch\", \"petrovitch\", \"stavrogin\", \"kirillov\", \"rouble\", \"liputin\", \"katerina\", \"svidriga\\u00eflov\", \"porfiry\", \"mihailovna\", \"petersburg\", \"police\", \"lebyadkin\", \"lizaveta\", \"shouted\", \"mavriky\", \"anyway\", \"russian\", \"nikolaevitch\", \"simply\", \"suddenly\", \"muttered\", \"everything\", \"though\", \"something\", \"course\", \"people\", \"understand\", \"looked\", \"turned\", \"almost\", \"anything\", \"perhaps\", \"nothing\", \"looking\", \"without\", \"thought\", \"another\", \"little\", \"moment\", \"believe\", \"always\", \"seemed\"], \"Freq\": [743.0, 692.0, 647.0, 782.0, 536.0, 496.0, 491.0, 486.0, 481.0, 481.0, 426.0, 414.0, 404.0, 391.0, 368.0, 333.0, 367.0, 329.0, 316.0, 308.0, 297.0, 297.0, 291.0, 274.0, 273.0, 272.0, 254.0, 913.0, 239.0, 239.0, 692.4983374835181, 647.1196920101095, 535.5607976951804, 480.725948402688, 425.8921106911219, 368.222510902837, 316.225556209016, 297.3166754340777, 290.69935045456657, 273.68204399175045, 253.82840131168987, 238.70195832034508, 228.30233225736558, 226.41118171563141, 222.6298375330393, 206.55788078588375, 204.66707199446253, 183.8680795987414, 175.35944003734585, 164.01435489860566, 151.72408408435516, 140.3792313358278, 136.59762742299785, 133.76129120575334, 133.7611818456532, 123.36196588304924, 123.36193854302421, 120.52569801586732, 118.63464316422078, 114.85333999166623, 141.3260300728152, 215.1050287725416, 286.9302546031903, 298.2718812287635, 412.6790593518775, 725.5957890309878, 186.70645264785102, 166.8527279477153, 316.2477836493698, 263.2969628220898, 620.6869183655122, 465.6330516220616, 397.55086659893044, 834.3407932892263, 481.734631286343, 338.92332701345623, 468.48494431355124, 721.8546893652477, 557.3464779244157, 630.1410443426935, 485.4652323824695, 605.5642209575972, 435.3838565427518, 387.15293561750315, 696.3303153924757, 398.4938240624004, 407.01978352965585, 391.8762256826389, 369.18714900616, 349.34370415543754, 321.9072235117417, 317.18592926843354, 743.0803066265372, 496.1713635073206, 491.43985573300955, 485.7643145503503, 481.0347335995342, 413.8673181269845, 404.40774333460337, 391.1634467812521, 333.4564043565315, 328.7263004107667, 307.9140742682212, 297.5074932540997, 272.91093071528303, 271.96508058763965, 238.8547108059354, 232.23260381833467, 223.71843873679805, 204.7982293991135, 196.28425699992638, 195.33815913783363, 193.44603222877296, 190.60779369459456, 145.1991151174321, 136.6852115333698, 136.68511519219504, 131.95519016575483, 131.0088858582876, 128.17067485015914, 125.33293178487948, 125.33289049580458, 362.7855481856065, 744.0347999338582, 150.8771198815599, 396.83739145301547, 1160.2332441771882, 679.6863822865869, 489.53824609974845, 544.4129130389255, 421.4318895380875, 525.4926073600661, 361.8349635773754, 499.94672641131507, 379.80685920594794, 500.8809605458886, 726.957198141413, 375.0778287761304, 588.8468391360195, 564.2462577939134, 436.55679334860434, 549.1233358589913, 443.1802215866017, 390.2064210773377, 416.6688369105583, 387.3641912659197], \"Total\": [743.0, 692.0, 647.0, 782.0, 536.0, 496.0, 491.0, 486.0, 481.0, 481.0, 426.0, 414.0, 404.0, 391.0, 368.0, 333.0, 367.0, 329.0, 316.0, 308.0, 297.0, 297.0, 291.0, 274.0, 273.0, 272.0, 254.0, 913.0, 239.0, 239.0, 692.9740266573613, 647.5942014346565, 536.0355384030611, 481.2015025268488, 426.36752858284024, 368.69734816693966, 316.69966360270104, 297.7913820569668, 291.1735053736972, 274.15606740947845, 254.302408600873, 239.1758085175011, 228.77625729096982, 226.88540631007592, 223.10374977133128, 207.03174920829548, 205.14089912779937, 184.34185495944917, 175.83311776659528, 164.4881499899, 152.1977933180289, 140.8528283148292, 137.07118155383168, 134.234949849558, 134.23494290571784, 123.83537689992816, 123.83539423597246, 120.99917601996933, 119.10834164553128, 115.32668364498939, 142.73077192490385, 222.1484937800847, 305.3521632997753, 326.1572889002226, 466.09485596711227, 913.3616006486137, 198.51624459258122, 173.93247357934777, 372.500820979473, 301.5832587764696, 861.3979034004764, 614.5861006136188, 521.9198636413635, 1383.4641291482176, 672.2830512912418, 427.3554005482013, 683.6437999864306, 1448.8118875066607, 974.015314834974, 1218.987883478713, 806.6156206495627, 1169.8104787515106, 684.6111261650972, 581.5258249994688, 1856.563559569664, 651.5240927428683, 711.1179765483878, 835.0564472692406, 756.5513402720796, 669.5301852061343, 533.3208022531985, 572.1106783352311, 743.5582274050696, 496.64675431896603, 491.91661461387645, 486.24052412098763, 481.5104287291536, 414.34291527038135, 404.88269312814964, 391.6383674192248, 333.9311517164158, 329.2009999256072, 308.3885329558881, 297.9822701967455, 273.3857821488173, 272.4397281991514, 239.3289667929479, 232.70682082445208, 224.19264052434684, 205.27222715104577, 196.75801130815918, 195.8119854272261, 193.91993439741458, 191.08188051520216, 145.6728619498768, 137.1586956986599, 137.1586906688969, 132.4285712185677, 131.48256065617417, 128.64448762973453, 125.80642385314557, 125.80642448188249, 367.9708983438525, 782.306880932257, 152.2810145747682, 495.62342933678843, 1856.563559569664, 985.5291538880451, 663.0211625087541, 769.888163677521, 566.5452239189525, 779.3300855600678, 475.74727357733667, 775.5320189058779, 527.7565708140951, 802.9494963712641, 1448.8118875066607, 534.371395221891, 1218.987883478713, 1169.8104787515106, 734.8384349663055, 1383.4641291482176, 835.0564472692406, 699.8283769994104, 974.015314834974, 756.5513402720796], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.1332, -5.201, -5.3902, -5.4983, -5.6194, -5.7649, -5.9171, -5.9788, -6.0013, -6.0616, -6.1369, -6.1983, -6.2429, -6.2512, -6.268, -6.343, -6.3522, -6.4593, -6.5067, -6.5736, -6.6515, -6.7292, -6.7565, -6.7775, -6.7775, -6.8584, -6.8584, -6.8817, -6.8975, -6.9299, -6.7225, -6.3024, -6.0143, -5.9755, -5.6509, -5.0866, -6.444, -6.5564, -5.917, -6.1003, -5.2427, -5.5302, -5.6882, -4.9469, -5.4962, -5.8478, -5.524, -5.0917, -5.3504, -5.2276, -5.4884, -5.2674, -5.5973, -5.7147, -5.1277, -5.6859, -5.6647, -5.7026, -5.7622, -5.8175, -5.8993, -5.9141, -5.0695, -5.4734, -5.483, -5.4946, -5.5044, -5.6548, -5.6779, -5.7112, -5.8708, -5.8851, -5.9505, -5.9849, -6.0712, -6.0747, -6.2045, -6.2326, -6.2699, -6.3583, -6.4008, -6.4056, -6.4153, -6.4301, -6.7022, -6.7627, -6.7627, -6.7979, -6.8051, -6.827, -6.8494, -6.8494, -5.7865, -5.0682, -6.6639, -5.6968, -4.624, -5.1587, -5.4869, -5.3806, -5.6367, -5.416, -5.7891, -5.4658, -5.7407, -5.464, -5.0915, -5.7532, -5.3022, -5.3448, -5.6014, -5.372, -5.5864, -5.7137, -5.648, -5.721], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6959, 0.6958, 0.6957, 0.6956, 0.6954, 0.6953, 0.695, 0.6949, 0.6949, 0.6948, 0.6947, 0.6946, 0.6945, 0.6945, 0.6944, 0.6943, 0.6942, 0.694, 0.6938, 0.6937, 0.6934, 0.6932, 0.6931, 0.693, 0.693, 0.6927, 0.6927, 0.6926, 0.6926, 0.6924, 0.6867, 0.6643, 0.6343, 0.6072, 0.5748, 0.4664, 0.6352, 0.655, 0.5328, 0.5608, 0.3688, 0.419, 0.4244, 0.1908, 0.3633, 0.4647, 0.3186, -0.0001, 0.1383, 0.0367, 0.1888, 0.0381, 0.2439, 0.2897, -0.2841, 0.2049, 0.1386, -0.06, -0.0209, 0.046, 0.1917, 0.1067, 0.6891, 0.6888, 0.6888, 0.6888, 0.6888, 0.6886, 0.6886, 0.6885, 0.6883, 0.6883, 0.6882, 0.6882, 0.688, 0.688, 0.6878, 0.6877, 0.6876, 0.6875, 0.6874, 0.6873, 0.6873, 0.6873, 0.6865, 0.6863, 0.6863, 0.6862, 0.6862, 0.6861, 0.686, 0.686, 0.6756, 0.6396, 0.6805, 0.4675, 0.2197, 0.3182, 0.3864, 0.3432, 0.3939, 0.2957, 0.4161, 0.2507, 0.3608, 0.2178, 0.0001, 0.3358, -0.0378, -0.0393, 0.169, -0.2343, 0.0562, 0.1056, -0.1594, 0.0204]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1], \"Freq\": [0.9678211017394458, 0.03151045447523777, 0.3558847259322463, 0.644718706398997, 0.5718595914422266, 0.42812468515513197, 0.4055313192942394, 0.5946885454079953, 0.28043232085524095, 0.7200289319256187, 0.9949901652094919, 0.9982497634324405, 0.4429657473010146, 0.5572794885399861, 0.9977907661955121, 0.5723382243484965, 0.4274958727320465, 0.9994041168908053, 0.999480696430681, 1.0000068097986927, 0.7932507687164803, 0.20591760367861434, 0.9970323090755779, 0.9960975616524852, 0.9952618836702811, 0.9398983681613602, 0.05894832971046858, 0.26092681468174983, 0.7390412670176729, 0.9992649402187, 0.9987401186673057, 0.9988108307642983, 0.9990824478765559, 0.9985944254475747, 0.9994307351613512, 0.9601427298957765, 0.040245503648325964, 0.6654906512541442, 0.3336051326700361, 0.1997484262042968, 0.8010113656879377, 0.996606914982517, 0.7625691753197674, 0.23758436618002804, 0.758233874040974, 0.24243958633498955, 0.6845670216116773, 0.31449125992844146, 0.993253994618206, 0.7209211881623168, 0.2797777879985803, 0.9995812512517299, 0.9987004192786448, 0.9990903941400368, 0.7169599160267858, 0.2841065227409047, 0.6353972107299608, 0.3637101275212879, 0.9972115458182633, 0.9995349707414707, 0.9986738237567547, 0.9986254618596482, 0.9981086162664129, 0.9988429774878542, 0.6108753374329556, 0.3883202521872809, 0.9991407366276774, 0.6028345675384326, 0.3968299491350114, 0.9988430141165464, 0.9878738697930222, 0.007006197658106539, 0.3259209476270407, 0.6736555019850251, 0.29754586682915024, 0.7017591198800713, 0.8860857285005981, 0.11371075934753437, 0.9999337014050095, 0.996329850485373, 0.9971673195251585, 0.9952561122698748, 0.46942934370712136, 0.5305030593424866, 0.6037641859076178, 0.3956342957344949, 0.601277718387641, 0.3979590672215108, 0.9932541336664785, 0.006566806786731853, 0.9915878247965099, 0.9935899578642057, 0.9983700079656868, 0.4983393677439582, 0.5017904713986948, 0.29225023921038557, 0.70659613391311, 0.3761133189133512, 0.6239495787271158, 0.9995714899027507, 0.998588872670023, 0.9989399425252309, 0.8483202779770879, 0.15033523913518013, 0.9953810068610562, 0.9958532393946444, 0.9992492485665612, 0.5540885916033406, 0.44571795223612565, 0.9993894309991384, 0.5212610390262692, 0.47794708449400036, 0.9136696009610368, 0.08584815042586923, 0.996962612346523, 0.9939452524664752, 0.993589962829824, 0.4877395364434831, 0.5115317089529213, 0.99917238775482, 0.9967637556259641, 0.01358803107121727, 0.9864910557703739, 0.7948659101547942, 0.20583304560482274, 0.941988401925413, 0.06044845359949175, 0.3104930978376325, 0.6899846618614056, 0.8720643216967586, 0.12600168906645182, 0.9983859615407121, 0.998697757886583, 0.9995053392116536, 0.048574288333903294, 0.9510334347480013, 0.9961474945639087, 0.3748861688103622, 0.6248102813506037, 0.5180326309324552, 0.48212937928367117, 0.9981366463610993, 0.23962302325515766, 0.7609081966523428, 0.2559372030303146, 0.7431004308673272, 0.9978198793301588, 1.0000594995240581, 0.9982497117939753, 0.9998466457032948, 0.9991380005319311, 0.9981455380302842, 0.9993131592559141, 0.5168222002355953, 0.48318773958534234, 0.9973424951000918], \"Term\": [\"affection\", \"affection\", \"almost\", \"almost\", \"always\", \"always\", \"another\", \"another\", \"anything\", \"anything\", \"anyway\", \"attachment\", \"believe\", \"believe\", \"bennet\", \"better\", \"better\", \"bingley\", \"brandon\", \"catherine\", \"chapter\", \"chapter\", \"charles\", \"churchill\", \"collins\", \"colonel\", \"colonel\", \"course\", \"course\", \"dashwood\", \"dounia\", \"edward\", \"elinor\", \"elizabeth\", \"elliot\", \"engagement\", \"engagement\", \"enough\", \"enough\", \"everything\", \"everything\", \"fairfax\", \"family\", \"family\", \"father\", \"father\", \"feeling\", \"feeling\", \"ferrars\", \"friend\", \"friend\", \"harriet\", \"hartfield\", \"highbury\", \"however\", \"however\", \"indeed\", \"indeed\", \"ivanovna\", \"jennings\", \"katerina\", \"kirillov\", \"knightley\", \"lebyadkin\", \"letter\", \"letter\", \"liputin\", \"little\", \"little\", \"lizaveta\", \"london\", \"london\", \"looked\", \"looked\", \"looking\", \"looking\", \"manner\", \"manner\", \"marianne\", \"mavriky\", \"middleton\", \"mihailovna\", \"moment\", \"moment\", \"morning\", \"morning\", \"mother\", \"mother\", \"musgrove\", \"muttered\", \"muttered\", \"nikolaevitch\", \"nikolay\", \"nothing\", \"nothing\", \"people\", \"people\", \"perhaps\", \"perhaps\", \"petersburg\", \"petrovitch\", \"petrovna\", \"pleasure\", \"pleasure\", \"police\", \"porfiry\", \"raskolnikov\", \"rather\", \"rather\", \"razumihin\", \"really\", \"really\", \"replied\", \"replied\", \"rouble\", \"russell\", \"russian\", \"seemed\", \"seemed\", \"shatov\", \"shouted\", \"simply\", \"simply\", \"sister\", \"sister\", \"situation\", \"situation\", \"something\", \"something\", \"spirit\", \"spirit\", \"stavrogin\", \"stepan\", \"stepanovitch\", \"suddenly\", \"suddenly\", \"svidriga\\u00eflov\", \"though\", \"though\", \"thought\", \"thought\", \"trofimovitch\", \"turned\", \"turned\", \"understand\", \"understand\", \"varvara\", \"vsyevolodovitch\", \"walter\", \"wentworth\", \"weston\", \"wickham\", \"willoughby\", \"without\", \"without\", \"woodhouse\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1423217421861712961021579111\", ldavis_el1423217421861712961021579111_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1423217421861712961021579111\", ldavis_el1423217421861712961021579111_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1423217421861712961021579111\", ldavis_el1423217421861712961021579111_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyLDAvis.gensim_models\n",
        "\n",
        "lda_visualization = pyLDAvis.gensim_models.prepare(lda_model, gensim_corpus, gensim_dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_visualization)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2 - Visualization basics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Use the Jane Austen vs Dostoyevsky topic visualization\n",
        "1. Inspect the conditional topic distribution given for the words 'elizabeth', 'sister' and 'suddenly'. (hover over the words to see their cond. topic distribution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* elizabeth - only appears in Jane Austen\n",
        "* sister - appears in both Jane Austen and Dostoyevsky, but more relevant in Jane Austen\n",
        "* suddenly - appears significantly more in Dostoyevsky than in Jane Austen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "2. Show the most relevant 30 terms of each topic when lambda = 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "3. Answer the following questions based on visual observations:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        ">*   The term 'father' is more relevant for which author?\n",
        ">*   In the books of which author do things happen 'suddenly' more often? \n",
        ">*   For which author does 'family' appear more often?\n",
        ">*   In the books of which author do 'letter(s)' play a bigger role?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* father - more relavant in Jane Austen\n",
        "* suddenly - significantly more in Dostoyevsky\n",
        "* family - a lot more in Jane Austen\n",
        "* letter(s) - more in Jane Austen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4SAMwYvKC2N"
      },
      "source": [
        "### Exercise 3 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Choose books from at least 3 different authors and create your clean corpus of books using lemmatization.\n",
        "For one book of your choice (out of the >=3) split the book into chapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "books = [\n",
        "    {'id': 996, 'name': 'Don Quixote'},\n",
        "    {'id': 8486, 'name': 'Ghost Stories of an Antiquary'},\n",
        "    {'id': 2600, 'name': 'War and Peace'},\n",
        "    {'id': 164, 'name': 'Twenty Thousand Leagues under the Sea'}\n",
        "]\n",
        "\n",
        "def process_book(book_id):\n",
        "    raw_book = gutenbergpy.textget.get_text_by_id(book_id)\n",
        "    the_possessed_text = gutenbergpy.textget.strip_headers(raw_book).decode(\"utf-8\")\n",
        "    \n",
        "    return the_possessed_text\n",
        "\n",
        "author_content = [process_book(book['id']) for book in books]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "author_texts = []\n",
        "for content in author_content:\n",
        "    lemmatized_words = preprocess_text(content)\n",
        "    author_texts.append(lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "author_dict = corpora.Dictionary(author_texts)\n",
        "author_corpus = [author_dict.doc2bow(text) for text in author_texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "1. Train (use) an LDA Model that correctly separates everything into N distinct topics, where N is the number of authors you have. E.g. If you have 4 different authors your model should separate your corpus into 4 different topics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.013*\"pierre\" + 0.013*\"prince\" + 0.008*\"natásha\" + 0.008*\"andrew\" + 0.006*\"princess\" + 0.006*\"thought\" + 0.006*\"french\" + 0.005*\"rostóv\"\n",
            "0.000*\"prince\" + 0.000*\"pierre\" + 0.000*\"natásha\" + 0.000*\"andrew\" + 0.000*\"princess\" + 0.000*\"thought\" + 0.000*\"chapter\" + 0.000*\"without\"\n",
            "0.024*\"quixote\" + 0.023*\"sancho\" + 0.009*\"knight\" + 0.007*\"without\" + 0.007*\"master\" + 0.006*\"though\" + 0.006*\"worship\" + 0.004*\"replied\"\n",
            "0.015*\"captain\" + 0.012*\"nautilus\" + 0.007*\"conseil\" + 0.004*\"seemed\" + 0.004*\"without\" + 0.004*\"surface\" + 0.004*\"thought\" + 0.004*\"little\"\n"
          ]
        }
      ],
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(author_corpus, num_topics=4, id2word=author_dict, passes=70)\n",
        "\n",
        "topics = lda_model.print_topics(num_words=8)\n",
        "for topic in topics:\n",
        "    print(topic[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "2. Separate the text into chapters for one book of your choice out of the ones chosen. Print the similitude of each chapter to all of the authors that you've chosen.\n",
        "Your output should look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def split_into_chapters(book_text):\n",
        "    return re.split(r'CHAPTER [IVXLCD]+\\.', book_text)\n",
        "    \n",
        "chapters = split_into_chapters(author_content[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chpater 1: [(2, 0.9998765)]\n",
            "Chpater 2: [(2, 0.99835914)]\n",
            "Chpater 3: [(2, 0.99862164)]\n",
            "Chpater 4: [(2, 0.9987401)]\n",
            "Chpater 5: [(2, 0.9985635)]\n",
            "Chpater 6: [(2, 0.9976819)]\n",
            "Chpater 7: [(2, 0.9986231)]\n",
            "Chpater 8: [(2, 0.9981378)]\n",
            "Chpater 9: [(2, 0.99894243)]\n",
            "Chpater 10: [(2, 0.99841154)]\n",
            "Chpater 11: [(2, 0.9982223)]\n",
            "Chpater 12: [(2, 0.9984106)]\n",
            "Chpater 13: [(2, 0.99837875)]\n",
            "Chpater 14: [(2, 0.9990796)]\n",
            "Chpater 15: [(2, 0.99881226)]\n",
            "Chpater 16: [(2, 0.99882686)]\n",
            "Chpater 17: [(2, 0.9988228)]\n",
            "Chpater 18: [(2, 0.9989061)]\n",
            "Chpater 19: [(2, 0.9990989)]\n",
            "Chpater 20: [(2, 0.9987935)]\n",
            "Chpater 21: [(2, 0.99927396)]\n",
            "Chpater 22: [(2, 0.9991786)]\n",
            "Chpater 23: [(2, 0.99909425)]\n",
            "Chpater 24: [(2, 0.999212)]\n",
            "Chpater 25: [(2, 0.9990816)]\n",
            "Chpater 26: [(2, 0.9994252)]\n",
            "Chpater 27: [(2, 0.99881923)]\n",
            "Chpater 28: [(2, 0.99946773)]\n",
            "Chpater 29: [(2, 0.99938834)]\n",
            "Chpater 30: [(2, 0.9992614)]\n",
            "Chpater 31: [(2, 0.9991401)]\n",
            "Chpater 32: [(2, 0.99889714)]\n",
            "Chpater 33: [(2, 0.998479)]\n",
            "Chpater 34: [(2, 0.999574)]\n",
            "Chpater 35: [(2, 0.9995663)]\n",
            "Chpater 36: [(2, 0.99898964)]\n",
            "Chpater 37: [(2, 0.99902797)]\n",
            "Chpater 38: [(2, 0.99915004)]\n",
            "Chpater 39: [(0, 0.07662589), (2, 0.91994786)]\n",
            "Chpater 40: [(0, 0.021737447), (2, 0.966782), (3, 0.011149725)]\n",
            "Chpater 41: [(2, 0.9992358)]\n",
            "Chpater 42: [(2, 0.99627906)]\n",
            "Chpater 43: [(2, 0.97691464), (3, 0.022307754)]\n",
            "Chpater 44: [(2, 0.99897975)]\n",
            "Chpater 45: [(2, 0.9989372)]\n",
            "Chpater 46: [(2, 0.99895006)]\n",
            "Chpater 47: [(2, 0.99900806)]\n",
            "Chpater 48: [(2, 0.99913716)]\n",
            "Chpater 49: [(2, 0.9987683)]\n",
            "Chpater 50: [(0, 0.016313456), (2, 0.9828711)]\n",
            "Chpater 51: [(2, 0.99859613)]\n",
            "Chpater 52: [(2, 0.9984072)]\n",
            "Chpater 53: [(2, 0.9994455)]\n",
            "Chpater 54: [(2, 0.99921566)]\n",
            "Chpater 55: [(2, 0.9981325)]\n",
            "Chpater 56: [(2, 0.99884844)]\n",
            "Chpater 57: [(2, 0.9982362)]\n",
            "Chpater 58: [(2, 0.99838597)]\n",
            "Chpater 59: [(2, 0.9983698)]\n",
            "Chpater 60: [(2, 0.9987329)]\n",
            "Chpater 61: [(2, 0.99880606)]\n",
            "Chpater 62: [(2, 0.9974646)]\n",
            "Chpater 63: [(2, 0.9989591)]\n",
            "Chpater 64: [(0, 0.01625557), (2, 0.98278236)]\n",
            "Chpater 65: [(2, 0.9985506)]\n",
            "Chpater 66: [(2, 0.99832404)]\n",
            "Chpater 67: [(2, 0.9991922)]\n",
            "Chpater 68: [(2, 0.9962541)]\n",
            "Chpater 69: [(2, 0.99912894)]\n",
            "Chpater 70: [(2, 0.9991167)]\n",
            "Chpater 71: [(2, 0.99890417)]\n",
            "Chpater 72: [(2, 0.9987793)]\n",
            "Chpater 73: [(2, 0.9989246)]\n",
            "Chpater 74: [(2, 0.99872047)]\n",
            "Chpater 75: [(2, 0.9988103)]\n",
            "Chpater 76: [(2, 0.9990559)]\n",
            "Chpater 77: [(2, 0.9985215)]\n",
            "Chpater 78: [(2, 0.998878)]\n",
            "Chpater 79: [(2, 0.9988772)]\n",
            "Chpater 80: [(2, 0.9986941)]\n",
            "Chpater 81: [(2, 0.99795485)]\n",
            "Chpater 82: [(2, 0.972676), (3, 0.026376588)]\n",
            "Chpater 83: [(2, 0.99832654)]\n",
            "Chpater 84: [(2, 0.99892205)]\n",
            "Chpater 85: [(2, 0.9993877)]\n",
            "Chpater 86: [(2, 0.9986498)]\n",
            "Chpater 87: [(0, 0.03099946), (2, 0.96814203)]\n",
            "Chpater 88: [(2, 0.99865156)]\n",
            "Chpater 89: [(2, 0.9984513)]\n",
            "Chpater 90: [(2, 0.9953843)]\n",
            "Chpater 91: [(2, 0.99860215)]\n",
            "Chpater 92: [(2, 0.9968457)]\n",
            "Chpater 93: [(2, 0.99176145)]\n",
            "Chpater 94: [(2, 0.99909204)]\n",
            "Chpater 95: [(2, 0.99828607)]\n",
            "Chpater 96: [(2, 0.9983743)]\n",
            "Chpater 97: [(2, 0.9990052)]\n",
            "Chpater 98: [(2, 0.99851114)]\n",
            "Chpater 99: [(2, 0.9977854)]\n",
            "Chpater 100: [(2, 0.9988061)]\n",
            "Chpater 101: [(2, 0.9989471)]\n",
            "Chpater 102: [(2, 0.9990386)]\n",
            "Chpater 103: [(2, 0.9988785)]\n",
            "Chpater 104: [(2, 0.99886096)]\n",
            "Chpater 105: [(2, 0.9987425)]\n",
            "Chpater 106: [(2, 0.99830115)]\n",
            "Chpater 107: [(2, 0.99878913)]\n",
            "Chpater 108: [(2, 0.9987028)]\n",
            "Chpater 109: [(2, 0.99843043)]\n",
            "Chpater 110: [(2, 0.9972557)]\n",
            "Chpater 111: [(2, 0.99926525)]\n",
            "Chpater 112: [(2, 0.9988672)]\n",
            "Chpater 113: [(2, 0.99929804)]\n",
            "Chpater 114: [(2, 0.9968712)]\n",
            "Chpater 115: [(2, 0.99926084)]\n",
            "Chpater 116: [(2, 0.99907357)]\n",
            "Chpater 117: [(2, 0.9980553)]\n",
            "Chpater 118: [(2, 0.9984074)]\n",
            "Chpater 119: [(2, 0.9981186)]\n",
            "Chpater 120: [(2, 0.9982794)]\n",
            "Chpater 121: [(2, 0.9980275)]\n",
            "Chpater 122: [(2, 0.9981642)]\n",
            "Chpater 123: [(2, 0.9987174)]\n",
            "Chpater 124: [(2, 0.99835867)]\n",
            "Chpater 125: [(2, 0.9982458)]\n",
            "Chpater 126: [(2, 0.998075)]\n",
            "Chpater 127: [(2, 0.9986221)]\n"
          ]
        }
      ],
      "source": [
        "for idx, chapter in enumerate(chapters):\n",
        "    processeded_chapter = preprocess_text(chapter)\n",
        "    bow_processeded_chapter = author_dict.doc2bow(processeded_chapter)\n",
        "    \n",
        "    print(f'Chpater {idx + 1}: {lda_model[bow_processeded_chapter]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "    Chapter 1 - (Author 1)\n",
        "\n",
        "    Chapter 2 - (Author 1 - 0.73, Author 3 - 0.25)\n",
        "\n",
        "    Chapter 3 - (Author 1)\n",
        "\n",
        "    etc.\n",
        "\n",
        "    It's possible that a chapter is similar to the writing style or theme of multiple authors - in that case print out the names and similitude weights for the authors.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "DM7 (2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
