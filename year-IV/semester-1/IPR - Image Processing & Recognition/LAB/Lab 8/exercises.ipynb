{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "import requests\n",
    "\n",
    "def download(url: str) -> str:\n",
    "    \"\"\"Download a file, return the local filename.\"\"\"\n",
    "    file_path = '../data/' + url.split('/')[-1]\n",
    "    if os.path.exists(file_path):\n",
    "        return file_path\n",
    "    \n",
    "    print(f'Downloading {file_path} from {url}...')\n",
    "    res = requests.get(url, stream=True, verify=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(res.content)\n",
    "    return file_path\n",
    "\n",
    "def read_time_machine() -> list[str]:\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    file = download('http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt')\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    lines_transformed = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "    return [line for line in lines_transformed if line]\n",
    "\n",
    "def tokenize(lines: list[str], token: Literal['word', 'char']='word') -> list[str]:\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        tokens: list[list[str]] =  [line.split() for line in lines]\n",
    "    else:\n",
    "        tokens: list[list[str]] = [list(line) for line in lines]\n",
    "    return [token for sublist in tokens for token in sublist]\n",
    "    \n",
    "def count_corpus(tokens: list[str]) -> collections.Counter[str]:\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens: list[str]=[], min_freq=0, reserved_tokens: list[str]=[]):\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens + [token for token, freq in self._token_freqs if freq >= min_freq]\n",
    "        self.token_to_idx =  {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens: str | list[str]) -> int | list[int]:\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.token_to_idx.get(token, self.unk) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices: int | list[int]) -> str | list[str]:\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self) -> Literal[0]:  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self) -> list[tuple[str, int]]:  # Token frequencies\n",
    "        return self._token_freqs\n",
    "\n",
    "\n",
    "class SeqDataLoader:\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size: int, num_steps: int, max_tokens: int=10000):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.vocab = self.generate_vocab()\n",
    "        self.corpus = self.generate_corpus(max_tokens)\n",
    "        \n",
    "    def generate_vocab(self) -> Vocab:\n",
    "        lines = read_time_machine()\n",
    "        tokens = tokenize(lines, 'char')\n",
    "        return Vocab(tokens)\n",
    "    \n",
    "    def generate_corpus(self, max_tokens):\n",
    "        tokens = tokenize(read_time_machine(), 'char')\n",
    "        corpus = [self.vocab[token] for line in tokens for token in line]\n",
    "        return corpus[:max_tokens]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate a mini-batch of subsequences using sequential partitioning.\"\"\"\n",
    "        # Start with a random offset to partition a sequence\n",
    "        offset = random.randint(0, self.num_steps)\n",
    "        num_tokens = ((len(self.corpus) - offset - 1) // self.batch_size) * self.batch_size\n",
    "        Xs = torch.tensor(self.corpus[offset: offset + num_tokens])\n",
    "        Ys = torch.tensor(self.corpus[offset + 1: offset + 1 + num_tokens])\n",
    "        Xs, Ys = Xs.reshape(self.batch_size, -1), Ys.reshape(self.batch_size, -1)\n",
    "        num_batches = Xs.shape[1] // self.num_steps\n",
    "        for i in range(0, self.num_steps * num_batches, self.num_steps):\n",
    "            X = Xs[:, i: i + self.num_steps]\n",
    "            Y = Ys[:, i: i + self.num_steps]\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 50\n",
    "steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = SeqDataLoader(batch, steps)\n",
    "vocab = train_iter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size = len(vocab)\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = nn.LSTM(input_size, hidden_size)\n",
    "net = nn.RNN(lstm_layer, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = train(net, train_iter, vocab, lr=1, num_epochs=100, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
