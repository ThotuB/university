{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the following dataset: yi = sin(xi) cos(xi) + xi^3 + âˆˆ, where E obeys a normal distribution with 0 mean and standard deviation 0.1. The number of training/testing examples is 30. Plot the prediction for this regression problem using Nadaraya-Watson kernel regression nonparametric attention model, and also plot the attention heatmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: torch.Tensor):\n",
    "    return torch.sin(x) * torch.cos(x) + x**3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 30\n",
    "x_train, _ = torch.sort(torch.rand(n_train) * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.normal(0, 0.1, size=(n_train,))\n",
    "y_train = f(x_train) + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 30\n",
    "x_test = torch.arange(0, 30, 1)\n",
    "y_test = f(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_kernel_reg(y_hat):\n",
    "    plt.plot(x_test, y_test, label='Truth')\n",
    "    plt.plot(x_test, y_hat, label='Pred')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.plot(x_train, y_train, 'o', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat = torch.repeat_interleave(y_train.mean(), n_test)\n",
    "plot_kernel_reg(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an encoder and a decoder with attention mechanism for machine translation. The attention mechanism is similar to Bahdanau attention excepting the attention weight a that will be computed using the scaled dot-product attention. The dataset is the English-French dataset from the laboratory. The embedding size, number of hidden and the dropout probability is 16, 20 and 50%, respectively. Train the model for 200 epochs and plot the training loss.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "from typing import Literal\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "\n",
    "def download(url: str) -> str:\n",
    "    \"\"\"Download a file, return the local filename.\"\"\"\n",
    "    file_path = '../data/' + url.split('/')[-1]\n",
    "    if os.path.exists(file_path):\n",
    "        return file_path\n",
    "    \n",
    "    print(f'Downloading {file_path} from {url}...')\n",
    "    res = requests.get(url, stream=True, verify=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(res.content)\n",
    "    return file_path\n",
    "\n",
    "def download_extract(url: str) -> str:\n",
    "    \"\"\"Download and extract a zip file.\"\"\"\n",
    "    file_path = download(url)\n",
    "    fp = zipfile.ZipFile(file_path, 'r')\n",
    "    \n",
    "    data_folder_path = os.path.dirname(file_path)\n",
    "    fp.extractall(data_folder_path)\n",
    "    \n",
    "    extracted_folder_path = os.path.splitext(file_path)[0]\n",
    "    return extracted_folder_path\n",
    "\n",
    "def read_data_nmt() -> str:\n",
    "    \"\"\"Load the English-French dataset.\"\"\"\n",
    "    folder = download_extract('http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip')\n",
    "    with open(folder + '/fra.txt', 'r') as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens: list[str]=[], min_freq=0, reserved_tokens: list[str]=[]):\n",
    "        # Sort according to frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens + [token for token, freq in self._token_freqs if freq >= min_freq]\n",
    "        self.token_to_idx =  {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens: str | list[str]) -> int | list[int]:\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.token_to_idx.get(token, self.unk) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices: int | list[int]) -> str | list[str]:\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self) -> Literal[0]:  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self) -> list[tuple[str, int]]:  # Token frequencies\n",
    "        return self._token_freqs\n",
    "\n",
    "def preprocess_nmt(text: str) -> str:\n",
    "    \"\"\"Preprocess the English-French dataset.\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # Replace non-breaking space with space, and convert uppercase letters to\n",
    "    # lowercase ones\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # Insert space between words and punctuation marks\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "def tokenize_nmt(text: str, num_examples: int):\n",
    "    \"\"\"Tokenize the English-French dataset.\"\"\"\n",
    "    source: list[str] = []\n",
    "    target: list[str] = []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.extend(parts[0].split(' '))\n",
    "            target.extend(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "def truncate_pad(line, num_steps: int, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "def build_array_nmt(lines: list[str], vocab: Vocab, num_steps: int):\n",
    "    \"\"\"Transform text sequences of machine translation into mini-batches.\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    print(lines)\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    \n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    \n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    \n",
    "    return array, valid_len\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    return DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "def load_data_nmt(batch_size: int, num_steps: int, num_examples=600):\n",
    "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    \n",
    "    src_vocab = Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    \n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    \n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = load_array(data_arrays, batch_size)\n",
    "    return data_iter, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess_nmt(read_data_nmt())\n",
    "source, target = tokenize_nmt(text, 654)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 4, 119, 4, 59, 6, 59, 6, 45, 11, 0, 6, 73, 6, 39, 6, 0, 4, 64, 6, 64, 6, 64, 6, 120, 6, 120, 6, 9, 22, 4, 9, 22, 4, 9, 22, 4, 121, 6, 121, 6, 5, 122, 4, 5, 33, 4, 5, 23, 6, 5, 23, 6, 5, 23, 4, 0, 26, 6, 123, 6, 123, 6, 74, 6, 74, 6, 74, 6, 74, 6, 13, 14, 4, 9, 75, 4, 9, 75, 4, 9, 75, 4, 34, 8, 6, 34, 8, 6, 34, 8, 11, 34, 8, 11, 34, 8, 11, 124, 46, 4, 124, 46, 4, 96, 12, 4, 96, 12, 4, 5, 97, 4, 5, 97, 4, 5, 125, 4, 5, 76, 4, 5, 76, 4, 5, 19, 4, 5, 126, 4, 7, 0, 4, 7, 77, 4, 7, 77, 4, 0, 4, 26, 35, 6, 26, 35, 6, 26, 35, 6, 26, 35, 6, 26, 35, 6, 26, 35, 6, 26, 35, 6, 26, 35, 6, 26, 35, 6, 98, 11, 98, 11, 98, 11, 0, 4, 17, 33, 4, 17, 23, 4, 17, 23, 4, 17, 23, 4, 17, 23, 4, 78, 10, 4, 0, 6, 15, 47, 4, 15, 47, 4, 15, 47, 4, 15, 65, 4, 15, 36, 4, 15, 36, 4, 15, 36, 4, 15, 36, 4, 15, 36, 4, 15, 36, 4, 15, 0, 4, 15, 29, 4, 15, 29, 4, 15, 29, 4, 15, 29, 4, 15, 29, 4, 15, 29, 4, 0, 8, 4, 48, 12, 4, 48, 12, 4, 48, 25, 4, 48, 25, 4, 16, 46, 4, 16, 46, 4, 16, 46, 4, 16, 46, 4, 16, 22, 6, 16, 22, 4, 16, 22, 4, 16, 22, 4, 79, 8, 6, 79, 8, 6, 79, 8, 6, 79, 8, 6, 13, 10, 4, 13, 37, 6, 13, 37, 6, 13, 37, 6, 13, 37, 4, 13, 37, 4, 9, 30, 6, 9, 30, 6, 9, 30, 4, 9, 30, 4, 9, 30, 4, 9, 30, 4, 9, 30, 4, 9, 30, 4, 9, 30, 4, 9, 30, 4, 9, 60, 4, 9, 60, 4, 9, 60, 4, 9, 60, 4, 9, 127, 4, 9, 127, 4, 0, 6, 80, 22, 6, 80, 22, 6, 80, 22, 4, 80, 22, 4, 49, 99, 4, 49, 99, 4, 49, 128, 4, 39, 12, 6, 39, 12, 4, 39, 12, 4, 39, 25, 4, 39, 25, 4, 40, 8, 6, 40, 22, 4, 96, 10, 4, 5, 129, 4, 5, 0, 4, 5, 130, 4, 5, 130, 4, 5, 66, 4, 5, 0, 4, 5, 0, 4, 5, 0, 4, 5, 131, 4, 5, 131, 4, 5, 132, 4, 5, 132, 4, 5, 100, 4, 5, 100, 4, 5, 100, 4, 5, 0, 4, 81, 9, 4, 7, 10, 4, 7, 67, 4, 7, 67, 4, 7, 0, 4, 7, 101, 6, 7, 101, 6, 7, 133, 4, 7, 134, 4, 7, 0, 4, 7, 102, 4, 7, 102, 4, 27, 12, 6, 135, 25, 4, 135, 25, 4, 82, 8, 4, 82, 8, 4, 103, 12, 4, 103, 12, 4, 12, 136, 137, 4, 138, 14, 4, 138, 14, 4, 0, 6, 122, 20, 4, 139, 12, 4, 139, 12, 4, 68, 14, 6, 68, 14, 6, 68, 14, 6, 68, 14, 6, 68, 14, 6, 104, 0, 4, 140, 8, 4, 140, 8, 4, 83, 12, 4, 83, 12, 4, 10, 23, 4, 69, 14, 6, 69, 14, 6, 69, 14, 6, 69, 14, 4, 69, 14, 4, 141, 14, 4, 141, 14, 4, 17, 125, 4, 17, 19, 4, 17, 19, 4, 17, 19, 4, 17, 19, 4, 17, 19, 4, 17, 19, 4, 17, 19, 4, 17, 19, 4, 17, 19, 4, 17, 19, 4, 0, 4, 45, 23, 11, 45, 23, 11, 20, 59, 4, 18, 5, 67, 11, 18, 5, 67, 11, 78, 142, 4, 78, 142, 4, 31, 41, 6, 31, 41, 6, 31, 41, 4, 31, 41, 4, 31, 41, 4, 31, 41, 4, 15, 50, 143, 4, 15, 50, 143, 4, 15, 105, 4, 15, 105, 4, 15, 105, 4, 144, 12, 4, 144, 12, 4, 48, 10, 4, 48, 10, 4, 0, 14, 6, 65, 41, 6, 0, 51, 4, 66, 22, 4, 66, 22, 4, 66, 22, 4, 66, 22, 4, 145, 10, 4, 145, 10, 4, 146, 21, 4, 146, 21, 4, 13, 28, 6, 13, 28, 4, 13, 28, 4, 13, 28, 4, 13, 28, 4, 13, 19, 6, 13, 19, 6, 13, 19, 6, 13, 0, 6, 9, 84, 4, 9, 84, 4, 9, 84, 4, 9, 84, 4, 52, 53, 6, 52, 53, 6, 52, 53, 6, 42, 51, 4, 42, 51, 4, 106, 32, 4, 106, 32, 4, 49, 0, 4, 61, 102, 4, 39, 10, 4, 39, 10, 4, 119, 136, 0, 4, 43, 0, 6, 43, 0, 11, 43, 29, 6, 43, 29, 6, 43, 29, 6, 43, 29, 6, 0, 12, 4, 85, 14, 4, 85, 14, 4, 85, 14, 4, 85, 14, 4, 5, 18, 67, 4, 5, 54, 77, 4, 5, 54, 77, 4, 5, 54, 8, 4, 5, 54, 8, 4, 5, 0, 4, 5, 0, 4, 5, 13, 8, 4, 5, 34, 8, 4, 5, 34, 8, 4, 5, 0, 4, 5, 0, 4, 5, 0, 4, 5, 0, 4, 5, 0, 4, 5, 0, 4, 5, 147, 4, 5, 147, 4, 5, 148, 4, 5, 148, 4, 5, 149, 4, 5, 149, 4, 5, 24, 8, 4, 5, 24, 8, 4, 5, 0, 4, 5, 150, 4, 5, 150, 4, 5, 0, 4, 5, 55, 8, 4, 5, 55, 8, 4, 5, 55, 8, 4, 81, 0, 4, 81, 33, 4, 81, 33, 4, 7, 31, 4, 7, 31, 4, 7, 0, 4, 7, 107, 4, 7, 107, 4, 7, 47, 4, 7, 151, 4, 7, 65, 4, 7, 65, 4, 7, 152, 4, 7, 152, 4, 7, 0, 4, 7, 36, 4, 7, 36, 4, 7, 36, 4, 7, 0, 4, 7, 86, 4, 7, 86, 4, 7, 86, 4, 7, 108, 6, 7, 108, 4, 7, 108, 4, 7, 153, 4, 7, 153, 4, 7, 154, 4, 7, 154, 4, 7, 0, 4, 7, 60, 4, 7, 109, 4, 7, 38, 4, 7, 38, 4, 7, 38, 4, 7, 38, 4, 7, 110, 4, 7, 110, 4, 7, 155, 4, 7, 0, 4, 7, 156, 4, 7, 62, 4, 7, 62, 4, 7, 62, 4, 7, 62, 4, 7, 0, 4, 7, 0, 4, 7, 157, 4, 7, 157, 4, 7, 158, 4, 7, 158, 4, 7, 159, 4, 7, 160, 4, 7, 160, 4, 161, 23, 4, 161, 23, 4, 8, 0, 4, 8, 0, 4, 8, 162, 4, 8, 162, 4, 27, 10, 4, 27, 32, 4, 27, 32, 4, 27, 163, 4, 27, 163, 4, 27, 164, 4, 27, 164, 4, 27, 0, 4, 27, 0, 4, 27, 134, 4, 82, 37, 6, 82, 37, 4, 103, 10, 4, 70, 8, 4, 70, 8, 4, 70, 12, 4, 70, 25, 4, 70, 25, 4, 165, 9, 6, 165, 9, 4, 166, 37, 6, 166, 37, 6, 167, 12, 4, 167, 12, 4, 111, 5, 9, 11, 111, 5, 9, 11, 111, 5, 9, 11, 168, 10, 4, 168, 10, 4, 112, 113, 4, 112, 87, 4, 112, 128, 4, 71, 28, 6, 71, 28, 6, 71, 28, 4, 71, 63, 4, 71, 63, 4, 169, 14, 6, 169, 14, 6, 64, 10, 4, 64, 10, 4, 88, 8, 4, 88, 8, 4, 88, 8, 4, 88, 8, 4, 83, 10, 4, 83, 10, 4, 114, 6, 114, 6, 114, 6, 89, 23, 4, 89, 23, 4, 89, 23, 4, 89, 23, 4, 10, 113, 4, 10, 87, 4, 10, 87, 4, 10, 170, 4, 10, 76, 4, 10, 76, 4, 10, 0, 4, 10, 0, 4, 10, 19, 4, 10, 126, 4, 137, 109, 4, 171, 12, 4, 171, 12, 4, 33, 0, 4, 33, 172, 4, 33, 172, 4, 33, 21, 4, 33, 21, 4, 55, 21, 4, 55, 21, 4, 55, 21, 4, 55, 21, 4, 173, 10, 4, 173, 10, 4, 90, 12, 4, 90, 12, 4, 90, 25, 4, 90, 25, 4, 17, 129, 4, 0, 9, 4, 91, 174, 11, 91, 174, 11, 91, 32, 6, 91, 32, 6, 45, 113, 11, 45, 87, 11, 45, 97, 11, 45, 99, 11, 0, 49, 11, 175, 12, 4, 175, 12, 4, 20, 19, 4, 20, 19, 4, 0, 20, 4, 0, 4, 73, 6, 18, 5, 109, 11, 0, 12, 4, 15, 176, 4, 15, 176, 4, 0, 0, 4, 0, 20, 4, 48, 60, 6, 47, 28, 6, 47, 28, 4, 72, 17, 9, 11, 72, 17, 9, 11, 72, 17, 9, 11, 115, 10, 4, 115, 10, 4, 115, 51, 4, 16, 31, 4, 16, 31, 4, 16, 63, 4, 16, 63, 4, 16, 56, 6, 16, 56, 6, 16, 56, 4, 16, 56, 4, 16, 56, 4, 16, 56, 4, 16, 56, 4, 16, 177, 4, 16, 177, 4, 65, 28, 4, 54, 5, 116, 11, 54, 5, 116, 11, 54, 5, 116, 11, 0, 8, 75, 4, 178, 179, 4, 178, 179, 4, 57, 78, 4, 57, 0, 4, 57, 180, 4, 57, 180, 4, 57, 0, 4, 57, 59, 4, 57, 59, 4, 181, 12, 4, 181, 12, 4, 0, 6, 92, 21, 4, 92, 21, 4, 92, 21, 4, 92, 21, 4, 117, 12, 4, 117, 25, 4, 117, 25, 4, 58, 8, 6, 58, 8, 6, 58, 8, 6, 58, 8, 6, 58, 8, 4, 58, 8, 4, 58, 8, 4, 13, 50, 53, 4, 13, 50, 53, 4, 13, 50, 53, 4, 13, 50, 53, 4, 13, 182, 4, 13, 182, 4, 9, 13, 8, 4, 9, 13, 8, 4, 9, 0, 4, 9, 183, 184, 4, 9, 183, 184, 4, 52, 185, 4, 52, 185, 4, 42, 93, 4, 42, 93, 4, 42, 93, 4, 42, 93, 4, 42, 21, 4, 42, 21, 4, 0, 41, 4, 49, 186, 133, 4, 49, 186, 0, 4, 61, 50, 0, 4, 61, 52, 4, 61, 38, 4, 61, 155, 4, 61, 0, 4, 63, 5, 18, 4, 0, 0, 4, 40, 73, 4, 40, 73, 4, 40, 21, 4, 40, 21, 4, 40, 21, 4, 40, 21, 4, 43, 0, 6, 43, 0, 6, 187, 10, 11, 187, 10, 11, 5, 18, 107, 4, 5, 18, 47, 4, 5, 18, 151, 4, 5, 18, 86, 4, 5, 18, 52, 4, 5, 18, 63, 4, 5, 18, 38, 4, 5, 18, 38, 4, 5, 18, 38, 4, 5, 18, 38, 4, 5, 18, 110, 4, 5, 18, 156, 4, 5, 18, 62, 4, 5, 18, 62, 4, 5, 18, 159, 4, 5, 94, 20, 4, 5, 94, 20, 4, 5, 94, 20, 4, 5, 94, 20, 4, 5, 72, 59, 4, 5, 72, 0, 4, 5, 118, 4, 5, 118, 4, 5, 118, 4, 5, 0, 4, 5, 0, 14, 4, 5, 188, 46, 4, 5, 188, 14, 4, 5, 34, 189, 4, 5, 34, 189, 4, 5, 95, 32, 4, 5, 95, 32, 4, 5, 95, 32, 4, 5, 95, 32, 4, 5, 0, 8, 4, 5, 106, 8, 4, 5, 101, 10, 4, 5, 0, 104, 4, 5, 190, 4, 5, 190, 4, 5, 0, 4, 5, 170, 8, 4, 5, 0, 8, 4, 5, 19, 8, 4, 5, 191, 8, 6, 5, 191, 8, 4, 5, 192, 8, 6, 5, 192, 8, 4, 5, 44, 9, 4, 5, 44, 9, 4, 5, 44, 9, 4, 5, 44, 9, 4, 5, 44, 9, 4, 5, 44, 9, 4, 5, 44, 9, 4, 5, 44, 9, 4, 5, 193, 8, 4, 5, 193, 8, 4, 5, 0, 4, 5, 0, 4, 5, 0, 4, 5, 194, 4, 5, 194, 4, 5, 0, 4, 5, 195, 26, 4, 5, 195, 104, 4, 5, 24, 51, 4, 5, 24, 51, 4, 5, 24, 51, 4, 5, 24, 196, 4, 5, 24, 196, 4, 5, 24, 20, 4, 5, 24, 20, 4, 5, 24, 20, 4, 5, 24, 20, 4, 5, 24, 20, 4, 5, 24, 20, 4, 5, 24, 20, 4, 5, 24, 20, 4]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23408/3363776755.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msrc_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_valid_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_array_nmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m654\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23408/1749878059.py\u001b[0m in \u001b[0;36mbuild_array_nmt\u001b[1;34m(lines, vocab, num_steps)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtruncate_pad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<pad>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23408/1749878059.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtruncate_pad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<pad>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "src_array, src_valid_len = build_array_nmt(source, src_vocab, 654)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['hi', '.'], ['run', '!'], ['run', '!'], ['who', '?'], ['wow', '!'], ['fire', '!'], ['help', '!'], ['jump', '.'], ['stop', '!'], ['stop', '!'], ['stop', '!'], ['wait', '!'], ['wait', '!'], ['go', 'on', '.'], ['go', 'on', '.'], ['go', 'on', '.'], ['hello', '!'], ['hello', '!'], ['i', 'see', '.'], ['i', 'try', '.'], ['i', 'won', '!'], ['i', 'won', '!'], ['i', 'won', '.'], ['oh', 'no', '!'], ['attack', '!'], ['attack', '!'], ['cheers', '!'], ['cheers', '!'], ['cheers', '!'], ['cheers', '!'], ['get', 'up', '.'], ['go', 'now', '.'], ['go', 'now', '.'], ['go', 'now', '.'], ['got', 'it', '!'], ['got', 'it', '!'], ['got', 'it', '?'], ['got', 'it', '?'], ['got', 'it', '?'], ['hop', 'in', '.'], ['hop', 'in', '.'], ['hug', 'me', '.'], ['hug', 'me', '.'], ['i', 'fell', '.'], ['i', 'fell', '.'], ['i', 'know', '.'], ['i', 'left', '.'], ['i', 'left', '.'], ['i', 'lost', '.'], ['i', 'paid', '.'], [\"i'm\", '19', '.'], [\"i'm\", 'ok', '.'], [\"i'm\", 'ok', '.'], ['listen', '.'], ['no', 'way', '!'], ['no', 'way', '!'], ['no', 'way', '!'], ['no', 'way', '!'], ['no', 'way', '!'], ['no', 'way', '!'], ['no', 'way', '!'], ['no', 'way', '!'], ['no', 'way', '!'], ['really', '?'], ['really', '?'], ['really', '?'], ['thanks', '.'], ['we', 'try', '.'], ['we', 'won', '.'], ['we', 'won', '.'], ['we', 'won', '.'], ['we', 'won', '.'], ['ask', 'tom', '.'], ['awesome', '!'], ['be', 'calm', '.'], ['be', 'calm', '.'], ['be', 'calm', '.'], ['be', 'cool', '.'], ['be', 'fair', '.'], ['be', 'fair', '.'], ['be', 'fair', '.'], ['be', 'fair', '.'], ['be', 'fair', '.'], ['be', 'fair', '.'], ['be', 'kind', '.'], ['be', 'nice', '.'], ['be', 'nice', '.'], ['be', 'nice', '.'], ['be', 'nice', '.'], ['be', 'nice', '.'], ['be', 'nice', '.'], ['beat', 'it', '.'], ['call', 'me', '.'], ['call', 'me', '.'], ['call', 'us', '.'], ['call', 'us', '.'], ['come', 'in', '.'], ['come', 'in', '.'], ['come', 'in', '.'], ['come', 'in', '.'], ['come', 'on', '!'], ['come', 'on', '.'], ['come', 'on', '.'], ['come', 'on', '.'], ['drop', 'it', '!'], ['drop', 'it', '!'], ['drop', 'it', '!'], ['drop', 'it', '!'], ['get', 'tom', '.'], ['get', 'out', '!'], ['get', 'out', '!'], ['get', 'out', '!'], ['get', 'out', '.'], ['get', 'out', '.'], ['go', 'away', '!'], ['go', 'away', '!'], ['go', 'away', '.'], ['go', 'away', '.'], ['go', 'away', '.'], ['go', 'away', '.'], ['go', 'away', '.'], ['go', 'away', '.'], ['go', 'away', '.'], ['go', 'away', '.'], ['go', 'home', '.'], ['go', 'home', '.'], ['go', 'home', '.'], ['go', 'home', '.'], ['go', 'slow', '.'], ['go', 'slow', '.'], ['goodbye', '!'], ['hang', 'on', '!'], ['hang', 'on', '!'], ['hang', 'on', '.'], ['hang', 'on', '.'], ['he', 'quit', '.'], ['he', 'quit', '.'], ['he', 'runs', '.'], ['help', 'me', '!'], ['help', 'me', '.'], ['help', 'me', '.'], ['help', 'us', '.'], ['help', 'us', '.'], ['hold', 'it', '!'], ['hold', 'on', '.'], ['hug', 'tom', '.'], ['i', 'agree', '.'], ['i', 'cried', '.'], ['i', 'dozed', '.'], ['i', 'dozed', '.'], ['i', 'drive', '.'], ['i', 'smoke', '.'], ['i', 'snore', '.'], ['i', 'stink', '.'], ['i', 'stood', '.'], ['i', 'stood', '.'], ['i', 'swore', '.'], ['i', 'swore', '.'], ['i', 'tried', '.'], ['i', 'tried', '.'], ['i', 'tried', '.'], ['i', 'waved', '.'], [\"i'll\", 'go', '.'], [\"i'm\", 'tom', '.'], [\"i'm\", 'fat', '.'], [\"i'm\", 'fat', '.'], [\"i'm\", 'fit', '.'], [\"i'm\", 'hit', '!'], [\"i'm\", 'hit', '!'], [\"i'm\", 'ill', '.'], [\"i'm\", 'sad', '.'], [\"i'm\", 'shy', '.'], [\"i'm\", 'wet', '.'], [\"i'm\", 'wet', '.'], [\"it's\", 'me', '!'], ['join', 'us', '.'], ['join', 'us', '.'], ['keep', 'it', '.'], ['keep', 'it', '.'], ['kiss', 'me', '.'], ['kiss', 'me', '.'], ['me', ',', 'too', '.'], ['open', 'up', '.'], ['open', 'up', '.'], ['perfect', '!'], ['see', 'you', '.'], ['show', 'me', '.'], ['show', 'me', '.'], ['shut', 'up', '!'], ['shut', 'up', '!'], ['shut', 'up', '!'], ['shut', 'up', '!'], ['shut', 'up', '!'], ['so', 'long', '.'], ['take', 'it', '.'], ['take', 'it', '.'], ['tell', 'me', '.'], ['tell', 'me', '.'], ['tom', 'won', '.'], ['wake', 'up', '!'], ['wake', 'up', '!'], ['wake', 'up', '!'], ['wake', 'up', '.'], ['wake', 'up', '.'], ['wash', 'up', '.'], ['wash', 'up', '.'], ['we', 'know', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['we', 'lost', '.'], ['welcome', '.'], ['who', 'won', '?'], ['who', 'won', '?'], ['you', 'run', '.'], ['am', 'i', 'fat', '?'], ['am', 'i', 'fat', '?'], ['ask', 'them', '.'], ['ask', 'them', '.'], ['back', 'off', '!'], ['back', 'off', '!'], ['back', 'off', '.'], ['back', 'off', '.'], ['back', 'off', '.'], ['back', 'off', '.'], ['be', 'a', 'man', '.'], ['be', 'a', 'man', '.'], ['be', 'still', '.'], ['be', 'still', '.'], ['be', 'still', '.'], ['beats', 'me', '.'], ['beats', 'me', '.'], ['call', 'tom', '.'], ['call', 'tom', '.'], ['cheer', 'up', '!'], ['cool', 'off', '!'], ['cuff', 'him', '.'], ['drive', 'on', '.'], ['drive', 'on', '.'], ['drive', 'on', '.'], ['drive', 'on', '.'], ['find', 'tom', '.'], ['find', 'tom', '.'], ['fix', 'this', '.'], ['fix', 'this', '.'], ['get', 'down', '!'], ['get', 'down', '.'], ['get', 'down', '.'], ['get', 'down', '.'], ['get', 'down', '.'], ['get', 'lost', '!'], ['get', 'lost', '!'], ['get', 'lost', '!'], ['get', 'real', '!'], ['go', 'ahead', '.'], ['go', 'ahead', '.'], ['go', 'ahead', '.'], ['go', 'ahead', '.'], ['good', 'job', '!'], ['good', 'job', '!'], ['good', 'job', '!'], ['grab', 'him', '.'], ['grab', 'him', '.'], ['have', 'fun', '.'], ['have', 'fun', '.'], ['he', 'tries', '.'], [\"he's\", 'wet', '.'], ['help', 'tom', '.'], ['help', 'tom', '.'], ['hi', ',', 'guys', '.'], ['how', 'cute', '!'], ['how', 'deep', '?'], ['how', 'nice', '!'], ['how', 'nice', '!'], ['how', 'nice', '!'], ['how', 'nice', '!'], ['humor', 'me', '.'], ['hurry', 'up', '.'], ['hurry', 'up', '.'], ['hurry', 'up', '.'], ['hurry', 'up', '.'], ['i', 'am', 'fat', '.'], ['i', 'did', 'ok', '.'], ['i', 'did', 'ok', '.'], ['i', 'did', 'it', '.'], ['i', 'did', 'it', '.'], ['i', 'failed', '.'], ['i', 'forgot', '.'], ['i', 'get', 'it', '.'], ['i', 'got', 'it', '.'], ['i', 'got', 'it', '.'], ['i', 'helped', '.'], ['i', 'jumped', '.'], ['i', 'looked', '.'], ['i', 'moaned', '.'], ['i', 'nodded', '.'], ['i', 'obeyed', '.'], ['i', 'phoned', '.'], ['i', 'phoned', '.'], ['i', 'refuse', '.'], ['i', 'refuse', '.'], ['i', 'rested', '.'], ['i', 'rested', '.'], ['i', 'saw', 'it', '.'], ['i', 'saw', 'it', '.'], ['i', 'sighed', '.'], ['i', 'stayed', '.'], ['i', 'stayed', '.'], ['i', 'talked', '.'], ['i', 'use', 'it', '.'], ['i', 'use', 'it', '.'], ['i', 'use', 'it', '.'], [\"i'll\", 'pay', '.'], [\"i'll\", 'try', '.'], [\"i'll\", 'try', '.'], [\"i'm\", 'back', '.'], [\"i'm\", 'back', '.'], [\"i'm\", 'bald', '.'], [\"i'm\", 'busy', '.'], [\"i'm\", 'busy', '.'], [\"i'm\", 'calm', '.'], [\"i'm\", 'cold', '.'], [\"i'm\", 'cool', '.'], [\"i'm\", 'cool', '.'], [\"i'm\", 'deaf', '.'], [\"i'm\", 'deaf', '.'], [\"i'm\", 'done', '.'], [\"i'm\", 'fair', '.'], [\"i'm\", 'fair', '.'], [\"i'm\", 'fair', '.'], [\"i'm\", 'fast', '.'], [\"i'm\", 'fine', '.'], [\"i'm\", 'fine', '.'], [\"i'm\", 'fine', '.'], [\"i'm\", 'free', '!'], [\"i'm\", 'free', '.'], [\"i'm\", 'free', '.'], [\"i'm\", 'full', '.'], [\"i'm\", 'full', '.'], [\"i'm\", 'game', '.'], [\"i'm\", 'game', '.'], [\"i'm\", 'glad', '.'], [\"i'm\", 'home', '.'], [\"i'm\", 'late', '.'], [\"i'm\", 'lazy', '.'], [\"i'm\", 'lazy', '.'], [\"i'm\", 'lazy', '.'], [\"i'm\", 'lazy', '.'], [\"i'm\", 'okay', '.'], [\"i'm\", 'okay', '.'], [\"i'm\", 'rich', '.'], [\"i'm\", 'safe', '.'], [\"i'm\", 'sick', '.'], [\"i'm\", 'sure', '.'], [\"i'm\", 'sure', '.'], [\"i'm\", 'sure', '.'], [\"i'm\", 'sure', '.'], [\"i'm\", 'tall', '.'], [\"i'm\", 'thin', '.'], [\"i'm\", 'tidy', '.'], [\"i'm\", 'tidy', '.'], [\"i'm\", 'ugly', '.'], [\"i'm\", 'ugly', '.'], [\"i'm\", 'weak', '.'], [\"i'm\", 'well', '.'], [\"i'm\", 'well', '.'], [\"i've\", 'won', '.'], [\"i've\", 'won', '.'], ['it', 'helps', '.'], ['it', 'hurts', '.'], ['it', 'works', '.'], ['it', 'works', '.'], [\"it's\", 'tom', '.'], [\"it's\", 'fun', '.'], [\"it's\", 'fun', '.'], [\"it's\", 'his', '.'], [\"it's\", 'his', '.'], [\"it's\", 'new', '.'], [\"it's\", 'new', '.'], [\"it's\", 'odd', '.'], [\"it's\", 'red', '.'], [\"it's\", 'sad', '.'], ['keep', 'out', '!'], ['keep', 'out', '.'], ['kiss', 'tom', '.'], ['leave', 'it', '.'], ['leave', 'it', '.'], ['leave', 'me', '.'], ['leave', 'us', '.'], ['leave', 'us', '.'], [\"let's\", 'go', '!'], [\"let's\", 'go', '.'], ['look', 'out', '!'], ['look', 'out', '!'], ['marry', 'me', '.'], ['marry', 'me', '.'], ['may', 'i', 'go', '?'], ['may', 'i', 'go', '?'], ['may', 'i', 'go', '?'], ['save', 'tom', '.'], ['save', 'tom', '.'], ['she', 'came', '.'], ['she', 'died', '.'], ['she', 'runs', '.'], ['sit', 'down', '!'], ['sit', 'down', '!'], ['sit', 'down', '.'], ['sit', 'here', '.'], ['sit', 'here', '.'], ['speak', 'up', '!'], ['speak', 'up', '!'], ['stop', 'tom', '.'], ['stop', 'tom', '.'], ['taste', 'it', '.'], ['taste', 'it', '.'], ['taste', 'it', '.'], ['taste', 'it', '.'], ['tell', 'tom', '.'], ['tell', 'tom', '.'], ['terrific', '!'], ['terrific', '!'], ['terrific', '!'], ['they', 'won', '.'], ['they', 'won', '.'], ['they', 'won', '.'], ['they', 'won', '.'], ['tom', 'came', '.'], ['tom', 'died', '.'], ['tom', 'died', '.'], ['tom', 'knew', '.'], ['tom', 'left', '.'], ['tom', 'left', '.'], ['tom', 'lied', '.'], ['tom', 'lies', '.'], ['tom', 'lost', '.'], ['tom', 'paid', '.'], ['too', 'late', '.'], ['trust', 'me', '.'], ['trust', 'me', '.'], ['try', 'hard', '.'], ['try', 'some', '.'], ['try', 'some', '.'], ['try', 'this', '.'], ['try', 'this', '.'], ['use', 'this', '.'], ['use', 'this', '.'], ['use', 'this', '.'], ['use', 'this', '.'], ['warn', 'tom', '.'], ['warn', 'tom', '.'], ['watch', 'me', '.'], ['watch', 'me', '.'], ['watch', 'us', '.'], ['watch', 'us', '.'], ['we', 'agree', '.'], [\"we'll\", 'go', '.'], ['what', 'for', '?'], ['what', 'for', '?'], ['what', 'fun', '!'], ['what', 'fun', '!'], ['who', 'came', '?'], ['who', 'died', '?'], ['who', 'fell', '?'], ['who', 'quit', '?'], [\"who's\", 'he', '?'], ['write', 'me', '.'], ['write', 'me', '.'], ['you', 'lost', '.'], ['you', 'lost', '.'], ['after', 'you', '.'], ['aim', '.', 'fire', '!'], ['am', 'i', 'late', '?'], ['answer', 'me', '.'], ['be', 'seated', '.'], ['be', 'seated', '.'], ['birds', 'fly', '.'], ['bless', 'you', '.'], ['call', 'home', '!'], ['calm', 'down', '!'], ['calm', 'down', '.'], ['can', 'we', 'go', '?'], ['can', 'we', 'go', '?'], ['can', 'we', 'go', '?'], ['catch', 'tom', '.'], ['catch', 'tom', '.'], ['catch', 'him', '.'], ['come', 'back', '.'], ['come', 'back', '.'], ['come', 'here', '.'], ['come', 'here', '.'], ['come', 'over', '!'], ['come', 'over', '!'], ['come', 'over', '.'], ['come', 'over', '.'], ['come', 'over', '.'], ['come', 'over', '.'], ['come', 'over', '.'], ['come', 'soon', '.'], ['come', 'soon', '.'], ['cool', 'down', '.'], ['did', 'i', 'win', '?'], ['did', 'i', 'win', '?'], ['did', 'i', 'win', '?'], ['do', 'it', 'now', '.'], ['dogs', 'bark', '.'], ['dogs', 'bark', '.'], [\"don't\", 'ask', '.'], [\"don't\", 'cry', '.'], [\"don't\", 'die', '.'], [\"don't\", 'die', '.'], [\"don't\", 'lie', '.'], [\"don't\", 'run', '.'], [\"don't\", 'run', '.'], ['excuse', 'me', '.'], ['excuse', 'me', '.'], ['fantastic', '!'], ['feel', 'this', '.'], ['feel', 'this', '.'], ['feel', 'this', '.'], ['feel', 'this', '.'], ['follow', 'me', '.'], ['follow', 'us', '.'], ['follow', 'us', '.'], ['forget', 'it', '!'], ['forget', 'it', '!'], ['forget', 'it', '!'], ['forget', 'it', '!'], ['forget', 'it', '.'], ['forget', 'it', '.'], ['forget', 'it', '.'], ['get', 'a', 'job', '.'], ['get', 'a', 'job', '.'], ['get', 'a', 'job', '.'], ['get', 'a', 'job', '.'], ['get', 'ready', '.'], ['get', 'ready', '.'], ['go', 'get', 'it', '.'], ['go', 'get', 'it', '.'], ['go', 'inside', '.'], ['go', 'to', 'bed', '.'], ['go', 'to', 'bed', '.'], ['good', 'luck', '.'], ['good', 'luck', '.'], ['grab', 'that', '.'], ['grab', 'that', '.'], ['grab', 'that', '.'], ['grab', 'that', '.'], ['grab', 'this', '.'], ['grab', 'this', '.'], ['hands', 'off', '.'], ['he', 'is', 'ill', '.'], ['he', 'is', 'old', '.'], [\"he's\", 'a', 'dj', '.'], [\"he's\", 'good', '.'], [\"he's\", 'lazy', '.'], [\"he's\", 'rich', '.'], [\"he's\", 'sexy', '.'], ['here', 'i', 'am', '.'], [\"here's\", '$5', '.'], ['hold', 'fire', '.'], ['hold', 'fire', '.'], ['hold', 'this', '.'], ['hold', 'this', '.'], ['hold', 'this', '.'], ['hold', 'this', '.'], ['how', 'awful', '!'], ['how', 'weird', '!'], [\"how's\", 'tom', '?'], [\"how's\", 'tom', '?'], ['i', 'am', 'busy', '.'], ['i', 'am', 'calm', '.'], ['i', 'am', 'cold', '.'], ['i', 'am', 'fine', '.'], ['i', 'am', 'good', '.'], ['i', 'am', 'here', '.'], ['i', 'am', 'lazy', '.'], ['i', 'am', 'lazy', '.'], ['i', 'am', 'lazy', '.'], ['i', 'am', 'lazy', '.'], ['i', 'am', 'okay', '.'], ['i', 'am', 'sick', '.'], ['i', 'am', 'sure', '.'], ['i', 'am', 'sure', '.'], ['i', 'am', 'weak', '.'], ['i', 'beg', 'you', '.'], ['i', 'beg', 'you', '.'], ['i', 'beg', 'you', '.'], ['i', 'beg', 'you', '.'], ['i', 'can', 'run', '.'], ['i', 'can', 'ski', '.'], ['i', 'cringed', '.'], ['i', 'cringed', '.'], ['i', 'cringed', '.'], ['i', 'exhaled', '.'], ['i', 'gave', 'up', '.'], ['i', 'give', 'in', '.'], ['i', 'give', 'up', '.'], ['i', 'got', 'hot', '.'], ['i', 'got', 'hot', '.'], ['i', 'had', 'fun', '.'], ['i', 'had', 'fun', '.'], ['i', 'had', 'fun', '.'], ['i', 'had', 'fun', '.'], ['i', 'hate', 'it', '.'], ['i', 'have', 'it', '.'], ['i', 'hit', 'tom', '.'], ['i', 'hope', 'so', '.'], ['i', 'hurried', '.'], ['i', 'hurried', '.'], ['i', 'inhaled', '.'], ['i', 'knew', 'it', '.'], ['i', 'like', 'it', '.'], ['i', 'lost', 'it', '.'], ['i', 'love', 'it', '!'], ['i', 'love', 'it', '.'], ['i', 'mean', 'it', '!'], ['i', 'mean', 'it', '.'], ['i', 'must', 'go', '.'], ['i', 'must', 'go', '.'], ['i', 'must', 'go', '.'], ['i', 'must', 'go', '.'], ['i', 'must', 'go', '.'], ['i', 'must', 'go', '.'], ['i', 'must', 'go', '.'], ['i', 'must', 'go', '.'], ['i', 'need', 'it', '.'], ['i', 'need', 'it', '.'], ['i', 'noticed', '.'], ['i', 'prepaid', '.'], ['i', 'promise', '.'], ['i', 'relaxed', '.'], ['i', 'relaxed', '.'], ['i', 'retired', '.'], ['i', 'said', 'no', '.'], ['i', 'said', 'so', '.'], ['i', 'saw', 'him', '.'], ['i', 'saw', 'him', '.'], ['i', 'saw', 'him', '.'], ['i', 'saw', 'one', '.'], ['i', 'saw', 'one', '.'], ['i', 'saw', 'you', '.'], ['i', 'saw', 'you', '.'], ['i', 'saw', 'you', '.'], ['i', 'saw', 'you', '.'], ['i', 'saw', 'you', '.'], ['i', 'saw', 'you', '.'], ['i', 'saw', 'you', '.'], ['i', 'saw', 'you', '.']]\n",
      "[['va', '!'], ['salut', '!'], ['coursÃ¢â‚¬Â¯', '!'], ['courezÃ¢â‚¬Â¯', '!'], ['qui', '?'], ['Ã£â€¡a', 'alorsÃ¢â‚¬Â¯', '!'], ['au', 'feu', '!'], ['Ã£â‚¬', \"l'aideÃ¢â‚¬Â¯\", '!'], ['saute', '.'], ['Ã£â€¡a', 'suffitÃ¢â‚¬Â¯', '!'], ['stopÃ¢â‚¬Â¯', '!'], ['arrÃ£Âªte-toi', '!'], ['attends', '!'], ['attendez', '!'], ['poursuis', '.'], ['continuez', '.'], ['poursuivez', '.'], ['bonjour', '!'], ['salut', '!'], ['je', 'comprends', '.'], [\"j'essaye\", '.'], [\"j'ai\", 'gagnÃ£Â©', '!'], ['je', \"l'ai\", 'emportÃ£Â©', '!'], ['jÃ¢â‚¬â„¢ai', 'gagnÃ£Â©', '.'], ['oh', 'non', '!'], ['attaque', '!'], ['attaquez', '!'], ['santÃ£Â©', '!'], ['Ã£â‚¬', 'votre', 'santÃ£Â©', '!'], ['merci', '!'], ['tchin-tchin', '!'], ['lÃ£Â¨ve-toi', '.'], ['va', ',', 'maintenant', '.'], ['allez-y', 'maintenant', '.'], ['vas-y', 'maintenant', '.'], [\"j'ai\", 'pigÃ£Â©', '!'], ['compris', '!'], ['pigÃ£Â©Ã¢â‚¬Â¯', '?'], ['comprisÃ¢â‚¬Â¯', '?'], [\"t'as\", 'captÃ£Â©Ã¢â‚¬Â¯', '?'], ['monte', '.'], ['montez', '.'], ['serre-moi', 'dans', 'tes', 'bras', '!'], ['serrez-moi', 'dans', 'vos', 'bras', '!'], ['je', 'suis', 'tombÃ£Â©e', '.'], ['je', 'suis', 'tombÃ£Â©', '.'], ['je', 'sais', '.'], ['je', 'suis', 'parti', '.'], ['je', 'suis', 'partie', '.'], [\"j'ai\", 'perdu', '.'], ['jÃ¢â‚¬â„¢ai', 'payÃ£Â©', '.'], [\"j'ai\", '19', 'ans', '.'], ['je', 'vais', 'bien', '.'], ['Ã£â€¡a', 'va', '.'], ['Ã£â€°coutez', '!'], [\"c'est\", 'pas', 'possibleÃ¢â‚¬Â¯', '!'], ['impossibleÃ¢â‚¬Â¯', '!'], ['en', 'aucun', 'cas', '.'], ['sans', 'faÃ£Â§onsÃ¢â‚¬Â¯', '!'], [\"c'est\", 'hors', 'de', 'question', '!'], ['il', \"n'en\", 'est', 'pas', 'question', '!'], [\"c'est\", 'exclu', '!'], ['en', 'aucune', 'maniÃ£Â¨re', '!'], ['hors', 'de', 'question', '!'], ['vraimentÃ¢â‚¬Â¯', '?'], ['vrai', '?'], ['ah', 'bon', '?'], ['merci', '!'], ['on', 'essaye', '.'], ['nous', 'avons', 'gagnÃ£Â©', '.'], ['nous', 'gagnÃ£Â¢mes', '.'], ['nous', \"l'avons\", 'emportÃ£Â©', '.'], ['nous', \"l'emportÃ£Â¢mes\", '.'], ['demande', 'Ã£', '', 'tom', '.'], ['fantastiqueÃ¢â‚¬Â¯', '!'], ['sois', 'calme', '!'], ['soyez', 'calme', '!'], ['soyez', 'calmes', '!'], ['sois', 'dÃ£Â©tendu', '!'], ['sois', 'juste', '!'], ['soyez', 'juste', '!'], ['soyez', 'justes', '!'], ['sois', 'Ã£Â©quitable', '!'], ['soyez', 'Ã£Â©quitable', '!'], ['soyez', 'Ã£Â©quitables', '!'], ['sois', 'gentil', '.'], ['sois', 'gentil', '!'], ['sois', 'gentille', '!'], ['soyez', 'gentil', '!'], ['soyez', 'gentille', '!'], ['soyez', 'gentils', '!'], ['soyez', 'gentilles', '!'], ['dÃ£Â©gageÃ¢â‚¬Â¯', '!'], ['appelle-moi', '!'], ['appellez-moi', '!'], ['appelle-nous', '!'], ['appelez-nous', '!'], ['entrezÃ¢â‚¬Â¯', '!'], ['entre', '.'], ['entre', '!'], ['entrez', '!'], ['allezÃ¢â‚¬Â¯', '!'], ['allez', '!'], ['viens', '!'], ['venez', '!'], ['laisse', 'tomber', '!'], ['laissez', 'tomber', '!'], ['laisse-le', 'tomber', '!'], ['laissez-le', 'tomber', '!'], ['va', 'chercher', 'tom', '.'], ['sortezÃ¢â‚¬Â¯', '!'], ['sors', '!'], ['sortez', '!'], ['sors', '.'], ['casse-toi', '.'], ['dÃ£Â©gageÃ¢â‚¬Â¯', '!'], ['pars', '!'], ['va', 'te', 'faire', 'foutre', '!'], ['pars', '!'], ['dÃ£Â©gage', '!'], ['fous', 'le', 'camp', '!'], ['pars', \"d'ici\", '.'], ['va', \"t'en\", '!'], ['disparais', '!'], ['allez-vous', 'en', '!'], ['rentrez', 'Ã£', '', 'la', 'maison', '.'], ['rentre', 'Ã£', '', 'la', 'maison', '.'], ['rentre', 'chez', 'toi', '.'], ['rentrez', 'chez', 'vous', '.'], ['va', 'doucement', '!'], ['allez', 'doucement', '!'], ['Ã£â‚¬', 'la', 'revoyure', '.'], ['attends', 'un', 'peu', '!'], ['attendez', 'un', 'peu', '!'], ['tiens', 'bon', '!'], ['tenez', 'bon', '!'], ['il', 'laissa', 'tomber', '.'], ['il', 'a', 'laissÃ£Â©', 'tomber', '.'], ['il', 'court', '.'], ['aide-moi', '!'], ['aide-moi', '.'], ['aidez-moi', '.'], ['aidez-nous', '!'], ['aide-nous', '!'], ['ne', 'bouge', 'plus', '!'], ['ne', 'quittez', 'pas', '.'], ['fais', 'un', 'cÃ£Â¢lin', 'Ã£', '', 'tom', '.'], ['je', 'suis', 'du', 'mÃ£Âªme', 'avis', '.'], [\"j'ai\", 'pleurÃ£Â©', '.'], ['je', 'me', 'suis', 'assoupi', '.'], ['je', 'me', 'suis', 'assoupie', '.'], ['je', 'conduis', '.'], ['je', 'fume', '.'], ['je', 'ronfle', '.'], ['je', 'pue', '.'], ['je', 'me', 'suis', 'tenu', 'debout', '.'], ['je', 'me', 'suis', 'tenue', 'debout', '.'], ['jÃ¢â‚¬â„¢ai', 'promis', '.'], ['jÃ¢â‚¬â„¢ai', 'jurÃ£Â©', '.'], [\"j'essayai\", '.'], [\"j'ai\", 'essayÃ£Â©', '.'], [\"j'ai\", 'tentÃ£Â©', '.'], ['jÃ¢â‚¬â„¢ai', 'fait', 'signe', '.'], [\"j'irai\", '.'], ['je', 'suis', 'tom', '.'], ['je', 'suis', 'gras', '.'], ['je', 'suis', 'gros', '.'], ['je', 'suis', 'en', 'forme', '.'], ['je', 'suis', 'touchÃ£Â©', '!'], ['je', 'suis', 'touchÃ£Â©e', '!'], ['je', 'suis', 'malade', '.'], ['je', 'suis', 'triste', '.'], ['je', 'suis', 'timide', '.'], ['je', 'suis', 'mouillÃ£Â©', '.'], ['je', 'suis', 'mouillÃ£Â©e', '.'], [\"c'est\", 'bibiÃ¢â‚¬Â¯', '!'], ['joignez-vous', '.'], ['joignez-vous', 'Ã£', '', 'nous', '.'], ['garde-le', '!'], ['gardez-le', '!'], ['embrasse-moi', '.'], ['embrassez-moi', '.'], ['moi', 'aussi', '.'], ['ouvre-moiÃ¢â‚¬Â¯', '!'], ['ouvre', '.'], ['parfaitÃ¢â‚¬Â¯', '!'], ['Ã£â‚¬', 'plus', '.'], ['montre-moi', '!'], ['montrez-moi', '!'], ['taisez-vousÃ¢â‚¬Â¯', '!'], ['ferme-laÃ¢â‚¬Â¯', '!'], ['tais-toi', '!'], ['ferme-la', '!'], ['la', 'ferme', '!'], ['Ã£â‚¬', 'plus', 'tard', '!'], ['prends-le', '!'], ['prenez-le', '!'], ['dis-moi', '!'], ['dites-moi', '!'], ['tom', 'a', 'gagnÃ£Â©', '.'], ['rÃ£Â©veille-toiÃ¢â‚¬Â¯', '!'], ['rÃ£Â©veille-toi', '!'], ['rÃ£Â©veillez-vous', '!'], ['rÃ£Â©veille-toi', '!'], ['rÃ£Â©veillez-vous', '!'], ['lave-toi', '!'], ['lavez-vous', '!'], ['nous', 'savons', '.'], ['nous', 'perdÃ£Â®mes', '.'], ['nous', 'avons', 'perdu', '.'], ['nous', 'fÃ£Â»mes', 'battus', '.'], ['nous', 'fÃ£Â»mes', 'battues', '.'], ['nous', 'fÃ£Â»mes', 'dÃ£Â©faits', '.'], ['nous', 'fÃ£Â»mes', 'dÃ£Â©faites', '.'], ['nous', 'avons', 'Ã£Â©tÃ£Â©', 'dÃ£Â©faits', '.'], ['nous', 'avons', 'Ã£Â©tÃ£Â©', 'dÃ£Â©faites', '.'], ['nous', 'avons', 'Ã£Â©tÃ£Â©', 'battus', '.'], ['nous', 'avons', 'Ã£Â©tÃ£Â©', 'battues', '.'], ['bienvenueÃ¢â‚¬Â¯', '!'], ['qui', 'a', 'gagnÃ£Â©', '?'], ['qui', \"l'a\", 'emportÃ£Â©', '?'], ['tu', 'cours', '.'], ['suis-je', 'gros', '?'], ['suis-je', 'grosse', '?'], ['demande-leur', '.'], ['demandez-leur', '.'], ['reculeÃ¢â‚¬â€°', '!'], ['reculez', '.'], ['reculeÃ¢â‚¬â€°', '!'], ['reculez', '.'], ['retire-toiÃ¢â‚¬â€°', '!'], ['retirez-vous', '.'], ['sois', 'un', 'homme', '!'], ['soyez', 'un', 'homme', '!'], ['sois', 'calme', '!'], ['soyez', 'calme', '!'], ['soyez', 'calmes', '!'], ['aucune', 'idÃ£Â©e', '.'], [\"j'en\", 'sais', 'foutre', 'rien', '.'], ['appelle', 'tom', '.'], ['appelez', 'tom', '.'], ['courageÃ¢â‚¬Â¯', '!'], ['dÃ£Â©tends-toiÃ¢â‚¬Â¯', '!'], ['menottez-le', '.'], ['avance', '!'], ['avancez', '!'], ['continue', 'Ã£', '', 'rouler', '!'], ['continuez', 'Ã£', '', 'rouler', '!'], ['trouve', 'tom', '.'], ['trouvez', 'tom', '.'], ['rÃ£Â©parez', 'ceci', '.'], ['rÃ£Â©pare', 'Ã£Â§a', '.'], ['lÃ£Â¢che-toi', '!'], ['descends', '!'], ['descendez', '!'], ['lÃ£Â¢che-toi', '!'], ['lÃ£Â¢chez-vous', '!'], ['va', 'voir', 'ailleurs', 'si', \"j'y\", 'suisÃ¢â‚¬Â¯', '!'], ['dÃ£Â©gageÃ¢â‚¬Â¯', '!'], ['va', 'au', 'diable', '!'], ['sois', 'rÃ£Â©aliste', '!'], ['vas-y', '.'], ['poursuis', '!'], ['passe', 'devant', '!'], ['vas-y', '!'], ['bien', 'jouÃ£Â©Ã¢â‚¬Â¯', '!'], ['bon', 'boulotÃ¢â‚¬Â¯', '!'], ['beau', 'travailÃ¢â‚¬Â¯', '!'], ['attrape-le', '.'], ['attrapez-le', '.'], ['amuse-toi', 'bien', '!'], ['amusez-vous', 'bien', '!'], ['il', 'essaye', '.'], ['il', 'est', 'mouillÃ£Â©', '.'], ['aide', 'tom', '.'], ['aidez', 'tom', '.'], ['salut', ',', 'les', 'mecs', '!'], ['comme', \"c'est\", 'mignonÃ¢â‚¬Â¯', '!'], ['quelle', 'profondeurÃ¢â‚¬Â¯', '?'], ['comme', \"c'est\", 'chouette', '!'], ['comme', \"c'est\", 'gentil', '!'], [\"c'est\", 'du', 'joli', '!'], ['comme', \"c'est\", 'agrÃ£Â©able', '!'], ['fais-moi', 'rire', '.'], ['dÃ£Â©pÃ£Âªche-toi', '.'], ['grouilleÃ¢â‚¬Â¯', '!'], ['pressez-vous', '!'], ['fiÃ£Â§a', '!'], ['je', 'suis', 'gras', '.'], ['je', \"m'en\", 'suis', 'bien', 'sorti', '.'], ['je', \"m'en\", 'suis', 'bien', 'sortie', '.'], ['je', \"l'ai\", 'fait', '.'], [\"c'est\", 'moi', 'qui', \"l'ai\", 'fait', '.'], [\"j'ai\", 'Ã£Â©chouÃ£Â©', '.'], [\"j'ai\", 'oubliÃ£Â©', '.'], [\"j'ai\", 'compris', '.'], [\"j'ai\", 'compris', '.'], [\"j'ai\", 'captÃ£Â©', '.'], [\"j'ai\", 'aidÃ£Â©', '.'], [\"j'ai\", 'sautÃ£Â©', '.'], ['jÃ¢â‚¬â„¢ai', 'regardÃ£Â©', '.'], ['jÃ¢â‚¬â„¢ai', 'rÃ£Â¢lÃ£Â©', '.'], ['jÃ¢â‚¬â„¢ai', 'fait', 'signe', 'de', 'la', 'tÃ£Âªte', '.'], ['jÃ¢â‚¬â„¢ai', 'obÃ£Â©i', '.'], ['je', 'tÃ£Â©lÃ£Â©phonai', '.'], [\"j'ai\", 'tÃ£Â©lÃ£Â©phonÃ£Â©', '.'], ['je', 'refuse', '.'], ['je', 'le', 'refuse', '.'], ['je', 'me', 'suis', 'reposÃ£Â©', '.'], ['je', 'me', 'suis', 'reposÃ£Â©e', '.'], ['je', \"l'ai\", 'vu', '.'], ['je', 'lÃ¢â‚¬â„¢ai', 'vu', '.'], ['jÃ¢â‚¬â„¢ai', 'soupirÃ£Â©', '.'], ['je', 'suis', 'restÃ£Â©', '.'], ['je', 'suis', 'restÃ£Â©e', '.'], ['jÃ¢â‚¬â„¢ai', 'parlÃ£Â©', '.'], ['je', \"l'utilise\", '.'], [\"j'en\", 'fais', 'usage', '.'], ['je', \"m'en\", 'sers', '.'], ['je', 'paierai', '.'], ['je', 'vais', 'essayer', '.'], [\"j'essaierai\", '.'], ['je', 'suis', 'revenu', '.'], ['me', 'revoilÃ£', '.'], ['je', 'suis', 'chauve', '.'], ['je', 'suis', 'occupÃ£Â©', '.'], ['je', 'suis', 'occupÃ£Â©e', '.'], ['je', 'suis', 'calme', '.'], [\"j'ai\", 'froid', '.'], ['je', 'suis', 'dÃ£Â©tendu', '.'], ['je', 'suis', 'dÃ£Â©tendue', '.'], ['je', 'suis', 'sourd', '.'], ['je', 'suis', 'sourde', '.'], [\"j'en\", 'ai', 'fini', '.'], ['je', 'suis', 'juste', '.'], [\"j'ai\", 'la', 'peau', 'claire', '.'], [\"j'ai\", 'le', 'teint', 'clair', '.'], ['je', 'suis', 'rapide', '.'], ['tout', 'va', 'bien', '.'], ['je', 'vais', 'bien', '.'], ['Ã£â€¡a', 'va', '.'], ['je', 'suis', 'libre', '!'], ['je', 'suis', 'libre', '.'], ['je', 'suis', 'disponible', '.'], ['je', 'suis', 'repuÃ¢â‚¬Â¯', '!'], ['je', 'suis', 'rassasiÃ£Â©Ã¢â‚¬Â¯', '!'], [\"j'en\", 'suis', '.'], ['je', 'suis', 'de', 'la', 'partie', '.'], ['je', 'suis', 'content', '.'], ['je', 'suis', 'chez', 'moi', '.'], ['je', 'suis', 'en', 'retard', '.'], ['je', 'suis', 'paresseux', '.'], ['je', 'suis', 'fainÃ£Â©ant', '.'], ['je', 'suis', 'paresseuse', '.'], ['je', 'suis', 'fainÃ£Â©ante', '.'], ['je', 'vais', 'bien', '.'], ['je', 'me', 'porte', 'bien', '.'], ['je', 'suis', 'riche', '.'], ['je', 'suis', 'en', 'sÃ£Â©curitÃ£Â©', '.'], ['je', 'suis', 'malade', '.'], [\"j'en\", 'suis', 'certain', '.'], ['je', 'suis', 'certain', '.'], [\"j'en\", 'suis', 'sÃ£Â»r', '.'], [\"j'en\", 'suis', 'sÃ£Â»re', '.'], ['je', 'suis', 'grande', '.'], ['je', 'suis', 'mince', '.'], ['je', 'suis', 'ordonnÃ£Â©', '.'], ['je', 'suis', 'ordonnÃ£Â©e', '.'], ['je', 'suis', 'laid', '.'], ['je', 'suis', 'laide', '.'], ['je', 'suis', 'faible', '.'], ['je', 'vais', 'bien', '.'], ['je', 'me', 'porte', 'bien', '.'], [\"j'ai\", 'gagnÃ£Â©', '.'], ['je', \"l'ai\", 'emportÃ£Â©', '.'], ['Ã£â€¡a', 'aide', '.'], ['Ã£â€¡a', 'fait', 'mal', '.'], ['elle', 'marche', '.'], ['Ã£â€¡a', 'fonctionne', '.'], [\"c'est\", 'tom', '.'], [\"c'est\", 'marrant', '.'], [\"c'est\", 'rigolo', '.'], [\"c'est\", 'le', 'sien', '.'], [\"c'est\", 'la', 'sienne', '.'], [\"c'est\", 'nouveau', '.'], [\"c'est\", 'neuf', '.'], [\"c'est\", 'bizarre', '.'], ['il', 'est', 'rouge', '.'], ['cÃ¢â‚¬â„¢est', 'triste', '.'], ['dÃ£Â©fense', \"d'entrer\", '.'], [\"n'entrez\", 'pas', '.'], ['embrasse', 'tom', '.'], ['laisse', 'tomber', '!'], ['laissez', 'tomber', '!'], ['laissez-moi', '!'], ['laisse-nous', '!'], ['laissez-nous', '!'], ['allons-y', '!'], ['allons-y', '!'], ['attention', '!'], ['regarde', 'donc', '!'], ['Ã£â€°pouse-moi', '!'], ['Ã£â€°pousez-moi', '!'], ['puis-je', 'partir', '?'], ['puis-je', 'y', 'aller', '?'], ['puis-je', \"m'y\", 'rendre', '?'], ['sauve', 'tom', '.'], ['sauvez', 'tom', '.'], ['elle', 'est', 'venue', '.'], ['elle', 'est', 'morte', '.'], ['elle', 'court', '.'], ['assieds-toi', '!'], ['asseyez-vous', '!'], ['assieds-toi', '.'], ['assieds-toi', 'ici', '.'], ['asseyez-vous', 'ici', '.'], ['parle', 'plus', 'fortÃ¢â‚¬Â¯', '!'], ['parlez', 'plus', 'fortÃ¢â‚¬Â¯', '!'], ['arrÃ£Âªte', 'tom', '.'], ['stoppez', 'tom', '.'], ['goÃ£Â»te-le', '.'], ['goÃ£Â»te-la', '.'], ['goÃ£Â»tez-le', '.'], ['goÃ£Â»tez-la', '.'], ['dis-le', 'Ã£', '', 'tom', '.'], ['informez-en', 'tom', '.'], ['gÃ£Â©nialÃ¢â‚¬Â¯', '!'], ['excellentÃ¢â‚¬Â¯', '!'], ['formidable', '!'], ['ils', 'gagnÃ£Â¨rent', '.'], ['elles', 'gagnÃ£Â¨rent', '.'], ['ils', 'ont', 'gagnÃ£Â©', '.'], ['elles', 'ont', 'gagnÃ£Â©', '.'], ['tom', 'est', 'venu', '.'], ['tom', 'est', 'mort', '.'], ['tom', 'muera', '.'], ['tom', 'savait', '.'], ['tom', 'est', 'parti', '.'], ['tom', 'partit', '.'], ['tom', 'a', 'menti', '.'], ['tom', 'ment', '.'], ['tom', 'a', 'perdu', '.'], ['tom', 'a', 'payÃ£Â©', '.'], ['trop', 'tard', '.'], ['faites-moi', 'confiance', '.'], ['fais-moi', 'confiance', '.'], ['fais', 'un', 'effort', '.'], ['essaies-en', '!'], ['essayez-en', '!'], ['essaie', 'ceci', '!'], ['essayez', 'ceci', '!'], ['utilise', 'ceci', '.'], ['utilisez', 'ceci', '.'], ['emploie', 'ceci', '!'], ['employez', 'ceci', '!'], ['avertis', 'tom', '.'], ['prÃ£Â©viens', 'tom', '.'], ['regarde-moi', '!'], ['regardez-moi', '!'], ['regardez-nous', '!'], ['regarde-nous', '!'], ['nous', 'sommes', \"d'accord\", '.'], ['nous', 'irons', '.'], ['pour', 'quoi', 'faireÃ¢â‚¬Â¯', '?'], ['Ã£â‚¬', 'quoi', 'bon', '?'], [\"qu'est-ce\", \"qu'on\", \"s'est\", 'marrÃ£Â©s', '!'], [\"qu'est-ce\", \"qu'on\", \"s'est\", 'marrÃ£Â©es', '!'], ['qui', 'est', 'venu', '?'], ['qui', 'est', 'mort', '?'], ['qui', 'est', 'tombÃ£Â©Ã¢', '?'], ['qui', 'dÃ£Â©missionne', '?'], ['qui', 'est-ilÃ¢â‚¬Â¯', '?'], ['Ã£â€°cris-moi', '!'], ['Ã£â€°crivez-moi', '!'], ['tu', 'as', 'perdu', '.'], ['vous', 'avez', 'perdu', '.'], ['aprÃ£Â¨s', 'vous', '.'], ['en', 'joue', '!', 'feu', '!'], ['suis-je', 'en', 'retard', '?'], ['rÃ£Â©pondez-moi', '.'], ['assieds-toi', '!'], ['asseyez-vous', '!'], ['les', 'oiseaux', 'volent', '.'], ['Ã£â‚¬', 'tes', 'souhaitsÃ¢â‚¬Â¯', '!'], ['appelle', 'Ã£', '', 'la', 'maison', '!'], ['calmez-vous', '!'], ['calme-toi', '.'], ['pouvons-nous', 'partir', '?'], ['pouvons-nous', 'nous', 'en', 'aller', '?'], ['pouvons-nous', 'y', 'aller', '?'], ['attrape', 'tom', '.'], ['attrapez', 'tom', '.'], ['rattrape-le', '.'], ['reviens', '!'], ['revenez', '!'], ['viens', 'ici', '.'], ['venez', 'lÃ£', '.'], ['viensÃ¢â‚¬Â¯', '!'], ['venezÃ¢â‚¬Â¯', '!'], ['venez', 'ici', '!'], ['viens', 'chez', 'nous', '!'], ['venez', 'chez', 'nous', '!'], ['viens', 'chez', 'moi', '!'], ['venez', 'chez', 'moi', '!'], ['viens', 'bientÃ£Â´t', '!'], ['venez', 'bientÃ£Â´t', '!'], ['calmez-vous', '!'], ['ai-je', 'gagnÃ£Â©', '?'], [\"l'ai-je\", 'emportÃ£Â©', '?'], ['est-ce', 'moi', 'qui', 'ai', 'gagnÃ£Â©', '?'], ['faites-le', 'maintenant', '.'], ['des', 'chiens', 'aboient', '.'], ['les', 'chiens', 'aboient', '.'], ['ne', 'demande', 'pas', '!'], ['ne', 'pleure', 'pas', '!'], ['ne', 'meurs', 'pas', '!'], ['ne', 'mourez', 'pas', '!'], ['ne', 'mens', 'pas', '.'], ['ne', 'courez', 'pas', '.'], ['ne', 'cours', 'pas', '.'], ['excuse-moi', '.'], ['excusez-moi', '.'], ['fantastiqueÃ¢â‚¬Â¯', '!'], ['sens', 'Ã£Â§a', '!'], ['sentez', 'Ã£Â§a', '!'], ['touche', 'Ã£Â§a', '!'], ['touchez', 'Ã£Â§a', '!'], ['suis-moi', '.'], ['suis-nous', '!'], ['suivez-nous', '!'], ['oublie', '!'], ['oublie-le', '!'], ['oubliez', '!'], ['oubliez-le', '!'], ['laisse', 'tomber', '.'], ['oublie', '.'], ['oublie-le', '!'], ['trouve', 'un', 'emploi', '!'], ['trouve', 'un', 'boulot', '!'], ['trouvez', 'un', 'emploi', '!'], ['trouvez', 'un', 'boulot', '!'], ['prÃ£Â©pare-toi', '.'], ['prÃ£Â©parez-vous', '.'], ['va', 'le', 'chercher', '!'], ['allez', 'le', 'chercher', '!'], ['entrezÃ¢â‚¬Â¯', '!'], ['va', 'au', 'lit', '!'], ['allez', 'au', 'lit', '!'], ['bonne', 'chanceÃ¢â‚¬Â¯', '!'], ['bonne', 'chance', '.'], ['attrape', 'Ã£Â§a', '!'], ['attrapez', 'Ã£Â§a', '!'], ['saisis-toi', 'de', 'Ã£Â§a', '!'], ['saisissez-vous', 'de', 'Ã£Â§a', '!'], ['attrape', 'Ã£Â§a', '!'], ['attrapez', 'Ã£Â§a', '!'], ['pas', 'toucheÃ¢â‚¬Â¯', '!'], ['il', 'est', 'malade', '.'], ['il', 'est', 'vieux', '.'], ['il', 'est', 'dj', '.'], ['il', 'est', 'bon', '.'], ['il', 'est', 'paresseux', '.'], ['il', 'est', 'riche', '.'], ['il', 'est', 'sexy', '.'], ['me', 'voici', '.'], ['voilÃ£', '', 'cinq', 'dollars', '.'], ['halte', 'au', 'feu', '!'], ['cessez', 'le', 'feu', '!'], ['tiens', 'Ã£Â§a', '!'], ['tenez', 'Ã£Â§a', '!'], ['tenez', 'ceci', '!'], ['tiens', 'ceci', '!'], [\"c'est\", 'affreuxÃ¢â‚¬Â¯', '!'], ['comme', \"c'est\", 'bizarre', '!'], ['comment', 'tom', 'va-t-il', '?'], ['comment', 'va', 'tom', '?'], ['je', 'suis', 'occupÃ£Â©', '.'], ['je', 'suis', 'calme', '.'], [\"j'ai\", 'froid', '.'], ['je', 'vais', 'bien', '.'], ['je', 'suis', 'bon', '.'], ['je', 'suis', 'ici', '.'], ['je', 'suis', 'paresseux', '.'], ['je', 'suis', 'fainÃ£Â©ant', '.'], ['je', 'suis', 'paresseuse', '.'], ['je', 'suis', 'fainÃ£Â©ante', '.'], ['je', 'vais', 'bien', '.'], ['je', 'suis', 'malade', '.'], ['je', 'suis', 'sÃ£Â»r', '.'], ['je', 'suis', 'certain', '.'], ['je', 'suis', 'faible', '.'], ['je', 'vous', 'en', 'prie', '.'], ['je', 'vous', 'en', 'conjure', '.'], ['je', 'vous', 'en', 'supplie', '.'], ['je', 'te', 'prie', '.'], ['je', 'sais', 'courir', '.'], ['je', 'sais', 'skier', '.'], [\"j'eus\", 'un', 'mouvement', 'de', 'recul', '.'], [\"j'ai\", 'eu', 'un', 'mouvement', 'de', 'recul', '.'], ['je', 'suis', 'rentrÃ£Â©', 'en', 'moi-mÃ£Âªme', '.'], ['jÃ¢â‚¬â„¢ai', 'expirÃ£Â©', '.'], [\"j'ai\", 'abandonnÃ£Â©', '.'], ['je', 'donne', 'ma', 'langue', 'au', 'chat', '.'], [\"j'abandonne\", '.'], ['je', 'me', 'suis', 'mis', 'Ã£', '', 'avoir', 'chaud', '.'], ['je', 'me', 'suis', 'mise', 'Ã£', '', 'avoir', 'chaud', '.'], ['je', 'me', 'suis', 'amusÃ£Â©', '.'], ['je', 'me', 'suis', 'amusÃ£Â©e', '.'], ['je', 'me', 'suis', 'marrÃ£Â©', '.'], ['je', 'me', 'suis', 'marrÃ£Â©e', '.'], ['je', 'dÃ£Â©teste', 'Ã£Â§a', '.'], ['je', \"l'ai\", '.'], [\"j'ai\", 'frappÃ£Â©', 'tom', '.'], [\"j'espÃ£Â¨re\", 'bien', '.'], ['je', 'me', 'suis', 'dÃ£Â©pÃ£ÂªchÃ£Â©', '.'], ['je', 'me', 'suis', 'dÃ£Â©pÃ£ÂªchÃ£Â©e', '.'], ['jÃ¢â‚¬â„¢ai', 'inspirÃ£Â©', '.'], ['je', 'le', 'savais', '.'], [\"j'aime\", 'Ã£Â§a', '.'], ['je', 'lÃ¢â‚¬â„¢ai', 'perdu', '.'], [\"j'adore\", 'Ã£Â§a', '!'], [\"j'adore\", 'Ã£Â§a', '!'], ['je', 'suis', 'sÃ£Â©rieuxÃ¢â‚¬Â¯', '!'], ['je', 'suis', 'sÃ£Â©rieux', '.'], ['je', 'dois', 'y', 'aller', '.'], ['il', 'faut', 'que', \"j'y\", 'aille', '.'], ['il', 'me', 'faut', 'y', 'aller', '.'], ['il', 'me', 'faut', 'partir', '.'], ['il', 'me', 'faut', \"m'en\", 'aller', '.'], ['je', 'dois', 'partir', '.'], ['je', 'dois', \"m'en\", 'aller', '.'], ['il', 'faut', 'que', 'je', \"m'en\", 'aille', '.'], [\"j'en\", 'ai', 'besoin', '.'], ['il', 'me', 'le', 'faut', '.'], [\"j'ai\", 'remarquÃ£Â©', '.'], [\"j'ai\", 'payÃ£Â©', \"d'avance\", '.'], ['je', 'le', 'promets', '.'], ['je', 'me', 'suis', 'dÃ£Â©tendu', '.'], ['je', 'me', 'suis', 'dÃ£Â©tendue', '.'], [\"j'ai\", 'pris', 'ma', 'retraite', '.'], [\"j'ai\", 'dit', 'non', '.'], ['je', \"l'ai\", 'dit', '.'], ['je', \"l'ai\", 'vu', '.'], ['je', 'lÃ¢â‚¬â„¢ai', 'vu', '.'], ['je', 'le', 'vis', '.'], [\"j'en\", 'ai', 'vu', 'une', '.'], [\"j'en\", 'ai', 'vu', 'un', '.'], ['je', 'vous', 'vis', '.'], ['je', 'te', 'vis', '.'], ['je', \"t'ai\", 'vue', '.'], ['je', \"t'ai\", 'vu', '.'], ['je', 'vous', 'ai', 'vues', '.'], ['je', 'vous', 'ai', 'vus', '.'], ['je', 'vous', 'ai', 'vue', '.'], ['je', 'vous', 'ai', 'vu', '.']]\n"
     ]
    }
   ],
   "source": [
    "# print(text)\n",
    "print(source)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23408/824821610.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_nmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23408/4159522448.py\u001b[0m in \u001b[0;36mload_data_nmt\u001b[1;34m(batch_size, num_steps, num_examples)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mtgt_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreserved_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<pad>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'<bos>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0msrc_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_valid_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_array_nmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0mtgt_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_valid_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_array_nmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23408/4159522448.py\u001b[0m in \u001b[0;36mbuild_array_nmt\u001b[1;34m(lines, vocab, num_steps)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;34m\"\"\"Transform text sequences of machine translation into mini-batches.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     array = torch.tensor([truncate_pad(\n\u001b[0;32m     72\u001b[0m         l, num_steps, vocab['<pad>']) for l in lines])\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23408/4159522448.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;34m\"\"\"Transform text sequences of machine translation into mini-batches.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     array = torch.tensor([truncate_pad(\n\u001b[0;32m     72\u001b[0m         l, num_steps, vocab['<pad>']) for l in lines])\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 10\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                          value=-1e6)\n",
    "        return F.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Additive attention.\"\"\"\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of `queries` is: (`batch_size`, no. \n",
    "        # of queries, 1, `num_hiddens`) and shape of `keys` is: (`batch_size`, \n",
    "        # 1, no. of key-value pairs, `num_hiddens`). Sum them up with\n",
    "        # broadcasting\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # There is only one output of `self.w_v`, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of `scores` is:\n",
    "        # (`batch_size`, no. of queries, no. of key-value pairs)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of `values` is: (`batch_size`, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class AttentionDecoder(Decoder):\n",
    "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n",
    "        self.attention = AdditiveAttention(\n",
    "            num_hiddens, num_hiddens, num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "            dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        # Shape of `outputs` is: (`num_steps`, `batch_size`, `num_hiddens`).\n",
    "        # Shape of `hidden_state[0]` is: (`num_layers`, `batch_size`,\n",
    "        # `num_hiddens`)\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # Shape of `enc_outputs` is: (`batch_size`, `num_steps`, `num_hiddens`).\n",
    "        # Shape of `hidden_state[0]` is: (`num_layers`, `batch_size`,\n",
    "        # `num_hiddens`)\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        # Shape of the output `X` is: (`num_steps`, `batch_size`, `embed_size`)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            # Shape of `query` is: (`batch_size`, 1, `num_hiddens`)\n",
    "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
    "            # Shape of `context` is: (`batch_size`, 1, `num_hiddens`)\n",
    "            context = self.attention(\n",
    "                query, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "            # Concatenate on the feature dimension\n",
    "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
    "            # Reshape `x` as (1, `batch_size`, `embed_size` + `num_hiddens`)\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        # After fully-connected layer transformation, shape of `outputs` is:\n",
    "        # (`num_steps`, `batch_size`, `vocab_size`)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n",
    "                                          enc_valid_lens]\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
    "        X = self.embedding(X)\n",
    "        # In RNN models, the first axis corresponds to time steps\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # When state is not mentioned, it defaults to zeros\n",
    "        output, state = self.rnn(X)\n",
    "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
    "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "import requests\n",
    "\n",
    "def download(url: str) -> str:\n",
    "    \"\"\"Download a file, return the local filename.\"\"\"\n",
    "    file_path = '../data/' + url.split('/')[-1]\n",
    "    if os.path.exists(file_path):\n",
    "        return file_path\n",
    "    \n",
    "    print(f'Downloading {file_path} from {url}...')\n",
    "    res = requests.get(url, stream=True, verify=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(res.content)\n",
    "    return file_path\n",
    "\n",
    "def read_time_machine() -> list[str]:\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    file = download('http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt')\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    lines_transformed = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "    return [line for line in lines_transformed if line]\n",
    "\n",
    "def tokenize(lines: list[str], token: Literal['word', 'char']='word') -> list[str]:\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        tokens: list[list[str]] =  [line.split() for line in lines]\n",
    "    else:\n",
    "        tokens: list[list[str]] = [list(line) for line in lines]\n",
    "    return [token for sublist in tokens for token in sublist]\n",
    "    \n",
    "def count_corpus(tokens: list[str]) -> collections.Counter[str]:\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens: list[str]=[], min_freq=0, reserved_tokens: list[str]=[]):\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens + [token for token, freq in self._token_freqs if freq >= min_freq]\n",
    "        self.token_to_idx =  {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens: str | list[str]) -> int | list[int]:\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.token_to_idx.get(token, self.unk) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices: int | list[int]) -> str | list[str]:\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self) -> Literal[0]:  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self) -> list[tuple[str, int]]:  # Token frequencies\n",
    "        return self._token_freqs\n",
    "\n",
    "\n",
    "class SeqDataLoader:\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size: int, num_steps: int, max_tokens: int=10000):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.vocab = self.generate_vocab()\n",
    "        self.corpus = self.generate_corpus(max_tokens)\n",
    "        \n",
    "    def generate_vocab(self) -> Vocab:\n",
    "        lines = read_time_machine()\n",
    "        tokens = tokenize(lines, 'char')\n",
    "        return Vocab(tokens)\n",
    "    \n",
    "    def generate_corpus(self, max_tokens):\n",
    "        tokens = tokenize(read_time_machine(), 'char')\n",
    "        corpus = [self.vocab[token] for line in tokens for token in line]\n",
    "        return corpus[:max_tokens]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate a mini-batch of subsequences using sequential partitioning.\"\"\"\n",
    "        # Start with a random offset to partition a sequence\n",
    "        offset = random.randint(0, self.num_steps)\n",
    "        num_tokens = ((len(self.corpus) - offset - 1) // self.batch_size) * self.batch_size\n",
    "        Xs = torch.tensor(self.corpus[offset: offset + num_tokens])\n",
    "        Ys = torch.tensor(self.corpus[offset + 1: offset + 1 + num_tokens])\n",
    "        Xs, Ys = Xs.reshape(self.batch_size, -1), Ys.reshape(self.batch_size, -1)\n",
    "        num_batches = Xs.shape[1] // self.num_steps\n",
    "        for i in range(0, self.num_steps * num_batches, self.num_steps):\n",
    "            X = Xs[:, i: i + self.num_steps]\n",
    "            Y = Ys[:, i: i + self.num_steps]\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 50\n",
    "steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = SeqDataLoader(batch, steps)\n",
    "vocab = train_iter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size = len(vocab)\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = nn.LSTM(input_size, hidden_size)\n",
    "net = nn.RNN(lstm_layer, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = train(net, train_iter, vocab, lr=1, num_epochs=100, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
