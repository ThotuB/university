{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygJsrb4_7QS2"
      },
      "source": [
        "The following imports and functions are the same as the ones in *Laboratory 5*. We repeat them here for convenience. Also, please make sure the GPU hardware accelerator is selected in the *Change runtime type* dialog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LYMPCfZHTgeb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIfSULLZOXS",
        "outputId": "738ce3ee-7597-436a-f61a-cef3b77859cb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Iuzvx8aG0P"
      },
      "outputs": [],
      "source": [
        "def load_data_fashion_mnist(batch_size, resize=None):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
        "    trans = [transforms.ToTensor()]\n",
        "    if resize:\n",
        "        trans.insert(0, transforms.Resize(resize))\n",
        "    trans = transforms.Compose(trans)\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=True, transform=trans, download=True)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=False, transform=trans, download=True)\n",
        "    mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [50000, 10000],\n",
        "                                                           generator=torch.Generator().manual_seed(42))\n",
        "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                            num_workers=2),\n",
        "            torch.utils.data.DataLoader(mnist_val, batch_size, shuffle=False,\n",
        "                            num_workers=2),\n",
        "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
        "                            num_workers=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwi0Tc14ajyb"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy(net, data_iter, loss, device):\n",
        "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_loss = 0\n",
        "    total_hits = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            total_loss += float(l)\n",
        "            total_hits += sum(net(X).argmax(axis=1).type(y.dtype) == y)\n",
        "            total_samples += y.numel()\n",
        "    return float(total_loss) / len(data_iter), float(total_hits) / total_samples  * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1D4ZRJ2anNn"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net, train_iter, loss, optimizer, device):  \n",
        "    # Set the model to training mode\n",
        "    net.train()\n",
        "    # Sum of training loss, sum of training correct predictions, no. of examples\n",
        "    total_loss = 0\n",
        "    total_hits = 0\n",
        "    total_samples = 0\n",
        "    for X, y in train_iter:\n",
        "        # Compute gradients and update parameters\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat = net(X)\n",
        "        l = loss(y_hat, y)\n",
        "        # Using PyTorch built-in optimizer & loss criterion\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(l)\n",
        "        total_hits += sum(y_hat.argmax(axis=1).type(y.dtype) == y)\n",
        "        total_samples += y.numel()\n",
        "    # Return training loss and training accuracy\n",
        "    return float(total_loss) / len(train_iter), float(total_hits) / total_samples  * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX_Tn-VPaqgw"
      },
      "outputs": [],
      "source": [
        "def train(net, train_iter, val_iter, test_iter, num_epochs, lr, device):\n",
        "    \"\"\"Train a model.\"\"\"\n",
        "    train_loss_all = []\n",
        "    train_acc_all = []\n",
        "    val_loss_all = []\n",
        "    val_acc_all = []\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "    net.apply(init_weights)\n",
        "    print('Training on', device)\n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_epoch(net, train_iter, loss, optimizer, device)\n",
        "        train_loss_all.append(train_loss)\n",
        "        train_acc_all.append(train_acc)\n",
        "        val_loss, val_acc = evaluate_accuracy(net, val_iter, loss, device)\n",
        "        val_loss_all.append(val_loss)\n",
        "        val_acc_all.append(val_acc)\n",
        "        print(f'Epoch {epoch + 1}, Train loss {train_loss:.2f}, Train accuracy {train_acc:.2f}, Validation loss {val_loss:.2f}, Validation accuracy {val_acc:.2f}')\n",
        "    test_loss, test_acc = evaluate_accuracy(net, test_iter, loss, device)\n",
        "    print(f'Test loss {test_loss:.2f}, Test accuracy {test_acc:.2f}')\n",
        "\n",
        "    return train_loss_all, train_acc_all, val_loss_all, val_acc_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLtbllYl5OI1"
      },
      "outputs": [],
      "source": [
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ--0hfkXqom"
      },
      "outputs": [],
      "source": [
        "def plot_loss(train_loss_all, val_loss_all):\n",
        "    epochs = range(1, len(train_loss_all) + 1) \n",
        "    plt.plot(epochs, train_loss_all, 'bo', label='Training loss') \n",
        "    plt.plot(epochs, val_loss_all, 'b', label='Validation loss') \n",
        "    plt.title('Training and validation loss') \n",
        "    plt.xlabel('Epochs') \n",
        "    plt.ylabel('Loss') \n",
        "    plt.legend()  \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG-Go78KXwwt"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy(train_acc_all, val_acc_all):\n",
        "    epochs = range(1, len(train_acc_all) + 1)\n",
        "    plt.plot(epochs, train_acc_all, 'bo', label='Training acc')\n",
        "    plt.plot(epochs, val_acc_all, 'b', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs') \n",
        "    plt.ylabel('Accuracy') \n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9RRJKsgdMR8"
      },
      "source": [
        "# Deep convolutional neural networks (AlexNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da9zWQXyrMq-"
      },
      "source": [
        "The AlexNet model can also be easily implemented using the `nn.Sequential` class. Note that AlexNet changed the sigmoid activation function to a simpler ReLU activation function. AlexNet controls the model complexity of the fully-connected layer by dropout. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sPUJsGN1q0U",
        "outputId": "6593771e-7791-4b17-e7ca-c1d9320c7043"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (4): ReLU()\n",
              "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (7): ReLU()\n",
              "  (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (9): ReLU()\n",
              "  (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU()\n",
              "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (13): Flatten(start_dim=1, end_dim=-1)\n",
              "  (14): Linear(in_features=6400, out_features=4096, bias=True)\n",
              "  (15): ReLU()\n",
              "  (16): Dropout(p=0.5, inplace=False)\n",
              "  (17): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (18): ReLU()\n",
              "  (19): Dropout(p=0.5, inplace=False)\n",
              "  (20): Linear(in_features=4096, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(\n",
        "    # Here, we use a larger 11 x 11 window to capture objects. At the same\n",
        "    # time, we use a stride of 4 to greatly reduce the height and width of the\n",
        "    # output. Here, the number of output channels is much larger than that in\n",
        "    # LeNet\n",
        "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    # Make the convolution window smaller, set padding to 2 for consistent\n",
        "    # height and width across the input and output, and increase the number of\n",
        "    # output channels\n",
        "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    # Use three successive convolutional layers and a smaller convolution\n",
        "    # window. Except for the final convolutional layer, the number of output\n",
        "    # channels is further increased. Pooling layers are not used to reduce the\n",
        "    # height and width of input after the first two convolutional layers\n",
        "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    nn.Flatten(),\n",
        "    # Here, the number of outputs of the fully-connected layer is several\n",
        "    # times larger than that in LeNet. Use the dropout layer to mitigate\n",
        "    # overfitting\n",
        "    nn.Linear(6400, 4096), nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(4096, 4096), nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),\n",
        "    # Output layer. Since we are using Fashion-MNIST, the number of classes is\n",
        "    # 10, instead of 1000 as in the paper\n",
        "    nn.Linear(4096, 10))\n",
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRIZPxTltbhP"
      },
      "source": [
        "We construct a single-channel data example with both height and width of $224$ to observe the output shape of each layer. It matches the AlexNet architecture presented in the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3a-bZrT2wF3",
        "outputId": "d6230c4c-5da0-458a-c339-480414e75842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
            "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
            "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
            "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
            "Flatten output shape:\t torch.Size([1, 6400])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "X = torch.randn(1, 1, 224, 224)\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6801RbhuFY0"
      },
      "source": [
        "Although AlexNet is trained on ImageNet in the paper, we use Fashion-MNIST here,\n",
        "since training an ImageNet model to convergence could take hours or days\n",
        "even on a modern GPU. One of the problems with applying AlexNet directly on Fashion-MNIST is that its images have lower resolution ($28 \\times 28$ pixels)\n",
        "than ImageNet images.To make things work, we upsample them to $224 \\times 224$ (generally not a smart practice, but we do it here to be faithful to the AlexNet architecture). We perform this resizing with the `resize` argument in the `load_data_fashion_mnist()` function.\n",
        "\n",
        "Now, we can start training AlexNet. Compared with LeNet,\n",
        "the main change here is the use of a smaller learning rate\n",
        "and much slower training, due to the deeper and wider network,\n",
        "the higher image resolution, and the more costly convolutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUNrEfqr2cQh",
        "outputId": "bbe8ac52-d0f5-461a-9829-7262f66c4d2f"
      },
      "outputs": [],
      "source": [
        "batch_size, lr, num_epochs = 128, 0.01, 5\n",
        "train_iter, val_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
        "train_loss_all, train_acc_all, val_loss_all, val_acc_all = train(net, train_iter, val_iter, test_iter, num_epochs, lr, try_gpu()) #22 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "GXZj0j8W39Ks",
        "outputId": "5bd1c423-17e7-4027-aa7d-83cb91a843fc"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_loss_all, val_loss_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "G8u6q9f74A7x",
        "outputId": "3b1cbe13-4be9-4cd3-fcd8-c6f1c20d6d63"
      },
      "outputs": [],
      "source": [
        "plot_accuracy(train_acc_all, val_acc_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV_YTfYndWDq"
      },
      "source": [
        "# Networks using blocks (VGG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAQsvFanvVKA"
      },
      "source": [
        "The basic building block of classic CNNs\n",
        "is a sequence of the following:\n",
        "(i) a convolutional layer\n",
        "with padding to maintain the resolution,\n",
        "(ii) a nonlinearity, such as a ReLU,\n",
        "(iii) a pooling layer, such\n",
        "as a maximum pooling layer.\n",
        "One VGG block consists of a sequence of convolutional layers,\n",
        "followed by a maximum pooling layer for spatial downsampling.\n",
        "In the original VGG paper, the authors employed convolutions with $3\\times3$ kernels with padding of $1$ (keeping height and width)\n",
        "and $2 \\times 2$ maximum pooling with stride of $2$\n",
        "(halving the resolution after each block).\n",
        "In the code below, we define a function called `vgg_block()`\n",
        "to implement one VGG block.\n",
        "\n",
        "The function takes three arguments corresponding to the number\n",
        "of convolutional layers `num_convs`, the number of input channels `in_channels`,\n",
        "and the number of output channels `out_channels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt8Oy3WwEMv9"
      },
      "outputs": [],
      "source": [
        "def vgg_block(num_convs, in_channels, out_channels):\n",
        "    layers = []\n",
        "    for _ in range(num_convs):\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels,\n",
        "                                kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "        in_channels = out_channels\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3cGmtEEv6Ys"
      },
      "source": [
        "Like AlexNet and LeNet, the VGG network can be partitioned into two parts:\n",
        "the first consisting mostly of convolutional and pooling layers,\n",
        "and the second consisting of fully-connected layers.\n",
        "\n",
        "The convolutional part of the network connects several VGG blocks (defined in the `vgg_block()` function), in succession. The following variable `conv_arch` consists of a list of tuples (one per block), where each contains two values: the number of convolutional layers and the number of output channels, which are precisely the arguments required to call the `vgg_block()` function. The fully-connected part of the VGG network is identical to that covered in AlexNet.\n",
        "\n",
        "The original VGG network had $5$ convolutional blocks, among which the first two have one convolutional layer each, and the latter three contain two convolutional layers each. The first block has $64$ output channels and each subsequent block doubles the number of output channels, until that number reaches $512$. Since this network uses $8$ convolutional layers and $3$ fully-connected layers, it is often called *VGG-11*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAYS05zdFHT4"
      },
      "outputs": [],
      "source": [
        "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzwTInYEwjDp"
      },
      "source": [
        "The following code implements VGG-11. This is a simple matter of executing a for-loop over `conv_arch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojVjxCHPFIix",
        "outputId": "4dff214c-2b39-4da8-be9c-369c9b5de642"
      },
      "outputs": [],
      "source": [
        "def vgg(conv_arch):\n",
        "    conv_blks = []\n",
        "    in_channels = 1\n",
        "    # The convolutional part\n",
        "    for (num_convs, out_channels) in conv_arch:\n",
        "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
        "        in_channels = out_channels\n",
        "\n",
        "    return nn.Sequential(\n",
        "        *conv_blks, nn.Flatten(),\n",
        "        # The fully-connected part\n",
        "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "        nn.Linear(4096, 10))\n",
        "\n",
        "net = vgg(conv_arch)\n",
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys0GeHufwq4e"
      },
      "source": [
        "Next, we will construct a single-channel data example with a height and width of $224$ to observe the output shape of each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5NY3A1XFTKV",
        "outputId": "c412f2c2-e1c3-4fdb-d825-52ec04c3176a"
      },
      "outputs": [],
      "source": [
        "X = torch.randn(1, 1, 224, 224)\n",
        "for blk in net:\n",
        "    X = blk(X)\n",
        "    print(blk.__class__.__name__, 'output shape:\\t', X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUPixKt3wyJR"
      },
      "source": [
        "As we can see, we halve height and width at each block, finally reaching a height and width of $7$, before flattening the representations for processing by the fully-connected part of the network.\n",
        "\n",
        "Since VGG-11 is more computationally-heavy than AlexNet, we construct a network with a smaller number of channels. This is more than sufficient for training on Fashion-MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAXIASTlF7b6",
        "outputId": "b7b61529-af4b-49cc-e186-ee356390f6a7"
      },
      "outputs": [],
      "source": [
        "small_conv_arch = ((1, 16), (1, 32), (2, 64), (2, 128), (2, 128))\n",
        "net = vgg(small_conv_arch)\n",
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJtCy-3gxNrT"
      },
      "source": [
        "Apart from using a slightly larger learning rate, the model training process is similar to that of AlexNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjWVdnY0GK04",
        "outputId": "80dfb7df-795c-41d8-df2a-28584892ed99"
      },
      "outputs": [],
      "source": [
        "batch_size, lr, num_epochs = 128, 0.05, 5\n",
        "train_iter, val_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
        "train_loss_all, train_acc_all, val_loss_all, val_acc_all = train(net, train_iter, val_iter, test_iter, num_epochs, lr, try_gpu()) #23 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Ov4PCO04bdlT",
        "outputId": "7ae6d380-9433-46b7-b1bf-177a8875fbcf"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_loss_all, val_loss_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Xwli2O_rbfuq",
        "outputId": "eb466adb-393b-49a5-d8e5-e3454e082bd2"
      },
      "outputs": [],
      "source": [
        "plot_accuracy(train_acc_all, val_acc_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1tU-GHldh62"
      },
      "source": [
        "# Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYB6k4A4xrLa"
      },
      "source": [
        "To see how to apply batch normalization in context, below we apply it to a traditional LeNet model. Recall that batch normalization is applied after the convolutional layers or fully-connected layers, but before the corresponding activation functions. The corresponding layers in PyTorch are `nn.BatchNorm2d` for batch normalization applied after convolutional layers, and `nn.BatchNorm1d` for batch normalization applied after fully-connected layers. The first parameter represents the number of output channels of the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUlfQP5kkZJi",
        "outputId": "136be682-fca5-4226-bd99-6872f5c6aa47"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
        "    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n",
        "    nn.Linear(84, 10))\n",
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rt-YHKdxtzB"
      },
      "source": [
        "As before, we will train our network on the Fashion-MNIST dataset. This code is virtually identical to that when we first trained LeNet. The main difference is the larger learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuIVLCKykjL8",
        "outputId": "9ec0d84b-acd5-4036-9e42-84b6055169ce"
      },
      "outputs": [],
      "source": [
        "batch_size, lr, num_epochs = 256, 1.0, 10\n",
        "train_iter, val_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
        "train_loss_all, train_acc_all, val_loss_all, val_acc_all = train(net, train_iter, val_iter, test_iter, num_epochs, lr, try_gpu()) #2 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "aUtTYGXwlQDh",
        "outputId": "fb192802-5505-434b-f557-6acf139a1a82"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_loss_all, val_loss_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kjzKkYo7lSIT",
        "outputId": "373c8816-0267-4ddf-ff05-da5b51149201"
      },
      "outputs": [],
      "source": [
        "plot_accuracy(train_acc_all, val_acc_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jamY3jqx08s"
      },
      "source": [
        "Let us have a look at the scale parameter $\\boldsymbol{\\gamma}$ (`weight`) and the shift parameter $\\boldsymbol{\\beta}$ (`bias`) learned from the first batch normalization layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9xFawa4x6O9",
        "outputId": "c261c19f-ceab-4b0b-b4c2-e794bc7a5eb4"
      },
      "outputs": [],
      "source": [
        "net[1].weight.data, net[1].bias.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3MxFfyjdr_V"
      },
      "source": [
        "# Residual networks (ResNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvVQufVm2tMc"
      },
      "source": [
        "ResNet follows VGG's full $3\\times 3$ convolutional layer design. The residual block has two $3\\times 3$ convolutional layers with the same number of output channels. Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. Then, we skip these two convolution operations and add the input directly before the final ReLU activation function.\n",
        "This kind of design requires the output of the two convolutional layers to be of the same shape as the input, so that they can be added together. If we want to change the number of channels, we need to introduce an additional $1\\times 1$ convolutional layer to transform the input into the desired shape for the addition operation. Let us have a look at the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxF4ViDPlbuv"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet.\"\"\"\n",
        "    def __init__(self, input_channels, num_channels,\n",
        "                 use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
        "                               kernel_size=3, padding=1, stride=strides)\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
        "                                   kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = nn.ReLU()(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return nn.ReLU()(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwztloeL3n8k"
      },
      "source": [
        "This code generates two types of networks: one where we add the input to the output before applying the ReLU nonlinearity, whenever `use_1x1conv=False`, and one where we adjust channels and resolution by means of a $1 \\times 1$ convolution before adding.\n",
        "\n",
        "Now, let us look at a situation where the input and output are of the same shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AjYM7OKliPC",
        "outputId": "3248a939-da47-43bd-a446-6c83d3e08091"
      },
      "outputs": [],
      "source": [
        "blk = Residual(3, 3)\n",
        "X = torch.rand(4, 3, 6, 6)\n",
        "Y = blk(X)\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73W74s1L31nr"
      },
      "source": [
        "We also have the option to halve the output height and width while increasing the number of output channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI5nlQj0lnJ-",
        "outputId": "11a3a7bb-8974-4627-a6bb-ae554e7a75f5"
      },
      "outputs": [],
      "source": [
        "blk = Residual(3, 6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6B_Q3NB4ALU"
      },
      "source": [
        "The first two layers of ResNet are: a $7\\times 7$ convolutional layer with $64$ output channels and a stride of $2$, followed by a $3\\times 3$ maximum pooling layer with a stride of $2$. The batch normalization layer is added after each convolutional layer in ResNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X51PAY-YlsWb"
      },
      "outputs": [],
      "source": [
        "b1 = nn.Sequential(\n",
        "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DH1tZ4U4bkM"
      },
      "source": [
        "ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a maximum pooling layer with a stride of $2$ has already been used, it is not necessary to reduce the height and width. In the first residual block, for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.\n",
        "\n",
        "Now, we implement this module. Note that special processing has been performed on the first module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je4CPDHLlu3C"
      },
      "outputs": [],
      "source": [
        "def resnet_block(input_channels, num_channels, num_residuals,\n",
        "                 first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(input_channels, num_channels,\n",
        "                                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels, num_channels))\n",
        "    return blk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J42oYedi4sEC"
      },
      "source": [
        "Then, we add all the modules to ResNet. Here, two residual blocks are used for each module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAnCohbMlyAA"
      },
      "outputs": [],
      "source": [
        "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
        "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
        "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
        "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG8hzxOL4yHw"
      },
      "source": [
        "Finally, we add a global average pooling layer, followed by the fully-connected layer output. The `nn.AdaptiveAvgPool2d` implements the global average pooling operation, ensuring that the output shape is the one given as its first parameter (in this case, $1\\times 1$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwimX2cXl0lo",
        "outputId": "2eb698c4-fe72-4944-d569-3680c0412249"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(\n",
        "    b1,\n",
        "    b2,\n",
        "    b3,\n",
        "    b4,\n",
        "    b5,\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(512, 10))\n",
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n70kcEF65cbM"
      },
      "source": [
        "There are $4$ convolutional layers in each module (excluding the $1\\times 1$ convolutional layer). Together with the first $7\\times 7$ convolutional layer and the final fully-connected layer, there are $18$ layers in total. Therefore, this model is commonly known as *ResNet-18*.\n",
        "\n",
        "By configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper $152$-layer *ResNet-152*.\n",
        "\n",
        "Before training ResNet, let us observe how the input shape changes across different modules in ResNet. As in all the previous architectures, the resolution decreases while the number of channels increases, up until the point where a global average pooling layer aggregates all features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebVSV486l-sN",
        "outputId": "71f03f93-74f7-40af-eddf-a17f34101352"
      },
      "outputs": [],
      "source": [
        "X = torch.rand(1, 1, 224, 224)\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlotK-sE6EDX"
      },
      "source": [
        "We train ResNet on the Fashion-MNIST dataset, just like before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ070pUSmVxA",
        "outputId": "8ee52e44-525f-442e-c9ef-e4a181d78c2d"
      },
      "outputs": [],
      "source": [
        "batch_size, lr, num_epochs = 256, 0.05, 5\n",
        "train_iter, val_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
        "train_loss_all, train_acc_all, val_loss_all, val_acc_all = train(net, train_iter, val_iter, test_iter, num_epochs, lr, try_gpu()) #11 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "CKZ5mx19pvkC",
        "outputId": "f978f079-8905-4268-ea75-a6d9aa9cf737"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_loss_all, val_loss_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "DhAE2qCOp2mU",
        "outputId": "803602d4-675a-40b3-a585-23a7eb5b2012"
      },
      "outputs": [],
      "source": [
        "plot_accuracy(train_acc_all, val_acc_all)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
