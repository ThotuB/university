{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5jhyhiSJVqWn"
      },
      "source": [
        " # Recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suoLpSoTs4Vq"
      },
      "source": [
        "First, we add the necessary imports and set a constant seed for the random number generator in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wBjOO9kPDJMT"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "\n",
        "torch.manual_seed(42);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPKDgFrzth4c"
      },
      "source": [
        "Text is one of the most popular examples of sequence data. For example, an article can be simply viewed as a sequence of words, or even a sequence of characters. The common preprocessing steps for text, usually, are:\n",
        "\n",
        "1. Load text as strings into memory.\n",
        "1. Split strings into tokens (e.g., words and characters).\n",
        "1. Build a table of vocabulary to map the split tokens to numerical indices.\n",
        "1. Convert text into sequences of numerical indices so they can be manipulated by models easily.\n",
        "\n",
        "To get started we load text from H. G. Wells' [*The Time Machine*](http://www.gutenberg.org/ebooks/35). This is a fairly small corpus of just over $30000$ words, but, for the purpose of what we want to illustrate, this is just fine. More realistic document collections contain many billions of words. The `read_time_machine()` function reads the dataset into a list of text lines, where each line is a string. For simplicity, here we ignore punctuation and capitalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B1Q50eDmDD_p"
      },
      "outputs": [],
      "source": [
        "def download(url, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file, return the local filename.\"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "        return fname\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PMDPiHuCc-h",
        "outputId": "f20fbc24-27bf-4bcf-cb8e-00395e9df5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# text lines: 3221\n",
            "the time machine by h g wells\n",
            "twinkled and his usually pale face was flushed and animated the\n"
          ]
        }
      ],
      "source": [
        "def read_time_machine():\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(download('http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "lines = read_time_machine()\n",
        "print(f'# text lines: {len(lines)}')\n",
        "print(lines[0])\n",
        "print(lines[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T--FND7uYhr"
      },
      "source": [
        "The following `tokenize()` function takes a list (`lines`) as the input, where each element is a text sequence (e.g., a text line). Each text sequence is split into a list of tokens. A *token* is the basic unit in text. In the end, a list of token lists are returned, where each token is a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NHosKb7CW_c",
        "outputId": "eab38285-3f87-4ba4-bf9c-2dbd84517cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['i']\n",
            "[]\n",
            "[]\n",
            "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
            "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
            "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
          ]
        }
      ],
      "source": [
        "def tokenize(lines, token='word'):\n",
        "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
        "    if token == 'word':\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == 'char':\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print('ERROR: unknown token type: ' + token)\n",
        "\n",
        "tokens = tokenize(lines)\n",
        "for i in range(11):\n",
        "    print(tokens[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp9vPYJ5upVt"
      },
      "source": [
        "The string type of the token is inconvenient to be used by models, which take numerical inputs. Now let us build a dictionary, often called *vocabulary* as well, to map string tokens into numerical indices starting from $0$. To do so, we first count the unique tokens in all the documents from the training set, namely a *corpus*, and then assign a numerical index to each unique token according to its frequency. Rarely appeared tokens are often removed to reduce the complexity. Any token that does not exist in the corpus or has been removed is mapped into a special unknown token “&lt;unk&gt;”. We optionally add a list of reserved tokens, such as “&lt;pad&gt;” for padding, “&lt;bos&gt;” to represent the beginning of a sequence, and “&lt;eos&gt;” for the end of a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vwnOjpzzCNjy"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Sort according to frequencies\n",
        "        counter = count_corpus(tokens)\n",
        "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                   reverse=True)\n",
        "        # The index for the unknown token is 0\n",
        "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "        for token, freq in self._token_freqs:\n",
        "            if freq < min_freq:\n",
        "                break\n",
        "            if token not in self.token_to_idx:\n",
        "                self.idx_to_token.append(token)\n",
        "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def token_freqs(self):  # Token frequencies\n",
        "        return self._token_freqs\n",
        "\n",
        "def count_corpus(tokens):\n",
        "    \"\"\"Count token frequencies.\"\"\"\n",
        "    # Here `tokens` is a 1D list or 2D list\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        # Flatten a list of token lists into a list of tokens\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNTQL-FkvUPy"
      },
      "source": [
        "We construct a vocabulary using the `time machine` dataset as the corpus. Then, we print the first few frequent tokens with their indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSfvoEGkvTqK",
        "outputId": "b40ecff2-2902-4ee5-a402-f4506f74e937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n",
            "4580\n"
          ]
        }
      ],
      "source": [
        "vocab = Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfbL1e35vm3n"
      },
      "source": [
        "Now, we can convert each text line into a list of numerical indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWJfWY2Nvp-z",
        "outputId": "3b674830-0c0d-4d8a-b1e0-e5d3da570ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
            "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
            "words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
            "indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
          ]
        }
      ],
      "source": [
        "for i in [0, 10]:\n",
        "    print('words:', tokens[i])\n",
        "    print('indices:', vocab[tokens[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvg1oSBrvunW"
      },
      "source": [
        "Using the above functions, we package everything into the `load_corpus_time_machine()` function, which returns `corpus`, a list of token indices, and `vocab`, the vocabulary of the time machine corpus.\n",
        "The modifications we did here are:\n",
        "(i) we tokenize text into characters, not words, to simplify the later training;\n",
        "(ii) `corpus` is a single list, not a list of token lists, since each text line in the `time machine` dataset is not necessarily a sentence or a paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99Br8poOCEUr",
        "outputId": "3ee42ffe-acf3-4b6a-80b5-c0fb9a864f26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(170580, 28)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
        "    lines = read_time_machine()\n",
        "    tokens = tokenize(lines, 'char')\n",
        "    vocab = Vocab(tokens)\n",
        "    # Since each text line in the time machine dataset is not necessarily a\n",
        "    # sentence or a paragraph, flatten all the text lines into a single list\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab\n",
        "\n",
        "corpus, vocab = load_corpus_time_machine()\n",
        "len(corpus), len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVE_0JDIwbZ1"
      },
      "source": [
        "We can ensure that the subsequences from two adjacent mini-batches during iteration are adjacent on the original sequence. This strategy preserves the order of split subsequences when iterating over mini-batches, hence is called *sequential partitioning*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9SrdiwSTBsxl"
      },
      "outputs": [],
      "source": [
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "    \"\"\"Generate a mini-batch of subsequences using sequential partitioning.\"\"\"\n",
        "    # Start with a random offset to partition a sequence\n",
        "    offset = random.randint(0, num_steps)\n",
        "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
        "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "    num_batches = Xs.shape[1] // num_steps\n",
        "    for i in range(0, num_steps * num_batches, num_steps):\n",
        "        X = Xs[:, i: i + num_steps]\n",
        "        Y = Ys[:, i: i + num_steps]\n",
        "        yield X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v28nktDxC9c"
      },
      "source": [
        "Let us manually generate a sequence from $0$ to $34$. We assume that the batch size and number of time steps are $2$ and $5$, respectively. This means that we can generate $\\lfloor (35 - 1) / 5 \\rfloor= 6$ feature-label subsequence pairs. With a mini-batch size of $2$, we only get $3$ mini-batches. \n",
        "\n",
        "Let us print features `X` and labels `Y` for each mini-batch of subsequences read by sequential partitioning. Note that the subsequences from two adjacent mini-batches during iteration are indeed adjacent in the original sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKUPBqXyxQZT",
        "outputId": "9bcd7d7e-c14a-4fc5-a871-c50554638419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X:  tensor([[ 1,  2,  3,  4,  5],\n",
            "        [17, 18, 19, 20, 21]]) \n",
            "Y: tensor([[ 2,  3,  4,  5,  6],\n",
            "        [18, 19, 20, 21, 22]])\n",
            "X:  tensor([[ 6,  7,  8,  9, 10],\n",
            "        [22, 23, 24, 25, 26]]) \n",
            "Y: tensor([[ 7,  8,  9, 10, 11],\n",
            "        [23, 24, 25, 26, 27]])\n",
            "X:  tensor([[11, 12, 13, 14, 15],\n",
            "        [27, 28, 29, 30, 31]]) \n",
            "Y: tensor([[12, 13, 14, 15, 16],\n",
            "        [28, 29, 30, 31, 32]])\n"
          ]
        }
      ],
      "source": [
        "my_seq = list(range(35))\n",
        "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsvQaQ7bx3FB"
      },
      "source": [
        "Now, we wrap the above two sampling functions to a class, so that we can use it as a data iterator later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O5pXs-IoBHtw"
      },
      "outputs": [],
      "source": [
        "class SeqDataLoader:\n",
        "    \"\"\"An iterator to load sequence data.\"\"\"\n",
        "    def __init__(self, batch_size, num_steps, max_tokens):\n",
        "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "        self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        return seq_data_iter_sequential(self.corpus, self.batch_size, self.num_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nes6WA9yJf8"
      },
      "source": [
        "Lastly, we define a function `load_data_time_machine()` that returns both the data iterator and the vocabulary, so we can use it similarly as other functions with the `load_data` prefix, such as `load_data_fashion_mnist()` defined in *Laboratory 3*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zxWxgOcS_auI"
      },
      "outputs": [],
      "source": [
        "def load_data_time_machine(batch_size, num_steps, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(\n",
        "        batch_size, num_steps, max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "743mlIXwzCyj"
      },
      "source": [
        "In order to implement a language model with RNNs, we use functions provided by the high-level APIs of PyTorch. We begin by reading the `time machine` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vmWtkkDzE5im"
      },
      "outputs": [],
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdUS7dvKzdN8"
      },
      "source": [
        "High-level APIs provide implementations of recurrent neural networks. We construct the recurrent neural network layer `rnn_layer` with a single hidden layer and $256$ hidden units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a6OUsw2KE8_o"
      },
      "outputs": [],
      "source": [
        "num_hiddens = 256\n",
        "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHKr4eqQz5ET"
      },
      "source": [
        "We use a tensor to initialize the hidden state, whose shape is\n",
        "(number of hidden layers, batch size, number of hidden units)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMga5UNiFFZZ",
        "outputId": "611145e9-704c-41a9-a453-185808088938"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 256])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = torch.zeros((1, batch_size, num_hiddens))\n",
        "state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmQH4slC0ALm"
      },
      "source": [
        "With a hidden state and an input, we can compute the output with the updated hidden state. It should be emphasized that the \"output\" (`Y`) of `rnn_layer` does *not* involve computation of output layers: it refers to the hidden state at *each* time step, and they can be used as the input to the subsequent output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kaj6poo5FHbX",
        "outputId": "e3e6ecc8-c466-44d5-c5fe-327b64a7fef8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
        "Y, state_new = rnn_layer(X, state)\n",
        "Y.shape, state_new.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIfiIdBeItzh"
      },
      "source": [
        "Recall that each token is represented as a numerical index in `train_iter`. Feeding these indices directly to a neural network might make it hard to learn. We often represent each token as a more expressive feature vector. The easiest representation is called *one-hot encoding*.\n",
        "\n",
        "In a nutshell, we map each index to a different unit vector: assume that the number of different tokens in the vocabulary is $N$ (`len(vocab)`) and the token indices range from $0$ to $N-1$. If the index of a token is the integer $i$, then we create a vector of all $0$s with a length of $N$ and set the element at position $i$ to $1$. This vector is the one-hot vector of the original token. The one-hot vectors with indices $0$ and $2$ are shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xwERALMJJUN",
        "outputId": "19869bfa-488b-4d59-ce01-61ba0fcf6e52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.one_hot(torch.tensor([0, 2]), len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrM4MzXrJM96"
      },
      "source": [
        "The shape of the mini-batch that we sample each time is (batch size, number of time steps). The `F.one_hot()` function transforms such a mini-batch into a three-dimensional tensor, where the last dimension equals to the vocabulary size (`len(vocab)`). We often transpose the input, so that we will obtain an output of shape (number of time steps, batch size, vocabulary size). This will allow us to more conveniently loop through the outermost dimension for updating hidden states of a mini-batch, time step by time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR5Lhs-YJv4V",
        "outputId": "1b622d02-64f3-40b9-ffbb-0ad8232ccec6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 28])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.arange(10).reshape((2, 5))\n",
        "F.one_hot(X.T, 28).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYM1tweo0OP8"
      },
      "source": [
        "We define an `RNNModel` class for a complete RNN model. Note that `rnn_layer` only contains the hidden recurrent layers, so we need to create a separate output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_cPsMX3tFKI8"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"The RNN model.\"\"\"\n",
        "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "        super(RNNModel, self).__init__(**kwargs)\n",
        "        self.rnn = rnn_layer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_hiddens = self.rnn.hidden_size\n",
        "        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
        "\n",
        "    def forward(self, inputs, state):\n",
        "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
        "        X = X.to(torch.float32)\n",
        "        Y, state = self.rnn(X, state)\n",
        "        # The fully connected layer will first change the shape of `Y` to\n",
        "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
        "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
        "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
        "        return output, state\n",
        "\n",
        "    def begin_state(self, device, batch_size=1):\n",
        "        if not isinstance(self.rnn, nn.LSTM):\n",
        "            # `nn.GRU` takes a tensor as hidden state\n",
        "            return  torch.zeros((self.rnn.num_layers,\n",
        "                                 batch_size, self.num_hiddens),\n",
        "                                 device=device)\n",
        "        else:\n",
        "            # `nn.LSTM` takes a tuple of hidden states\n",
        "            return (torch.zeros((\n",
        "                self.rnn.num_layers,\n",
        "                batch_size, self.num_hiddens), device=device),\n",
        "                    torch.zeros((\n",
        "                        self.rnn.num_layers,\n",
        "                        batch_size, self.num_hiddens), device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhL7VPtM0uPN"
      },
      "source": [
        "Let us first define the prediction function to generate new characters following\n",
        "the user-provided `prefix`, which is a string containing several characters.  When looping through these beginning characters in `prefix`, we keep passing the hidden state to the next time step without generating any output. This is called the *warm-up* period, during which the model updates itself (e.g., update the hidden state), but does not make predictions. After the warm-up period, the hidden state is generally better than its initialized value at the beginning. So we generate the predicted characters and emit them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qhqJVv_XFlIM"
      },
      "outputs": [],
      "source": [
        "def predict(prefix, num_preds, net, vocab, device):\n",
        "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
        "    state = net.begin_state(batch_size=1, device=device)\n",
        "    outputs = [vocab[prefix[0]]]\n",
        "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
        "    for y in prefix[1:]:  # Warm-up period\n",
        "        _, state = net(get_input(), state)\n",
        "        outputs.append(vocab[y])\n",
        "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
        "        y, state = net(get_input(), state)\n",
        "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0txXSfB1Rhq"
      },
      "source": [
        "Now we can test the `predict()` function. We specify the prefix as `time traveller` and have it generate $10$ additional characters. Given that we have not trained the network yet, i.e., the model has random weights, it will generate nonsensical predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GwJpmF1GFebv"
      },
      "outputs": [],
      "source": [
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eapyU5gTFOcx",
        "outputId": "c76a43f5-d229-4714-a960-4317cd8c75d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'time travellervv<unk>vvvvvvv'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = try_gpu()\n",
        "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
        "net = net.to(device)\n",
        "predict('time traveller', 10, net, vocab, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUYVRxzg10HO"
      },
      "source": [
        "Below we define a function to *clip the gradients* of a model that is constructed by the high-level APIs. Also, note that we compute the gradient norm over all the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7kQbdoDkF78-"
      },
      "outputs": [],
      "source": [
        "def grad_clipping(net, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8EUuKWc2KBr"
      },
      "source": [
        "Before training the model, let us define a function to train the model in one epoch. It differs from how we train the model of *Laboratory 3* in three places:\n",
        "\n",
        "1. Different types of recurrent layers will result in differences in the initialization of hidden states.\n",
        "1. We clip the gradients before updating the model parameters. This ensures that the model does not diverge, even when gradients blow up at some point during the training process.\n",
        "1. We use perplexity to evaluate the model. As discussed in the course, this ensures that sequences of different lengths are comparable.\n",
        "\n",
        "Specifically, when sequential partitioning is used, we initialize the hidden state only at the beginning of each epoch. Since the $i$th subsequence example in the next mini-batch is adjacent to the current $i$th subsequence example, the hidden state at the end of the current mini-batch will be used to initialize the hidden state at the beginning of the next mini-batch. In this way, historical information of the sequence stored in the hidden state might flow over adjacent subsequences within an epoch. However, the computation of the hidden state at any point depends on all the previous mini-batches in the same epoch, which complicates the gradient computation. To reduce computational cost, we detach the gradient before processing any mini-batch, so that the gradient computation of the hidden state is always limited to the time steps in one mini-batch. \n",
        "\n",
        "Same as the `train_epoch()` function in *Laboratory 3*, `optimizer` is a built-in optimization function in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EmBMWd6vF4VV"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net, train_iter, loss, optimizer, device):\n",
        "    \"\"\"Train a net within one epoch.\"\"\"\n",
        "    state = None\n",
        "    # Sum of training loss, no. of tokens\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    for X, Y in train_iter:\n",
        "        if state is None:\n",
        "            # Initialize `state` when it is the first iteration\n",
        "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "        else:\n",
        "            if not isinstance(state, tuple):\n",
        "                # `state` is a tensor for `nn.GRU`\n",
        "                state.detach_()\n",
        "            else:\n",
        "                # `state` is a tuple of tensors for `nn.LSTM`\n",
        "                for s in state:\n",
        "                    s.detach_()\n",
        "        y = Y.T.reshape(-1)\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat, state = net(X, state)\n",
        "        l = loss(y_hat, y.long()).mean()\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        grad_clipping(net, 1)\n",
        "        optimizer.step()\n",
        "        total_loss += float(l * y.numel())\n",
        "        total_tokens += y.numel()\n",
        "    return math.exp(total_loss / total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X8TY2qj32lD"
      },
      "source": [
        "The training function supports an RNN model implemented using high-level APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vr59uClMG-bo"
      },
      "outputs": [],
      "source": [
        "def train(net, train_iter, vocab, lr, num_epochs, device):\n",
        "    \"\"\"Train a model.\"\"\"\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    perplexities = []\n",
        "    # Initialize\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr)\n",
        "    # Train and predict\n",
        "    for epoch in range(num_epochs):\n",
        "        ppl = train_epoch(\n",
        "            net, train_iter, loss, optimizer, device)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(predict('time traveller', 50, net, vocab, device))\n",
        "            perplexities.append(ppl)\n",
        "    print(f'perplexity {ppl:.1f}, device {str(device)}')\n",
        "    print(predict('time traveller', 50, net, vocab, device))\n",
        "    print(predict('traveller', 50, net, vocab, device))\n",
        "\n",
        "    return perplexities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJQZD20NOdck"
      },
      "source": [
        "Now we can train the RNN model by calling the `train()` function. Since we only use $10000$ tokens in the dataset, the model needs more epochs to converge better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6RQ706HFVlc",
        "outputId": "93e85cb4-9aa2-42a8-8127-2db0c80e06e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time traveller an the the the the the the the the the the the th\n",
            "time travellere the the the the the the the the the the the the \n",
            "time traveller the the the the the the the the the the the the t\n",
            "time travellere ment andithe andithe andithe andithe andithe and\n",
            "time traveller sof cand the thas ans he thate the that me thavel\n",
            "time traveller aithe pareeree the here he maneed fice aid the th\n",
            "time traveller the f dimensions an sime and the t an at mithe mo\n",
            "time travellerist acand his there dithe is in and heregtred d an\n",
            "time traveller the rid the mert hour that in the ond the medichi\n",
            "time travelleris tha fe the ghat t ano three at that have at on \n",
            "time traveller of in tove aree this dimensions ffer and here thi\n",
            "time travellerit s ag in thas oo so serment abllerrectencoon s o\n",
            "time travellerthe thing song the wisc loug thent of the inges yo\n",
            "time traveller pacd hal hise tire erthit the ment lorep ctmat he\n",
            "time traveller comentryce this in thryg at whaver tre gensthed t\n",
            "time traveller cald and thed thick in and burti exurenis trackel\n",
            "time traveller tre tamy the otherthry is allagle the thing theit\n",
            "time traveller tem than lyon sowile wpone there are burad exanth\n",
            "time travellerit s against reason said filbys che trofee thivens\n",
            "time traveller a thad nother oflo the warkouj the poon ansther h\n",
            "time traveller amone might and there did falling if filw y mecei\n",
            "time travellerimes foreded thickling soon whiskees in time atmon\n",
            "time traveller for somith of opmental buchef readle this tome s \n",
            "time travellerit s aglins and tive y oomed mon gsare buth a said\n",
            "time travellerit s against reason amingttallic ffresheling and t\n",
            "time traveller held in his tandlast haln a on tht geseat this au\n",
            "time travelleryou tallogkous right on mont ulwryco jef animuredi\n",
            "time traveller se hed fo berydit bocaccentlicall racheard and fo\n",
            "time traveller a thand wores hat hith dine sians for said the me\n",
            "time travellerif riabt and ware asthis wis time this il soverthe\n",
            "time travellerit s aganoure buthat deime be and have uven tracll\n",
            "time traveller bether at fighed an this uofitry it revere the fo\n",
            "time traveller paccedthe time ssait bo bag hare alon as a conkw \n",
            "time traveller artinee hare wer anof eraved for ans are as abuen\n",
            "time traveller for so it will be convenient to speak ou himw ssa\n",
            "time traveller for so it will be convenient to speak oushis s ap\n",
            "time travellerit s against re sove ard mura s ano jerycolld his \n",
            "time traveller held in his hand was a glict accinnstored the tro\n",
            "time travelleryou can show black is whiten y time the ver for ve\n",
            "time traveller comben stof wraciebsime tofrely scolld ht readde \n",
            "time traveller some fichin ur allyackethere is a peptoun of fisc\n",
            "time travelleris filbre the gareoter on in tre weot i of hracien\n",
            "time travelleryounc there was thatluxusiogs and yured and of spa\n",
            "time traveller after the pauserequired fer anthad in and di fert\n",
            "time travelleryou can show black mothick of o it and ssmod he ma\n",
            "time traveller proceeded the thoure becr foc at aid in tantat an\n",
            "time traveller proceeded anyreal badnorf ip as anoud a domenty a\n",
            "time traveller for so it wis an therendth ard in all in that is \n",
            "time travellerit fou the ove hid ficp usmassonl sonnt they wine \n",
            "time traveller after tiee avencle nite cone we buthed have no sh\n",
            "perplexity 1.3, device cpu\n",
            "time traveller after tiee avencle nite cone we buthed have no sh\n",
            "traveller a tine you thing y uplesthe time traveller after \n"
          ]
        }
      ],
      "source": [
        "num_epochs, lr = 500, 1\n",
        "perplexities = train(net, train_iter, vocab, lr, num_epochs, device) #1 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4B8zHv6PMbw"
      },
      "source": [
        "Finally, we plot the perplexities obtained during training, using the `plot_perplexity()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YvS_979fHXRO"
      },
      "outputs": [],
      "source": [
        "def plot_perplexity(perplexities):\n",
        "    epochs = range(10, len(perplexities * 10) + 1, 10)\n",
        "    plt.plot(epochs, perplexities, 'b', label='Train perplexity') \n",
        "    plt.title('Training perplexity') \n",
        "    plt.xlabel('Epochs') \n",
        "    plt.ylabel('Perplexity') \n",
        "    plt.legend()  \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "WeOdstSoIfgG",
        "outputId": "7f347484-83af-4bd9-9230-537f79fc7e87"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_perplexity(perplexities)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
