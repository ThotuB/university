{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 4\n",
    "batch = 1\n",
    "seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_net = nn.RNN(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand((seq_len, batch, input_size))\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.rand((1, batch, hidden_size))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 4]), torch.Size([1, 1, 4]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, state_new = ex1_net(input, state)\n",
    "output.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3161,  0.1390,  0.7439, -0.6749]],\n",
       "\n",
       "        [[ 0.0449,  0.4974,  0.5711, -0.8761]],\n",
       "\n",
       "        [[ 0.2063,  0.4397,  0.6833, -0.8539]],\n",
       "\n",
       "        [[ 0.0434,  0.4223,  0.6619, -0.8188]],\n",
       "\n",
       "        [[-0.0951,  0.5377,  0.7382, -0.8407]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "import requests\n",
    "\n",
    "def download(url: str) -> str:\n",
    "    \"\"\"Download a file, return the local filename.\"\"\"\n",
    "    file_path = '../data/' + url.split('/')[-1]\n",
    "    if os.path.exists(file_path):\n",
    "        return file_path\n",
    "    \n",
    "    print(f'Downloading {file_path} from {url}...')\n",
    "    res = requests.get(url, stream=True, verify=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(res.content)\n",
    "    return file_path\n",
    "\n",
    "def read_time_machine() -> list[str]:\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    file = download('http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt')\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    lines_transformed = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "    return [line for line in lines_transformed if line]\n",
    "\n",
    "def tokenize(lines: list[str], token: Literal['word', 'char']='word') -> list[str]:\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        tokens: list[list[str]] =  [line.split() for line in lines]\n",
    "    else:\n",
    "        tokens: list[list[str]] = [list(line) for line in lines]\n",
    "    return [token for sublist in tokens for token in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens: list[str]=[], min_freq=0, reserved_tokens: list[str]=[]):\n",
    "        # Sort according to frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens + [token for token, freq in self._token_freqs if freq >= min_freq]\n",
    "        self.token_to_idx =  {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens: str | list[str]) -> int | list[int]:\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.token_to_idx.get(token, self.unk) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices: int | list[int]) -> str | list[str]:\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self) -> Literal[0]:  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self) -> list[tuple[str, int]]:  # Token frequencies\n",
    "        return self._token_freqs\n",
    "\n",
    "\n",
    "class SeqDataLoader:\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size: int, num_steps: int, max_tokens: int=10000):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.vocab = self.generate_vocab()\n",
    "        self.corpus = self.generate_corpus(max_tokens)\n",
    "        \n",
    "    def generate_vocab(self) -> Vocab:\n",
    "        lines = read_time_machine()\n",
    "        tokens = tokenize(lines, 'char')\n",
    "        return Vocab(tokens)\n",
    "    \n",
    "    def generate_corpus(self, max_tokens):\n",
    "        tokens = tokenize(read_time_machine(), 'char')\n",
    "        corpus = [self.vocab[token] for line in tokens for token in line]\n",
    "        return corpus[:max_tokens]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate a mini-batch of subsequences using sequential partitioning.\"\"\"\n",
    "        # Start with a random offset to partition a sequence\n",
    "        offset = random.randint(0, self.num_steps)\n",
    "        num_tokens = ((len(self.corpus) - offset - 1) // self.batch_size) * self.batch_size\n",
    "        Xs = torch.tensor(self.corpus[offset: offset + num_tokens])\n",
    "        Ys = torch.tensor(self.corpus[offset + 1: offset + 1 + num_tokens])\n",
    "        Xs, Ys = Xs.reshape(self.batch_size, -1), Ys.reshape(self.batch_size, -1)\n",
    "        num_batches = Xs.shape[1] // self.num_steps\n",
    "        for i in range(0, self.num_steps * num_batches, self.num_steps):\n",
    "            X = Xs[:, i: i + self.num_steps]\n",
    "            Y = Ys[:, i: i + self.num_steps]\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 30\n",
    "steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = SeqDataLoader(batch, steps)\n",
    "vocab = train_iter.vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(vocab)\n",
    "hidden_size = 32\n",
    "reccurent_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_net = nn.RNN(input_size, hidden_size, reccurent_layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net: nn.RNN, theta: float):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "Loss = callable\n",
    "\n",
    "\n",
    "def train_epoch(net: nn.RNN, train_iter: SeqDataLoader, loss, optimizer: torch.optim.Optimizer, device):\n",
    "    \"\"\"Train a net within one epoch.\"\"\"\n",
    "    state = None\n",
    "    # Sum of training loss, no. of tokens\n",
    "    total_loss: int = 0\n",
    "    total_tokens: int = 0\n",
    "    for X, Y in train_iter:\n",
    "        if state is None:\n",
    "            # Initialize `state` when it is the first iteration\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM`\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        optimizer.step()\n",
    "        total_loss += float(l * y.numel())\n",
    "        total_tokens += y.numel()\n",
    "        \n",
    "    return math.exp(total_loss / total_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix: str, num_preds: int, net: nn.RNN, vocab: Vocab, device) -> str:\n",
    "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net: nn.RNN, train_iter: SeqDataLoader, vocab: Vocab, lr: float, num_epochs: int, device):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    perplexities = []\n",
    "    # Initialize\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl = train_epoch(net, train_iter, loss, optimizer, device)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller', 50, net, vocab, device))\n",
    "            perplexities.append(ppl)\n",
    "    print(f'perplexity {ppl:.1f}, device {str(device)}')\n",
    "    print(predict('time traveller', 50, net, vocab, device))\n",
    "    print(predict('traveller', 50, net, vocab, device))\n",
    "\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "lr = 1.5\n",
    "perplexities = train(ex2_net, train_iter, vocab, lr, num_epochs, device) #1 min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
